{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('/Users/Afnan/Desktop/FinalData/FinalTraining.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation= pd.read_csv('/Users/Afnan/Desktop/FinalData/FinalValidation.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_jsonl(csv_file_path, jsonl_file_path):\n",
    "    with open(csv_file_path, 'r') as csv_file, open(jsonl_file_path, 'w') as jsonl_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            json.dump(row, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_jsonl('/Users/Afnan/Desktop/FinalData/FinalTraining.csv','train_split.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_jsonl('/Users/Afnan/Desktop/FinalData/FinalValidation.csv','validation_split.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1\n",
    "Prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_jsonl(input_file_path, output_file_path):\n",
    "    entries = []\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            entries.append(entry)\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in entries:\n",
    "            messages = []\n",
    "            messages.append({\"role\": \"system\", \n",
    " \"content\": \n",
    "\"You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness \"})\n",
    "            user_message = {\"role\": \"user\", \"content\": entry[\"text\"]}\n",
    "            assistant_message = {\"role\": \"assistant\", \"content\": entry[\"label\"]}\n",
    "            messages.extend([user_message, assistant_message])\n",
    "            result = {\"messages\": messages}\n",
    "            json.dump(result, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_jsonl(\"/Users/Afnan/Desktop/FinalData/train_split.jsonl\", \"Format_train_split.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_jsonl(\"/Users/Afnan/Desktop/FinalData/validation_split.jsonl\", \"Format_validation_split.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "Upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key= \"   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afnan\\anaconda3\\Lib\\site-packages\\pip\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "print(pip.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-a70lxStY2oLEIcM9NMlb8waJ\n",
      "Validation file id: file-V9YaNva6chzwov9jDnLg9nTV\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = openai.OpenAI(api_key=\"\")\n",
    "training_response = client.files.create(\n",
    "    file=open(\"/Users/Afnan/Desktop/FinalData/Format_train_split.jsonl\", \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file=open(\"/Users/Afnan/Desktop/FinalData/Format_validation_split.jsonl\", \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id\n",
    "\n",
    "print(\"Training file id:\", training_file_id)\n",
    "print(\"Validation file id:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afnan\\anaconda3\\Lib\\site-packages\\pip\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "print(pip.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3\n",
    "Create a fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Pyweyzaw7VycvlRk7qj5EjMW', created_at=1715703197, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-Fi9mlsoWL88JdIl2wSCu95we', result_files=[], seed=1116149602, status='validating_files', trained_tokens=None, training_file='file-a70lxStY2oLEIcM9NMlb8waJ', validation_file='file-V9YaNva6chzwov9jDnLg9nTV', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
     ]
    }
   ],
   "source": [
    "suffix_name = \"samantha-test\"\n",
    "\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Pyweyzaw7VycvlRk7qj5EjMW', created_at=1715703197, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-Fi9mlsoWL88JdIl2wSCu95we', result_files=[], seed=1116149602, status='validating_files', trained_tokens=None, training_file='file-a70lxStY2oLEIcM9NMlb8waJ', validation_file='file-V9YaNva6chzwov9jDnLg9nTV', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-Pyweyzaw7VycvlRk7qj5EjMW\n",
      "Validating training file: file-a70lxStY2oLEIcM9NMlb8waJ and validation file: file-V9YaNva6chzwov9jDnLg9nTV\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1767: training loss=1.19\n",
      "Step 2/1767: training loss=0.87\n",
      "Step 3/1767: training loss=1.09\n",
      "Step 4/1767: training loss=1.32\n",
      "Step 5/1767: training loss=0.55\n",
      "Step 6/1767: training loss=0.60\n",
      "Step 7/1767: training loss=0.51\n",
      "Step 8/1767: training loss=1.47\n",
      "Step 9/1767: training loss=1.37\n",
      "Step 10/1767: training loss=0.80\n",
      "Step 11/1767: training loss=0.70\n",
      "Step 12/1767: training loss=1.30\n",
      "Step 13/1767: training loss=0.36\n",
      "Step 14/1767: training loss=0.73\n",
      "Step 15/1767: training loss=0.07\n",
      "Step 16/1767: training loss=0.15\n",
      "Step 17/1767: training loss=0.11\n",
      "Step 18/1767: training loss=1.27\n",
      "Step 19/1767: training loss=0.46\n",
      "Step 20/1767: training loss=0.42\n",
      "Step 21/1767: training loss=1.01\n",
      "Step 22/1767: training loss=0.07\n",
      "Step 23/1767: training loss=0.34\n",
      "Step 24/1767: training loss=1.05\n",
      "Step 25/1767: training loss=0.88\n",
      "Step 26/1767: training loss=0.31\n",
      "Step 27/1767: training loss=0.15\n",
      "Step 28/1767: training loss=0.01\n",
      "Step 29/1767: training loss=0.01\n",
      "Step 30/1767: training loss=0.02\n",
      "Step 31/1767: training loss=0.80\n",
      "Step 32/1767: training loss=0.25\n",
      "Step 33/1767: training loss=0.26\n",
      "Step 34/1767: training loss=0.77\n",
      "Step 35/1767: training loss=0.23\n",
      "Step 36/1767: training loss=0.31\n",
      "Step 37/1767: training loss=0.32\n",
      "Step 38/1767: training loss=0.11\n",
      "Step 39/1767: training loss=0.01\n",
      "Step 40/1767: training loss=0.04\n",
      "Step 41/1767: training loss=0.02\n",
      "Step 42/1767: training loss=0.02\n",
      "Step 43/1767: training loss=0.28\n",
      "Step 44/1767: training loss=0.46\n",
      "Step 45/1767: training loss=0.00\n",
      "Step 46/1767: training loss=0.00\n",
      "Step 47/1767: training loss=0.03\n",
      "Step 48/1767: training loss=0.00\n",
      "Step 49/1767: training loss=0.00\n",
      "Step 50/1767: training loss=0.41\n",
      "Step 51/1767: training loss=0.59\n",
      "Step 52/1767: training loss=0.85\n",
      "Step 53/1767: training loss=0.00\n",
      "Step 54/1767: training loss=0.00\n",
      "Step 55/1767: training loss=0.92\n",
      "Step 56/1767: training loss=0.81\n",
      "Step 57/1767: training loss=0.00\n",
      "Step 58/1767: training loss=1.35\n",
      "Step 59/1767: training loss=0.57\n",
      "Step 60/1767: training loss=0.41\n",
      "Step 61/1767: training loss=0.65\n",
      "Step 62/1767: training loss=0.00\n",
      "Step 63/1767: training loss=0.01\n",
      "Step 64/1767: training loss=0.42\n",
      "Step 65/1767: training loss=0.78\n",
      "Step 66/1767: training loss=0.57\n",
      "Step 67/1767: training loss=0.16\n",
      "Step 68/1767: training loss=0.00\n",
      "Step 69/1767: training loss=0.00\n",
      "Step 70/1767: training loss=0.00\n",
      "Step 71/1767: training loss=0.00\n",
      "Step 72/1767: training loss=0.00\n",
      "Step 73/1767: training loss=0.00\n",
      "Step 74/1767: training loss=0.00\n",
      "Step 75/1767: training loss=0.04\n",
      "Step 76/1767: training loss=0.00\n",
      "Step 77/1767: training loss=0.84\n",
      "Step 78/1767: training loss=0.00\n",
      "Step 79/1767: training loss=0.02\n",
      "Step 80/1767: training loss=0.00\n",
      "Step 81/1767: training loss=0.00\n",
      "Step 82/1767: training loss=0.00\n",
      "Step 83/1767: training loss=0.00\n",
      "Step 84/1767: training loss=0.00\n",
      "Step 85/1767: training loss=1.71\n",
      "Step 86/1767: training loss=0.00\n",
      "Step 87/1767: training loss=0.00\n",
      "Step 88/1767: training loss=3.97\n",
      "Step 89/1767: training loss=0.00\n",
      "Step 90/1767: training loss=0.00\n",
      "Step 91/1767: training loss=0.00\n",
      "Step 92/1767: training loss=0.00\n",
      "Step 93/1767: training loss=0.00\n",
      "Step 94/1767: training loss=0.00\n",
      "Step 95/1767: training loss=0.00\n",
      "Step 96/1767: training loss=0.00\n",
      "Step 97/1767: training loss=0.00\n",
      "Step 98/1767: training loss=1.71\n",
      "Step 99/1767: training loss=0.00\n",
      "Step 100/1767: training loss=0.63, validation loss=0.00\n",
      "Step 101/1767: training loss=0.00\n",
      "Step 102/1767: training loss=1.29\n",
      "Step 103/1767: training loss=0.00\n",
      "Step 104/1767: training loss=0.00\n",
      "Step 105/1767: training loss=1.90\n",
      "Step 106/1767: training loss=1.48\n",
      "Step 107/1767: training loss=1.37\n",
      "Step 108/1767: training loss=1.02\n",
      "Step 109/1767: training loss=1.10\n",
      "Step 110/1767: training loss=0.00\n",
      "Step 111/1767: training loss=1.44\n",
      "Step 112/1767: training loss=0.00\n",
      "Step 113/1767: training loss=1.20\n",
      "Step 114/1767: training loss=0.29\n",
      "Step 115/1767: training loss=0.00\n",
      "Step 116/1767: training loss=0.00\n",
      "Step 117/1767: training loss=0.42\n",
      "Step 118/1767: training loss=0.00\n",
      "Step 119/1767: training loss=1.07\n",
      "Step 120/1767: training loss=0.46\n",
      "Step 121/1767: training loss=0.00\n",
      "Step 122/1767: training loss=0.00\n",
      "Step 123/1767: training loss=0.00\n",
      "Step 124/1767: training loss=0.59\n",
      "Step 125/1767: training loss=0.42\n",
      "Step 126/1767: training loss=0.00\n",
      "Step 127/1767: training loss=0.00\n",
      "Step 128/1767: training loss=0.10\n",
      "Step 129/1767: training loss=0.00\n",
      "Step 130/1767: training loss=0.00\n",
      "Step 131/1767: training loss=0.34\n",
      "Step 132/1767: training loss=0.00\n",
      "Step 133/1767: training loss=1.92\n",
      "Step 134/1767: training loss=0.00\n",
      "Step 135/1767: training loss=0.00\n",
      "Step 136/1767: training loss=0.08\n",
      "Step 137/1767: training loss=0.69\n",
      "Step 138/1767: training loss=0.00\n",
      "Step 139/1767: training loss=0.00\n",
      "Step 140/1767: training loss=0.00\n",
      "Step 141/1767: training loss=0.00\n",
      "Step 142/1767: training loss=2.50\n",
      "Step 143/1767: training loss=0.00\n",
      "Step 144/1767: training loss=1.11\n",
      "Step 145/1767: training loss=1.62\n",
      "Step 146/1767: training loss=0.00\n",
      "Step 147/1767: training loss=0.00\n",
      "Step 148/1767: training loss=0.00\n",
      "Step 149/1767: training loss=1.26\n",
      "Step 150/1767: training loss=0.00\n",
      "Step 151/1767: training loss=0.09\n",
      "Step 152/1767: training loss=1.16\n",
      "Step 153/1767: training loss=0.13\n",
      "Step 154/1767: training loss=0.00\n",
      "Step 155/1767: training loss=0.09\n",
      "Step 156/1767: training loss=0.00\n",
      "Step 157/1767: training loss=0.00\n",
      "Step 158/1767: training loss=0.00\n",
      "Step 159/1767: training loss=1.12\n",
      "Step 160/1767: training loss=0.01\n",
      "Step 161/1767: training loss=0.00\n",
      "Step 162/1767: training loss=0.00\n",
      "Step 163/1767: training loss=0.00\n",
      "Step 164/1767: training loss=0.00\n",
      "Step 165/1767: training loss=1.49\n",
      "Step 166/1767: training loss=1.97\n",
      "Step 167/1767: training loss=0.00\n",
      "Step 168/1767: training loss=0.82\n",
      "Step 169/1767: training loss=1.71\n",
      "Step 170/1767: training loss=0.00\n",
      "Step 171/1767: training loss=1.80\n",
      "Step 172/1767: training loss=0.00\n",
      "Step 173/1767: training loss=1.00\n",
      "Step 174/1767: training loss=0.52\n",
      "Step 175/1767: training loss=0.02\n",
      "Step 176/1767: training loss=0.02\n",
      "Step 177/1767: training loss=1.06\n",
      "Step 178/1767: training loss=1.05\n",
      "Step 179/1767: training loss=0.00\n",
      "Step 180/1767: training loss=0.06\n",
      "Step 181/1767: training loss=0.32\n",
      "Step 182/1767: training loss=0.31\n",
      "Step 183/1767: training loss=0.17\n",
      "Step 184/1767: training loss=0.12\n",
      "Step 185/1767: training loss=0.21\n",
      "Step 186/1767: training loss=0.27\n",
      "Step 187/1767: training loss=0.00\n",
      "Step 188/1767: training loss=0.33\n",
      "Step 189/1767: training loss=0.04\n",
      "Step 190/1767: training loss=0.05\n",
      "Step 191/1767: training loss=0.02\n",
      "Step 192/1767: training loss=0.42\n",
      "Step 193/1767: training loss=0.00\n",
      "Step 194/1767: training loss=0.17\n",
      "Step 195/1767: training loss=0.83\n",
      "Step 196/1767: training loss=0.66\n",
      "Step 197/1767: training loss=0.00\n",
      "Step 198/1767: training loss=0.33\n",
      "Step 199/1767: training loss=0.29\n",
      "Step 200/1767: training loss=0.00, validation loss=0.00\n",
      "Step 201/1767: training loss=0.24\n",
      "Step 202/1767: training loss=0.00\n",
      "Step 203/1767: training loss=0.00\n",
      "Step 204/1767: training loss=1.68\n",
      "Step 205/1767: training loss=0.00\n",
      "Step 206/1767: training loss=0.52\n",
      "Step 207/1767: training loss=0.00\n",
      "Step 208/1767: training loss=1.09\n",
      "Step 209/1767: training loss=0.52\n",
      "Step 210/1767: training loss=0.03\n",
      "Step 211/1767: training loss=1.33\n",
      "Step 212/1767: training loss=0.00\n",
      "Step 213/1767: training loss=1.05\n",
      "Step 214/1767: training loss=0.00\n",
      "Step 215/1767: training loss=0.01\n",
      "Step 216/1767: training loss=0.00\n",
      "Step 217/1767: training loss=0.01\n",
      "Step 218/1767: training loss=0.36\n",
      "Step 219/1767: training loss=1.11\n",
      "Step 220/1767: training loss=1.12\n",
      "Step 221/1767: training loss=0.00\n",
      "Step 222/1767: training loss=1.31\n",
      "Step 223/1767: training loss=0.00\n",
      "Step 224/1767: training loss=0.00\n",
      "Step 225/1767: training loss=0.00\n",
      "Step 226/1767: training loss=0.52\n",
      "Step 227/1767: training loss=0.00\n",
      "Step 228/1767: training loss=0.00\n",
      "Step 229/1767: training loss=0.00\n",
      "Step 230/1767: training loss=0.88\n",
      "Step 231/1767: training loss=1.23\n",
      "Step 232/1767: training loss=0.00\n",
      "Step 233/1767: training loss=1.90\n",
      "Step 234/1767: training loss=0.98\n",
      "Step 235/1767: training loss=1.55\n",
      "Step 236/1767: training loss=0.00\n",
      "Step 237/1767: training loss=0.95\n",
      "Step 238/1767: training loss=0.00\n",
      "Step 239/1767: training loss=0.28\n",
      "Step 240/1767: training loss=0.17\n",
      "Step 241/1767: training loss=0.01\n",
      "Step 242/1767: training loss=0.00\n",
      "Step 243/1767: training loss=0.93\n",
      "Step 244/1767: training loss=0.21\n",
      "Step 245/1767: training loss=0.07\n",
      "Step 246/1767: training loss=0.01\n",
      "Step 247/1767: training loss=0.71\n",
      "Step 248/1767: training loss=0.07\n",
      "Step 249/1767: training loss=0.00\n",
      "Step 250/1767: training loss=0.09\n",
      "Step 251/1767: training loss=0.95\n",
      "Step 252/1767: training loss=0.07\n",
      "Step 253/1767: training loss=0.58\n",
      "Step 254/1767: training loss=1.23\n",
      "Step 255/1767: training loss=0.00\n",
      "Step 256/1767: training loss=0.00\n",
      "Step 257/1767: training loss=0.00\n",
      "Step 258/1767: training loss=0.00\n",
      "Step 259/1767: training loss=0.00\n",
      "Step 260/1767: training loss=0.00\n",
      "Step 261/1767: training loss=1.28\n",
      "Step 262/1767: training loss=0.49\n",
      "Step 263/1767: training loss=0.65\n",
      "Step 264/1767: training loss=0.00\n",
      "Step 265/1767: training loss=0.00\n",
      "Step 266/1767: training loss=0.00\n",
      "Step 267/1767: training loss=1.16\n",
      "Step 268/1767: training loss=1.26\n",
      "Step 269/1767: training loss=0.00\n",
      "Step 270/1767: training loss=0.00\n",
      "Step 271/1767: training loss=0.00\n",
      "Step 272/1767: training loss=2.03\n",
      "Step 273/1767: training loss=0.92\n",
      "Step 274/1767: training loss=0.84\n",
      "Step 275/1767: training loss=0.00\n",
      "Step 276/1767: training loss=0.00\n",
      "Step 277/1767: training loss=1.25\n",
      "Step 278/1767: training loss=0.00\n",
      "Step 279/1767: training loss=0.89\n",
      "Step 280/1767: training loss=0.00\n",
      "Step 281/1767: training loss=0.00\n",
      "Step 282/1767: training loss=0.00\n",
      "Step 283/1767: training loss=1.22\n",
      "Step 284/1767: training loss=0.00\n",
      "Step 285/1767: training loss=0.00\n",
      "Step 286/1767: training loss=1.64\n",
      "Step 287/1767: training loss=0.83\n",
      "Step 288/1767: training loss=0.07\n",
      "Step 289/1767: training loss=0.73\n",
      "Step 290/1767: training loss=0.00\n",
      "Step 291/1767: training loss=0.61\n",
      "Step 292/1767: training loss=0.90\n",
      "Step 293/1767: training loss=0.32\n",
      "Step 294/1767: training loss=0.01\n",
      "Step 295/1767: training loss=0.02\n",
      "Step 296/1767: training loss=0.27\n",
      "Step 297/1767: training loss=0.04\n",
      "Step 298/1767: training loss=0.12\n",
      "Step 299/1767: training loss=0.21\n",
      "Step 300/1767: training loss=0.00, validation loss=0.17\n",
      "Step 301/1767: training loss=0.05\n",
      "Step 302/1767: training loss=0.00\n",
      "Step 303/1767: training loss=0.00\n",
      "Step 304/1767: training loss=0.36\n",
      "Step 305/1767: training loss=0.00\n",
      "Step 306/1767: training loss=1.09\n",
      "Step 307/1767: training loss=0.00\n",
      "Step 308/1767: training loss=0.00\n",
      "Step 309/1767: training loss=0.00\n",
      "Step 310/1767: training loss=0.22\n",
      "Step 311/1767: training loss=1.10\n",
      "Step 312/1767: training loss=0.00\n",
      "Step 313/1767: training loss=0.00\n",
      "Step 314/1767: training loss=0.00\n",
      "Step 315/1767: training loss=0.75\n",
      "Step 316/1767: training loss=0.00\n",
      "Step 317/1767: training loss=0.00\n",
      "Step 318/1767: training loss=0.53\n",
      "Step 319/1767: training loss=0.00\n",
      "Step 320/1767: training loss=0.01\n",
      "Step 321/1767: training loss=0.00\n",
      "Step 322/1767: training loss=0.00\n",
      "Step 323/1767: training loss=0.00\n",
      "Step 324/1767: training loss=0.00\n",
      "Step 325/1767: training loss=0.00\n",
      "Step 326/1767: training loss=0.00\n",
      "Step 327/1767: training loss=0.00\n",
      "Step 328/1767: training loss=0.00\n",
      "Step 329/1767: training loss=2.65\n",
      "Step 330/1767: training loss=1.03\n",
      "Step 331/1767: training loss=0.00\n",
      "Step 332/1767: training loss=0.00\n",
      "Step 333/1767: training loss=0.00\n",
      "Step 334/1767: training loss=2.90\n",
      "Step 335/1767: training loss=0.00\n",
      "Step 336/1767: training loss=0.00\n",
      "Step 337/1767: training loss=4.08\n",
      "Step 338/1767: training loss=1.26\n",
      "Step 339/1767: training loss=0.00\n",
      "Step 340/1767: training loss=4.18\n",
      "Step 341/1767: training loss=1.25\n",
      "Step 342/1767: training loss=0.00\n",
      "Step 343/1767: training loss=2.85\n",
      "Step 344/1767: training loss=1.45\n",
      "Step 345/1767: training loss=1.65\n",
      "Step 346/1767: training loss=0.78\n",
      "Step 347/1767: training loss=0.00\n",
      "Step 348/1767: training loss=0.97\n",
      "Step 349/1767: training loss=0.00\n",
      "Step 350/1767: training loss=1.05\n",
      "Step 351/1767: training loss=0.00\n",
      "Step 352/1767: training loss=0.93\n",
      "Step 353/1767: training loss=0.00\n",
      "Step 354/1767: training loss=0.49\n",
      "Step 355/1767: training loss=0.71\n",
      "Step 356/1767: training loss=0.00\n",
      "Step 357/1767: training loss=0.79\n",
      "Step 358/1767: training loss=0.00\n",
      "Step 359/1767: training loss=0.59\n",
      "Step 360/1767: training loss=0.42\n",
      "Step 361/1767: training loss=0.12\n",
      "Step 362/1767: training loss=0.00\n",
      "Step 363/1767: training loss=0.05\n",
      "Step 364/1767: training loss=0.00\n",
      "Step 365/1767: training loss=0.00\n",
      "Step 366/1767: training loss=0.00\n",
      "Step 367/1767: training loss=0.34\n",
      "Step 368/1767: training loss=0.00\n",
      "Step 369/1767: training loss=0.62\n",
      "Step 370/1767: training loss=0.00\n",
      "Step 371/1767: training loss=0.86\n",
      "Step 372/1767: training loss=0.00\n",
      "Step 373/1767: training loss=0.31\n",
      "Step 374/1767: training loss=0.00\n",
      "Step 375/1767: training loss=0.95\n",
      "Step 376/1767: training loss=0.30\n",
      "Step 377/1767: training loss=0.01\n",
      "Step 378/1767: training loss=0.50\n",
      "Step 379/1767: training loss=0.00\n",
      "Step 380/1767: training loss=1.05\n",
      "Step 381/1767: training loss=0.00\n",
      "Step 382/1767: training loss=0.03\n",
      "Step 383/1767: training loss=0.52\n",
      "Step 384/1767: training loss=0.00\n",
      "Step 385/1767: training loss=0.59\n",
      "Step 386/1767: training loss=0.15\n",
      "Step 387/1767: training loss=0.00\n",
      "Step 388/1767: training loss=0.00\n",
      "Step 389/1767: training loss=0.01\n",
      "Step 390/1767: training loss=0.58\n",
      "Step 391/1767: training loss=0.00\n",
      "Step 392/1767: training loss=2.81\n",
      "Step 393/1767: training loss=0.17\n",
      "Step 394/1767: training loss=0.00\n",
      "Step 395/1767: training loss=0.73\n",
      "Step 396/1767: training loss=0.08\n",
      "Step 397/1767: training loss=0.00\n",
      "Step 398/1767: training loss=0.00\n",
      "Step 399/1767: training loss=0.12\n",
      "Step 400/1767: training loss=0.00, validation loss=1.21\n",
      "Step 401/1767: training loss=1.60\n",
      "Step 402/1767: training loss=0.00\n",
      "Step 403/1767: training loss=0.03\n",
      "Step 404/1767: training loss=0.00\n",
      "Step 405/1767: training loss=0.00\n",
      "Step 406/1767: training loss=0.00\n",
      "Step 407/1767: training loss=1.22\n",
      "Step 408/1767: training loss=0.00\n",
      "Step 409/1767: training loss=2.69\n",
      "Step 410/1767: training loss=0.90\n",
      "Step 411/1767: training loss=0.00\n",
      "Step 412/1767: training loss=0.12\n",
      "Step 413/1767: training loss=0.00\n",
      "Step 414/1767: training loss=0.00\n",
      "Step 415/1767: training loss=1.93\n",
      "Step 416/1767: training loss=1.25\n",
      "Step 417/1767: training loss=0.00\n",
      "Step 418/1767: training loss=0.28\n",
      "Step 419/1767: training loss=0.00\n",
      "Step 420/1767: training loss=0.06\n",
      "Step 421/1767: training loss=0.00\n",
      "Step 422/1767: training loss=0.42\n",
      "Step 423/1767: training loss=0.15\n",
      "Step 424/1767: training loss=0.00\n",
      "Step 425/1767: training loss=0.60\n",
      "Step 426/1767: training loss=0.00\n",
      "Step 427/1767: training loss=0.65\n",
      "Step 428/1767: training loss=0.00\n",
      "Step 429/1767: training loss=0.06\n",
      "Step 430/1767: training loss=1.32\n",
      "Step 431/1767: training loss=0.51\n",
      "Step 432/1767: training loss=1.58\n",
      "Step 433/1767: training loss=0.04\n",
      "Step 434/1767: training loss=1.12\n",
      "Step 435/1767: training loss=0.46\n",
      "Step 436/1767: training loss=0.00\n",
      "Step 437/1767: training loss=0.01\n",
      "Step 438/1767: training loss=1.22\n",
      "Step 439/1767: training loss=0.00\n",
      "Step 440/1767: training loss=0.36\n",
      "Step 441/1767: training loss=0.00\n",
      "Step 442/1767: training loss=0.19\n",
      "Step 443/1767: training loss=0.00\n",
      "Step 444/1767: training loss=0.00\n",
      "Step 445/1767: training loss=0.00\n",
      "Step 446/1767: training loss=1.03\n",
      "Step 447/1767: training loss=1.53\n",
      "Step 448/1767: training loss=1.33\n",
      "Step 449/1767: training loss=0.00\n",
      "Step 450/1767: training loss=0.00\n",
      "Step 451/1767: training loss=0.17\n",
      "Step 452/1767: training loss=0.00\n",
      "Step 453/1767: training loss=0.00\n",
      "Step 454/1767: training loss=0.16\n",
      "Step 455/1767: training loss=0.01\n",
      "Step 456/1767: training loss=1.29\n",
      "Step 457/1767: training loss=0.00\n",
      "Step 458/1767: training loss=1.38\n",
      "Step 459/1767: training loss=0.00\n",
      "Step 460/1767: training loss=1.13\n",
      "Step 461/1767: training loss=0.39\n",
      "Step 462/1767: training loss=0.00\n",
      "Step 463/1767: training loss=0.00\n",
      "Step 464/1767: training loss=0.00\n",
      "Step 465/1767: training loss=0.00\n",
      "Step 466/1767: training loss=1.43\n",
      "Step 467/1767: training loss=0.05\n",
      "Step 468/1767: training loss=0.00\n",
      "Step 469/1767: training loss=1.33\n",
      "Step 470/1767: training loss=0.29\n",
      "Step 471/1767: training loss=0.00\n",
      "Step 472/1767: training loss=0.00\n",
      "Step 473/1767: training loss=0.01\n",
      "Step 474/1767: training loss=2.41\n",
      "Step 475/1767: training loss=0.03\n",
      "Step 476/1767: training loss=0.00\n",
      "Step 477/1767: training loss=0.00\n",
      "Step 478/1767: training loss=0.00\n",
      "Step 479/1767: training loss=0.13\n",
      "Step 480/1767: training loss=1.61\n",
      "Step 481/1767: training loss=0.00\n",
      "Step 482/1767: training loss=0.98\n",
      "Step 483/1767: training loss=1.11\n",
      "Step 484/1767: training loss=0.00\n",
      "Step 485/1767: training loss=0.00\n",
      "Step 486/1767: training loss=0.00\n",
      "Step 487/1767: training loss=0.05\n",
      "Step 488/1767: training loss=0.53\n",
      "Step 489/1767: training loss=0.73\n",
      "Step 490/1767: training loss=0.00\n",
      "Step 491/1767: training loss=1.02\n",
      "Step 492/1767: training loss=0.00\n",
      "Step 493/1767: training loss=0.92\n",
      "Step 494/1767: training loss=0.13\n",
      "Step 495/1767: training loss=1.02\n",
      "Step 496/1767: training loss=0.00\n",
      "Step 497/1767: training loss=0.00\n",
      "Step 498/1767: training loss=0.00\n",
      "Step 499/1767: training loss=0.43\n",
      "Step 500/1767: training loss=0.40, validation loss=1.22\n",
      "Step 501/1767: training loss=0.00\n",
      "Step 502/1767: training loss=0.60\n",
      "Step 503/1767: training loss=0.88\n",
      "Step 504/1767: training loss=0.00\n",
      "Step 505/1767: training loss=0.00\n",
      "Step 506/1767: training loss=1.57\n",
      "Step 507/1767: training loss=0.47\n",
      "Step 508/1767: training loss=1.42\n",
      "Step 509/1767: training loss=0.00\n",
      "Step 510/1767: training loss=0.01\n",
      "Step 511/1767: training loss=2.20\n",
      "Step 512/1767: training loss=0.00\n",
      "Step 513/1767: training loss=0.00\n",
      "Step 514/1767: training loss=0.00\n",
      "Step 515/1767: training loss=1.10\n",
      "Step 516/1767: training loss=0.00\n",
      "Step 517/1767: training loss=0.00\n",
      "Step 518/1767: training loss=1.09\n",
      "Step 519/1767: training loss=0.00\n",
      "Step 520/1767: training loss=1.23\n",
      "Step 521/1767: training loss=0.57\n",
      "Step 522/1767: training loss=0.00\n",
      "Step 523/1767: training loss=0.00\n",
      "Step 524/1767: training loss=0.00\n",
      "Step 525/1767: training loss=0.07\n",
      "Step 526/1767: training loss=0.00\n",
      "Step 527/1767: training loss=0.00\n",
      "Step 528/1767: training loss=0.00\n",
      "Step 529/1767: training loss=0.00\n",
      "Step 530/1767: training loss=0.00\n",
      "Step 531/1767: training loss=0.62\n",
      "Step 532/1767: training loss=0.00\n",
      "Step 533/1767: training loss=0.00\n",
      "Step 534/1767: training loss=1.00\n",
      "Step 535/1767: training loss=0.00\n",
      "Step 536/1767: training loss=1.42\n",
      "Step 537/1767: training loss=0.00\n",
      "Step 538/1767: training loss=2.66\n",
      "Step 539/1767: training loss=0.00\n",
      "Step 540/1767: training loss=0.45\n",
      "Step 541/1767: training loss=0.00\n",
      "Step 542/1767: training loss=0.00\n",
      "Step 543/1767: training loss=0.00\n",
      "Step 544/1767: training loss=0.00\n",
      "Step 545/1767: training loss=0.00\n",
      "Step 546/1767: training loss=0.00\n",
      "Step 547/1767: training loss=2.70\n",
      "Step 548/1767: training loss=1.63\n",
      "Step 549/1767: training loss=1.92\n",
      "Step 550/1767: training loss=0.00\n",
      "Step 551/1767: training loss=1.13\n",
      "Step 552/1767: training loss=0.78\n",
      "Step 553/1767: training loss=0.97\n",
      "Step 554/1767: training loss=1.11\n",
      "Step 555/1767: training loss=0.00\n",
      "Step 556/1767: training loss=0.00\n",
      "Step 557/1767: training loss=0.00\n",
      "Step 558/1767: training loss=0.00\n",
      "Step 559/1767: training loss=0.00\n",
      "Step 560/1767: training loss=0.00\n",
      "Step 561/1767: training loss=0.00\n",
      "Step 562/1767: training loss=0.00\n",
      "Step 563/1767: training loss=0.47\n",
      "Step 564/1767: training loss=0.00\n",
      "Step 565/1767: training loss=0.17\n",
      "Step 566/1767: training loss=1.24\n",
      "Step 567/1767: training loss=0.00\n",
      "Step 568/1767: training loss=0.00\n",
      "Step 569/1767: training loss=0.00\n",
      "Step 570/1767: training loss=0.00\n",
      "Step 571/1767: training loss=3.86\n",
      "Step 572/1767: training loss=0.00\n",
      "Step 573/1767: training loss=1.88\n",
      "Step 574/1767: training loss=3.37\n",
      "Step 575/1767: training loss=3.43\n",
      "Step 576/1767: training loss=0.00\n",
      "Step 577/1767: training loss=1.62\n",
      "Step 578/1767: training loss=1.76\n",
      "Step 579/1767: training loss=1.44\n",
      "Step 580/1767: training loss=0.00\n",
      "Step 581/1767: training loss=0.19\n",
      "Step 582/1767: training loss=0.00\n",
      "Step 583/1767: training loss=0.00\n",
      "Step 584/1767: training loss=0.68\n",
      "Step 585/1767: training loss=0.77\n",
      "Step 586/1767: training loss=0.00\n",
      "Step 587/1767: training loss=0.00\n",
      "Step 588/1767: training loss=1.32\n",
      "Step 589/1767: training loss=0.86, full validation loss=0.33\n",
      "Step 590/1767: training loss=1.05\n",
      "Step 591/1767: training loss=0.01\n",
      "Step 592/1767: training loss=0.00\n",
      "Step 593/1767: training loss=0.00\n",
      "Step 594/1767: training loss=0.30\n",
      "Step 595/1767: training loss=0.00\n",
      "Step 596/1767: training loss=0.02\n",
      "Step 597/1767: training loss=0.00\n",
      "Step 598/1767: training loss=0.00\n",
      "Step 599/1767: training loss=0.00\n",
      "Step 600/1767: training loss=0.13, validation loss=0.00\n",
      "Step 601/1767: training loss=0.00\n",
      "Step 602/1767: training loss=0.98\n",
      "Step 603/1767: training loss=0.00\n",
      "Step 604/1767: training loss=0.24\n",
      "Step 605/1767: training loss=0.00\n",
      "Step 606/1767: training loss=1.19\n",
      "Step 607/1767: training loss=0.95\n",
      "Step 608/1767: training loss=0.03\n",
      "Step 609/1767: training loss=0.00\n",
      "Step 610/1767: training loss=0.00\n",
      "Step 611/1767: training loss=0.00\n",
      "Step 612/1767: training loss=0.00\n",
      "Step 613/1767: training loss=0.00\n",
      "Step 614/1767: training loss=0.00\n",
      "Step 615/1767: training loss=2.52\n",
      "Step 616/1767: training loss=0.00\n",
      "Step 617/1767: training loss=0.10\n",
      "Step 618/1767: training loss=0.00\n",
      "Step 619/1767: training loss=0.00\n",
      "Step 620/1767: training loss=0.00\n",
      "Step 621/1767: training loss=0.00\n",
      "Step 622/1767: training loss=0.29\n",
      "Step 623/1767: training loss=0.00\n",
      "Step 624/1767: training loss=0.00\n",
      "Step 625/1767: training loss=0.01\n",
      "Step 626/1767: training loss=0.00\n",
      "Step 627/1767: training loss=0.00\n",
      "Step 628/1767: training loss=1.85\n",
      "Step 629/1767: training loss=0.00\n",
      "Step 630/1767: training loss=1.99\n",
      "Step 631/1767: training loss=0.00\n",
      "Step 632/1767: training loss=0.00\n",
      "Step 633/1767: training loss=0.00\n",
      "Step 634/1767: training loss=0.00\n",
      "Step 635/1767: training loss=0.00\n",
      "Step 636/1767: training loss=0.12\n",
      "Step 637/1767: training loss=1.63\n",
      "Step 638/1767: training loss=1.75\n",
      "Step 639/1767: training loss=0.00\n",
      "Step 640/1767: training loss=0.00\n",
      "Step 641/1767: training loss=0.00\n",
      "Step 642/1767: training loss=2.49\n",
      "Step 643/1767: training loss=0.00\n",
      "Step 644/1767: training loss=0.00\n",
      "Step 645/1767: training loss=0.00\n",
      "Step 646/1767: training loss=1.34\n",
      "Step 647/1767: training loss=0.00\n",
      "Step 648/1767: training loss=1.14\n",
      "Step 649/1767: training loss=0.00\n",
      "Step 650/1767: training loss=1.28\n",
      "Step 651/1767: training loss=0.00\n",
      "Step 652/1767: training loss=0.00\n",
      "Step 653/1767: training loss=0.00\n",
      "Step 654/1767: training loss=0.00\n",
      "Step 655/1767: training loss=2.20\n",
      "Step 656/1767: training loss=1.08\n",
      "Step 657/1767: training loss=1.14\n",
      "Step 658/1767: training loss=0.00\n",
      "Step 659/1767: training loss=1.14\n",
      "Step 660/1767: training loss=0.00\n",
      "Step 661/1767: training loss=0.00\n",
      "Step 662/1767: training loss=0.00\n",
      "Step 663/1767: training loss=0.00\n",
      "Step 664/1767: training loss=0.62\n",
      "Step 665/1767: training loss=0.00\n",
      "Step 666/1767: training loss=0.00\n",
      "Step 667/1767: training loss=0.00\n",
      "Step 668/1767: training loss=0.00\n",
      "Step 669/1767: training loss=0.00\n",
      "Step 670/1767: training loss=0.00\n",
      "Step 671/1767: training loss=0.00\n",
      "Step 672/1767: training loss=0.00\n",
      "Step 673/1767: training loss=0.00\n",
      "Step 674/1767: training loss=0.00\n",
      "Step 675/1767: training loss=2.94\n",
      "Step 676/1767: training loss=0.00\n",
      "Step 677/1767: training loss=0.00\n",
      "Step 678/1767: training loss=0.92\n",
      "Step 679/1767: training loss=0.87\n",
      "Step 680/1767: training loss=0.00\n",
      "Step 681/1767: training loss=0.00\n",
      "Step 682/1767: training loss=0.00\n",
      "Step 683/1767: training loss=3.07\n",
      "Step 684/1767: training loss=0.00\n",
      "Step 685/1767: training loss=1.96\n",
      "Step 686/1767: training loss=0.00\n",
      "Step 687/1767: training loss=0.74\n",
      "Step 688/1767: training loss=0.00\n",
      "Step 689/1767: training loss=0.86\n",
      "Step 690/1767: training loss=0.00\n",
      "Step 691/1767: training loss=0.65\n",
      "Step 692/1767: training loss=0.00\n",
      "Step 693/1767: training loss=0.00\n",
      "Step 694/1767: training loss=1.51\n",
      "Step 695/1767: training loss=0.20\n",
      "Step 696/1767: training loss=0.00\n",
      "Step 697/1767: training loss=0.00\n",
      "Step 698/1767: training loss=0.00\n",
      "Step 699/1767: training loss=0.00\n",
      "Step 700/1767: training loss=0.05, validation loss=1.73\n",
      "Step 701/1767: training loss=0.00\n",
      "Step 702/1767: training loss=1.03\n",
      "Step 703/1767: training loss=0.00\n",
      "Step 704/1767: training loss=2.46\n",
      "Step 705/1767: training loss=0.67\n",
      "Step 706/1767: training loss=1.51\n",
      "Step 707/1767: training loss=2.52\n",
      "Step 708/1767: training loss=0.00\n",
      "Step 709/1767: training loss=0.00\n",
      "Step 710/1767: training loss=0.94\n",
      "Step 711/1767: training loss=1.39\n",
      "Step 712/1767: training loss=0.00\n",
      "Step 713/1767: training loss=0.00\n",
      "Step 714/1767: training loss=1.04\n",
      "Step 715/1767: training loss=0.00\n",
      "Step 716/1767: training loss=0.00\n",
      "Step 717/1767: training loss=0.89\n",
      "Step 718/1767: training loss=0.00\n",
      "Step 719/1767: training loss=0.10\n",
      "Step 720/1767: training loss=2.08\n",
      "Step 721/1767: training loss=0.00\n",
      "Step 722/1767: training loss=0.00\n",
      "Step 723/1767: training loss=0.00\n",
      "Step 724/1767: training loss=1.18\n",
      "Step 725/1767: training loss=0.00\n",
      "Step 726/1767: training loss=2.76\n",
      "Step 727/1767: training loss=0.00\n",
      "Step 728/1767: training loss=1.16\n",
      "Step 729/1767: training loss=0.00\n",
      "Step 730/1767: training loss=0.98\n",
      "Step 731/1767: training loss=0.33\n",
      "Step 732/1767: training loss=0.00\n",
      "Step 733/1767: training loss=0.00\n",
      "Step 734/1767: training loss=0.00\n",
      "Step 735/1767: training loss=0.00\n",
      "Step 736/1767: training loss=0.32\n",
      "Step 737/1767: training loss=1.43\n",
      "Step 738/1767: training loss=0.00\n",
      "Step 739/1767: training loss=0.00\n",
      "Step 740/1767: training loss=2.91\n",
      "Step 741/1767: training loss=0.00\n",
      "Step 742/1767: training loss=1.33\n",
      "Step 743/1767: training loss=0.00\n",
      "Step 744/1767: training loss=1.87\n",
      "Step 745/1767: training loss=0.00\n",
      "Step 746/1767: training loss=0.02\n",
      "Step 747/1767: training loss=0.82\n",
      "Step 748/1767: training loss=1.45\n",
      "Step 749/1767: training loss=1.15\n",
      "Step 750/1767: training loss=0.00\n",
      "Step 751/1767: training loss=0.56\n",
      "Step 752/1767: training loss=1.12\n",
      "Step 753/1767: training loss=1.73\n",
      "Step 754/1767: training loss=0.00\n",
      "Step 755/1767: training loss=0.72\n",
      "Step 756/1767: training loss=0.00\n",
      "Step 757/1767: training loss=0.81\n",
      "Step 758/1767: training loss=0.00\n",
      "Step 759/1767: training loss=0.00\n",
      "Step 760/1767: training loss=0.00\n",
      "Step 761/1767: training loss=0.00\n",
      "Step 762/1767: training loss=0.00\n",
      "Step 763/1767: training loss=0.00\n",
      "Step 764/1767: training loss=0.00\n",
      "Step 765/1767: training loss=0.00\n",
      "Step 766/1767: training loss=0.00\n",
      "Step 767/1767: training loss=0.00\n",
      "Step 768/1767: training loss=1.68\n",
      "Step 769/1767: training loss=0.00\n",
      "Step 770/1767: training loss=0.00\n",
      "Step 771/1767: training loss=1.46\n",
      "Step 772/1767: training loss=0.00\n",
      "Step 773/1767: training loss=1.82\n",
      "Step 774/1767: training loss=0.00\n",
      "Step 775/1767: training loss=0.00\n",
      "Step 776/1767: training loss=0.00\n",
      "Step 777/1767: training loss=1.85\n",
      "Step 778/1767: training loss=0.00\n",
      "Step 779/1767: training loss=1.99\n",
      "Step 780/1767: training loss=0.00\n",
      "Step 781/1767: training loss=0.20\n",
      "Step 782/1767: training loss=0.00\n",
      "Step 783/1767: training loss=0.00\n",
      "Step 784/1767: training loss=0.00\n",
      "Step 785/1767: training loss=0.00\n",
      "Step 786/1767: training loss=0.00\n",
      "Step 787/1767: training loss=0.00\n",
      "Step 788/1767: training loss=0.00\n",
      "Step 789/1767: training loss=1.40\n",
      "Step 790/1767: training loss=1.93\n",
      "Step 791/1767: training loss=1.35\n",
      "Step 792/1767: training loss=0.00\n",
      "Step 793/1767: training loss=0.75\n",
      "Step 794/1767: training loss=1.45\n",
      "Step 795/1767: training loss=0.00\n",
      "Step 796/1767: training loss=0.00\n",
      "Step 797/1767: training loss=1.32\n",
      "Step 798/1767: training loss=0.00\n",
      "Step 799/1767: training loss=0.00\n",
      "Step 800/1767: training loss=1.49, validation loss=0.00\n",
      "Step 801/1767: training loss=0.00\n",
      "Step 802/1767: training loss=2.68\n",
      "Step 803/1767: training loss=0.00\n",
      "Step 804/1767: training loss=0.64\n",
      "Step 805/1767: training loss=1.34\n",
      "Step 806/1767: training loss=0.00\n",
      "Step 807/1767: training loss=1.22\n",
      "Step 808/1767: training loss=0.65\n",
      "Step 809/1767: training loss=2.40\n",
      "Step 810/1767: training loss=0.00\n",
      "Step 811/1767: training loss=1.53\n",
      "Step 812/1767: training loss=0.91\n",
      "Step 813/1767: training loss=0.00\n",
      "Step 814/1767: training loss=0.00\n",
      "Step 815/1767: training loss=0.97\n",
      "Step 816/1767: training loss=0.00\n",
      "Step 817/1767: training loss=1.31\n",
      "Step 818/1767: training loss=0.00\n",
      "Step 819/1767: training loss=0.00\n",
      "Step 820/1767: training loss=0.00\n",
      "Step 821/1767: training loss=0.00\n",
      "Step 822/1767: training loss=0.00\n",
      "Step 823/1767: training loss=0.00\n",
      "Step 824/1767: training loss=0.00\n",
      "Step 825/1767: training loss=0.00\n",
      "Step 826/1767: training loss=0.00\n",
      "Step 827/1767: training loss=0.00\n",
      "Step 828/1767: training loss=2.45\n",
      "Step 829/1767: training loss=0.00\n",
      "Step 830/1767: training loss=1.02\n",
      "Step 831/1767: training loss=0.00\n",
      "Step 832/1767: training loss=1.27\n",
      "Step 833/1767: training loss=0.00\n",
      "Step 834/1767: training loss=0.00\n",
      "Step 835/1767: training loss=1.47\n",
      "Step 836/1767: training loss=0.00\n",
      "Step 837/1767: training loss=0.00\n",
      "Step 838/1767: training loss=0.00\n",
      "Step 839/1767: training loss=0.00\n",
      "Step 840/1767: training loss=0.00\n",
      "Step 841/1767: training loss=1.21\n",
      "Step 842/1767: training loss=0.00\n",
      "Step 843/1767: training loss=0.00\n",
      "Step 844/1767: training loss=2.86\n",
      "Step 845/1767: training loss=0.00\n",
      "Step 846/1767: training loss=0.00\n",
      "Step 847/1767: training loss=0.00\n",
      "Step 848/1767: training loss=0.00\n",
      "Step 849/1767: training loss=1.65\n",
      "Step 850/1767: training loss=0.00\n",
      "Step 851/1767: training loss=0.00\n",
      "Step 852/1767: training loss=0.00\n",
      "Step 853/1767: training loss=1.69\n",
      "Step 854/1767: training loss=0.00\n",
      "Step 855/1767: training loss=2.24\n",
      "Step 856/1767: training loss=0.00\n",
      "Step 857/1767: training loss=1.37\n",
      "Step 858/1767: training loss=0.00\n",
      "Step 859/1767: training loss=0.00\n",
      "Step 860/1767: training loss=0.00\n",
      "Step 861/1767: training loss=0.00\n",
      "Step 862/1767: training loss=0.75\n",
      "Step 863/1767: training loss=0.00\n",
      "Step 864/1767: training loss=0.00\n",
      "Step 865/1767: training loss=2.02\n",
      "Step 866/1767: training loss=0.00\n",
      "Step 867/1767: training loss=0.00\n",
      "Step 868/1767: training loss=0.13\n",
      "Step 869/1767: training loss=1.14\n",
      "Step 870/1767: training loss=0.00\n",
      "Step 871/1767: training loss=2.49\n",
      "Step 872/1767: training loss=1.06\n",
      "Step 873/1767: training loss=0.00\n",
      "Step 874/1767: training loss=0.00\n",
      "Step 875/1767: training loss=0.00\n",
      "Step 876/1767: training loss=0.00\n",
      "Step 877/1767: training loss=0.00\n",
      "Step 878/1767: training loss=0.00\n",
      "Step 879/1767: training loss=0.91\n",
      "Step 880/1767: training loss=0.00\n",
      "Step 881/1767: training loss=0.00\n",
      "Step 882/1767: training loss=1.03\n",
      "Step 883/1767: training loss=0.00\n",
      "Step 884/1767: training loss=1.31\n",
      "Step 885/1767: training loss=1.25\n",
      "Step 886/1767: training loss=0.15\n",
      "Step 887/1767: training loss=0.00\n",
      "Step 888/1767: training loss=0.74\n",
      "Step 889/1767: training loss=0.95\n",
      "Step 890/1767: training loss=0.00\n",
      "Step 891/1767: training loss=0.00\n",
      "Step 892/1767: training loss=0.00\n",
      "Step 893/1767: training loss=0.00\n",
      "Step 894/1767: training loss=0.00\n",
      "Step 895/1767: training loss=0.00\n",
      "Step 896/1767: training loss=0.00\n",
      "Step 897/1767: training loss=2.07\n",
      "Step 898/1767: training loss=0.00\n",
      "Step 899/1767: training loss=0.00\n",
      "Step 900/1767: training loss=0.00, validation loss=0.00\n",
      "Step 901/1767: training loss=0.93\n",
      "Step 902/1767: training loss=0.00\n",
      "Step 903/1767: training loss=0.00\n",
      "Step 904/1767: training loss=0.00\n",
      "Step 905/1767: training loss=0.00\n",
      "Step 906/1767: training loss=0.00\n",
      "Step 907/1767: training loss=0.00\n",
      "Step 908/1767: training loss=0.00\n",
      "Step 909/1767: training loss=0.15\n",
      "Step 910/1767: training loss=1.29\n",
      "Step 911/1767: training loss=0.31\n",
      "Step 912/1767: training loss=0.00\n",
      "Step 913/1767: training loss=0.00\n",
      "Step 914/1767: training loss=0.00\n",
      "Step 915/1767: training loss=1.78\n",
      "Step 916/1767: training loss=0.79\n",
      "Step 917/1767: training loss=0.00\n",
      "Step 918/1767: training loss=0.00\n",
      "Step 919/1767: training loss=0.00\n",
      "Step 920/1767: training loss=0.00\n",
      "Step 921/1767: training loss=0.00\n",
      "Step 922/1767: training loss=0.00\n",
      "Step 923/1767: training loss=1.49\n",
      "Step 924/1767: training loss=1.33\n",
      "Step 925/1767: training loss=1.47\n",
      "Step 926/1767: training loss=0.00\n",
      "Step 927/1767: training loss=0.56\n",
      "Step 928/1767: training loss=1.35\n",
      "Step 929/1767: training loss=0.00\n",
      "Step 930/1767: training loss=1.56\n",
      "Step 931/1767: training loss=1.54\n",
      "Step 932/1767: training loss=0.00\n",
      "Step 933/1767: training loss=0.00\n",
      "Step 934/1767: training loss=0.00\n",
      "Step 935/1767: training loss=0.50\n",
      "Step 936/1767: training loss=1.15\n",
      "Step 937/1767: training loss=1.22\n",
      "Step 938/1767: training loss=0.00\n",
      "Step 939/1767: training loss=0.92\n",
      "Step 940/1767: training loss=0.00\n",
      "Step 941/1767: training loss=0.00\n",
      "Step 942/1767: training loss=0.97\n",
      "Step 943/1767: training loss=0.00\n",
      "Step 944/1767: training loss=0.60\n",
      "Step 945/1767: training loss=0.00\n",
      "Step 946/1767: training loss=1.63\n",
      "Step 947/1767: training loss=0.00\n",
      "Step 948/1767: training loss=0.00\n",
      "Step 949/1767: training loss=0.00\n",
      "Step 950/1767: training loss=0.00\n",
      "Step 951/1767: training loss=0.00\n",
      "Step 952/1767: training loss=0.00\n",
      "Step 953/1767: training loss=0.58\n",
      "Step 954/1767: training loss=0.00\n",
      "Step 955/1767: training loss=0.76\n",
      "Step 956/1767: training loss=0.00\n",
      "Step 957/1767: training loss=1.56\n",
      "Step 958/1767: training loss=0.00\n",
      "Step 959/1767: training loss=0.00\n",
      "Step 960/1767: training loss=0.00\n",
      "Step 961/1767: training loss=0.00\n",
      "Step 962/1767: training loss=0.00\n",
      "Step 963/1767: training loss=0.00\n",
      "Step 964/1767: training loss=0.00\n",
      "Step 965/1767: training loss=0.00\n",
      "Step 966/1767: training loss=1.39\n",
      "Step 967/1767: training loss=0.00\n",
      "Step 968/1767: training loss=0.00\n",
      "Step 969/1767: training loss=0.00\n",
      "Step 970/1767: training loss=0.00\n",
      "Step 971/1767: training loss=1.23\n",
      "Step 972/1767: training loss=0.00\n",
      "Step 973/1767: training loss=0.00\n",
      "Step 974/1767: training loss=0.00\n",
      "Step 975/1767: training loss=0.00\n",
      "Step 976/1767: training loss=2.44\n",
      "Step 977/1767: training loss=1.74\n",
      "Step 978/1767: training loss=0.00\n",
      "Step 979/1767: training loss=1.52\n",
      "Step 980/1767: training loss=0.00\n",
      "Step 981/1767: training loss=0.00\n",
      "Step 982/1767: training loss=0.00\n",
      "Step 983/1767: training loss=0.79\n",
      "Step 984/1767: training loss=1.75\n",
      "Step 985/1767: training loss=0.00\n",
      "Step 986/1767: training loss=0.00\n",
      "Step 987/1767: training loss=1.76\n",
      "Step 988/1767: training loss=0.50\n",
      "Step 989/1767: training loss=0.00\n",
      "Step 990/1767: training loss=0.00\n",
      "Step 991/1767: training loss=0.00\n",
      "Step 992/1767: training loss=0.00\n",
      "Step 993/1767: training loss=1.76\n",
      "Step 994/1767: training loss=1.63\n",
      "Step 995/1767: training loss=1.33\n",
      "Step 996/1767: training loss=0.00\n",
      "Step 997/1767: training loss=0.00\n",
      "Step 998/1767: training loss=0.00\n",
      "Step 999/1767: training loss=0.29\n",
      "Step 1000/1767: training loss=0.00, validation loss=2.00\n",
      "Step 1001/1767: training loss=1.10\n",
      "Step 1002/1767: training loss=0.00\n",
      "Step 1003/1767: training loss=1.39\n",
      "Step 1004/1767: training loss=0.17\n",
      "Step 1005/1767: training loss=0.00\n",
      "Step 1006/1767: training loss=0.79\n",
      "Step 1007/1767: training loss=0.00\n",
      "Step 1008/1767: training loss=1.10\n",
      "Step 1009/1767: training loss=0.59\n",
      "Step 1010/1767: training loss=0.00\n",
      "Step 1011/1767: training loss=0.00\n",
      "Step 1012/1767: training loss=1.12\n",
      "Step 1013/1767: training loss=0.00\n",
      "Step 1014/1767: training loss=1.31\n",
      "Step 1015/1767: training loss=1.35\n",
      "Step 1016/1767: training loss=0.00\n",
      "Step 1017/1767: training loss=2.16\n",
      "Step 1018/1767: training loss=0.00\n",
      "Step 1019/1767: training loss=0.11\n",
      "Step 1020/1767: training loss=0.00\n",
      "Step 1021/1767: training loss=0.08\n",
      "Step 1022/1767: training loss=0.01\n",
      "Step 1023/1767: training loss=0.00\n",
      "Step 1024/1767: training loss=0.00\n",
      "Step 1025/1767: training loss=0.87\n",
      "Step 1026/1767: training loss=0.00\n",
      "Step 1027/1767: training loss=0.00\n",
      "Step 1028/1767: training loss=0.00\n",
      "Step 1029/1767: training loss=0.00\n",
      "Step 1030/1767: training loss=0.00\n",
      "Step 1031/1767: training loss=0.00\n",
      "Step 1032/1767: training loss=0.00\n",
      "Step 1033/1767: training loss=0.00\n",
      "Step 1034/1767: training loss=0.00\n",
      "Step 1035/1767: training loss=0.89\n",
      "Step 1036/1767: training loss=0.00\n",
      "Step 1037/1767: training loss=0.78\n",
      "Step 1038/1767: training loss=0.00\n",
      "Step 1039/1767: training loss=0.00\n",
      "Step 1040/1767: training loss=0.00\n",
      "Step 1041/1767: training loss=1.66\n",
      "Step 1042/1767: training loss=0.00\n",
      "Step 1043/1767: training loss=0.00\n",
      "Step 1044/1767: training loss=0.00\n",
      "Step 1045/1767: training loss=0.00\n",
      "Step 1046/1767: training loss=0.00\n",
      "Step 1047/1767: training loss=0.00\n",
      "Step 1048/1767: training loss=0.00\n",
      "Step 1049/1767: training loss=0.00\n",
      "Step 1050/1767: training loss=0.00\n",
      "Step 1051/1767: training loss=0.27\n",
      "Step 1052/1767: training loss=0.00\n",
      "Step 1053/1767: training loss=0.00\n",
      "Step 1054/1767: training loss=0.00\n",
      "Step 1055/1767: training loss=0.00\n",
      "Step 1056/1767: training loss=0.00\n",
      "Step 1057/1767: training loss=0.00\n",
      "Step 1058/1767: training loss=0.00\n",
      "Step 1059/1767: training loss=0.00\n",
      "Step 1060/1767: training loss=0.00\n",
      "Step 1061/1767: training loss=0.00\n",
      "Step 1062/1767: training loss=0.00\n",
      "Step 1063/1767: training loss=0.00\n",
      "Step 1064/1767: training loss=0.00\n",
      "Step 1065/1767: training loss=0.00\n",
      "Step 1066/1767: training loss=1.95\n",
      "Step 1067/1767: training loss=0.00\n",
      "Step 1068/1767: training loss=0.00\n",
      "Step 1069/1767: training loss=1.31\n",
      "Step 1070/1767: training loss=0.00\n",
      "Step 1071/1767: training loss=0.94\n",
      "Step 1072/1767: training loss=0.00\n",
      "Step 1073/1767: training loss=0.00\n",
      "Step 1074/1767: training loss=0.00\n",
      "Step 1075/1767: training loss=0.00\n",
      "Step 1076/1767: training loss=1.59\n",
      "Step 1077/1767: training loss=0.00\n",
      "Step 1078/1767: training loss=0.26\n",
      "Step 1079/1767: training loss=0.00\n",
      "Step 1080/1767: training loss=0.00\n",
      "Step 1081/1767: training loss=0.00\n",
      "Step 1082/1767: training loss=0.00\n",
      "Step 1083/1767: training loss=0.09\n",
      "Step 1084/1767: training loss=0.00\n",
      "Step 1085/1767: training loss=1.53\n",
      "Step 1086/1767: training loss=0.00\n",
      "Step 1087/1767: training loss=0.00\n",
      "Step 1088/1767: training loss=0.00\n",
      "Step 1089/1767: training loss=0.00\n",
      "Step 1090/1767: training loss=0.00\n",
      "Step 1091/1767: training loss=0.00\n",
      "Step 1092/1767: training loss=0.88\n",
      "Step 1093/1767: training loss=2.84\n",
      "Step 1094/1767: training loss=0.00\n",
      "Step 1095/1767: training loss=0.07\n",
      "Step 1096/1767: training loss=3.07\n",
      "Step 1097/1767: training loss=0.00\n",
      "Step 1098/1767: training loss=0.57\n",
      "Step 1099/1767: training loss=0.00\n",
      "Step 1100/1767: training loss=0.00, validation loss=0.00\n",
      "Step 1101/1767: training loss=1.03\n",
      "Step 1102/1767: training loss=0.00\n",
      "Step 1103/1767: training loss=1.36\n",
      "Step 1104/1767: training loss=1.50\n",
      "Step 1105/1767: training loss=0.00\n",
      "Step 1106/1767: training loss=0.00\n",
      "Step 1107/1767: training loss=0.00\n",
      "Step 1108/1767: training loss=1.23\n",
      "Step 1109/1767: training loss=0.00\n",
      "Step 1110/1767: training loss=0.57\n",
      "Step 1111/1767: training loss=1.66\n",
      "Step 1112/1767: training loss=0.01\n",
      "Step 1113/1767: training loss=0.00\n",
      "Step 1114/1767: training loss=1.24\n",
      "Step 1115/1767: training loss=0.01\n",
      "Step 1116/1767: training loss=1.47\n",
      "Step 1117/1767: training loss=0.00\n",
      "Step 1118/1767: training loss=0.00\n",
      "Step 1119/1767: training loss=0.00\n",
      "Step 1120/1767: training loss=0.00\n",
      "Step 1121/1767: training loss=0.00\n",
      "Step 1122/1767: training loss=0.00\n",
      "Step 1123/1767: training loss=0.00\n",
      "Step 1124/1767: training loss=0.00\n",
      "Step 1125/1767: training loss=2.19\n",
      "Step 1126/1767: training loss=0.00\n",
      "Step 1127/1767: training loss=0.00\n",
      "Step 1128/1767: training loss=2.18\n",
      "Step 1129/1767: training loss=0.00\n",
      "Step 1130/1767: training loss=0.00\n",
      "Step 1131/1767: training loss=0.00\n",
      "Step 1132/1767: training loss=0.00\n",
      "Step 1133/1767: training loss=0.33\n",
      "Step 1134/1767: training loss=0.00\n",
      "Step 1135/1767: training loss=0.01\n",
      "Step 1136/1767: training loss=0.57\n",
      "Step 1137/1767: training loss=1.88\n",
      "Step 1138/1767: training loss=0.00\n",
      "Step 1139/1767: training loss=0.00\n",
      "Step 1140/1767: training loss=0.00\n",
      "Step 1141/1767: training loss=0.00\n",
      "Step 1142/1767: training loss=0.00\n",
      "Step 1143/1767: training loss=0.00\n",
      "Step 1144/1767: training loss=0.00\n",
      "Step 1145/1767: training loss=0.00\n",
      "Step 1146/1767: training loss=1.04\n",
      "Step 1147/1767: training loss=0.00\n",
      "Step 1148/1767: training loss=1.67\n",
      "Step 1149/1767: training loss=1.95\n",
      "Step 1150/1767: training loss=0.00\n",
      "Step 1151/1767: training loss=0.00\n",
      "Step 1152/1767: training loss=1.54\n",
      "Step 1153/1767: training loss=0.00\n",
      "Step 1154/1767: training loss=0.00\n",
      "Step 1155/1767: training loss=0.00\n",
      "Step 1156/1767: training loss=0.00\n",
      "Step 1157/1767: training loss=1.71\n",
      "Step 1158/1767: training loss=0.00\n",
      "Step 1159/1767: training loss=1.29\n",
      "Step 1160/1767: training loss=1.48\n",
      "Step 1161/1767: training loss=0.23\n",
      "Step 1162/1767: training loss=1.10\n",
      "Step 1163/1767: training loss=1.31\n",
      "Step 1164/1767: training loss=0.21\n",
      "Step 1165/1767: training loss=0.00\n",
      "Step 1166/1767: training loss=0.00\n",
      "Step 1167/1767: training loss=0.00\n",
      "Step 1168/1767: training loss=0.00\n",
      "Step 1169/1767: training loss=0.00\n",
      "Step 1170/1767: training loss=1.11\n",
      "Step 1171/1767: training loss=1.18\n",
      "Step 1172/1767: training loss=0.00\n",
      "Step 1173/1767: training loss=0.00\n",
      "Step 1174/1767: training loss=0.00\n",
      "Step 1175/1767: training loss=1.18\n",
      "Step 1176/1767: training loss=0.00\n",
      "Step 1177/1767: training loss=0.00\n",
      "Step 1178/1767: training loss=0.00, full validation loss=0.62\n",
      "Step 1179/1767: training loss=0.00\n",
      "Step 1180/1767: training loss=0.00\n",
      "Step 1181/1767: training loss=0.00\n",
      "Step 1182/1767: training loss=1.57\n",
      "Step 1183/1767: training loss=1.51\n",
      "Step 1184/1767: training loss=0.00\n",
      "Step 1185/1767: training loss=0.00\n",
      "Step 1186/1767: training loss=0.00\n",
      "Step 1187/1767: training loss=0.00\n",
      "Step 1188/1767: training loss=1.45\n",
      "Step 1189/1767: training loss=0.00\n",
      "Step 1190/1767: training loss=1.41\n",
      "Step 1191/1767: training loss=0.00\n",
      "Step 1192/1767: training loss=0.00\n",
      "Step 1193/1767: training loss=0.46\n",
      "Step 1194/1767: training loss=1.63\n",
      "Step 1195/1767: training loss=0.00\n",
      "Step 1196/1767: training loss=0.00\n",
      "Step 1197/1767: training loss=0.00\n",
      "Step 1198/1767: training loss=0.99\n",
      "Step 1199/1767: training loss=1.53\n",
      "Step 1200/1767: training loss=1.35, validation loss=1.21\n",
      "Step 1201/1767: training loss=1.13\n",
      "Step 1202/1767: training loss=0.00\n",
      "Step 1203/1767: training loss=1.14\n",
      "Step 1204/1767: training loss=0.88\n",
      "Step 1205/1767: training loss=0.00\n",
      "Step 1206/1767: training loss=0.00\n",
      "Step 1207/1767: training loss=0.00\n",
      "Step 1208/1767: training loss=0.00\n",
      "Step 1209/1767: training loss=1.59\n",
      "Step 1210/1767: training loss=0.00\n",
      "Step 1211/1767: training loss=0.00\n",
      "Step 1212/1767: training loss=0.00\n",
      "Step 1213/1767: training loss=0.78\n",
      "Step 1214/1767: training loss=1.36\n",
      "Step 1215/1767: training loss=0.00\n",
      "Step 1216/1767: training loss=0.00\n",
      "Step 1217/1767: training loss=0.00\n",
      "Step 1218/1767: training loss=0.00\n",
      "Step 1219/1767: training loss=1.56\n",
      "Step 1220/1767: training loss=0.00\n",
      "Step 1221/1767: training loss=0.00\n",
      "Step 1222/1767: training loss=0.00\n",
      "Step 1223/1767: training loss=1.42\n",
      "Step 1224/1767: training loss=0.00\n",
      "Step 1225/1767: training loss=0.00\n",
      "Step 1226/1767: training loss=0.00\n",
      "Step 1227/1767: training loss=0.00\n",
      "Step 1228/1767: training loss=0.00\n",
      "Step 1229/1767: training loss=0.00\n",
      "Step 1230/1767: training loss=0.64\n",
      "Step 1231/1767: training loss=0.00\n",
      "Step 1232/1767: training loss=0.00\n",
      "Step 1233/1767: training loss=0.00\n",
      "Step 1234/1767: training loss=0.00\n",
      "Step 1235/1767: training loss=0.00\n",
      "Step 1236/1767: training loss=0.00\n",
      "Step 1237/1767: training loss=0.00\n",
      "Step 1238/1767: training loss=0.00\n",
      "Step 1239/1767: training loss=0.00\n",
      "Step 1240/1767: training loss=0.00\n",
      "Step 1241/1767: training loss=1.85\n",
      "Step 1242/1767: training loss=0.00\n",
      "Step 1243/1767: training loss=0.00\n",
      "Step 1244/1767: training loss=0.00\n",
      "Step 1245/1767: training loss=0.00\n",
      "Step 1246/1767: training loss=0.00\n",
      "Step 1247/1767: training loss=0.00\n",
      "Step 1248/1767: training loss=0.00\n",
      "Step 1249/1767: training loss=0.00\n",
      "Step 1250/1767: training loss=1.50\n",
      "Step 1251/1767: training loss=0.00\n",
      "Step 1252/1767: training loss=0.00\n",
      "Step 1253/1767: training loss=3.82\n",
      "Step 1254/1767: training loss=0.91\n",
      "Step 1255/1767: training loss=0.00\n",
      "Step 1256/1767: training loss=0.00\n",
      "Step 1257/1767: training loss=1.49\n",
      "Step 1258/1767: training loss=0.00\n",
      "Step 1259/1767: training loss=0.00\n",
      "Step 1260/1767: training loss=1.97\n",
      "Step 1261/1767: training loss=0.00\n",
      "Step 1262/1767: training loss=0.00\n",
      "Step 1263/1767: training loss=0.00\n",
      "Step 1264/1767: training loss=2.44\n",
      "Step 1265/1767: training loss=0.54\n",
      "Step 1266/1767: training loss=0.00\n",
      "Step 1267/1767: training loss=0.00\n",
      "Step 1268/1767: training loss=0.12\n",
      "Step 1269/1767: training loss=1.32\n",
      "Step 1270/1767: training loss=0.00\n",
      "Step 1271/1767: training loss=0.04\n",
      "Step 1272/1767: training loss=0.00\n",
      "Step 1273/1767: training loss=0.00\n",
      "Step 1274/1767: training loss=0.00\n",
      "Step 1275/1767: training loss=1.15\n",
      "Step 1276/1767: training loss=1.37\n",
      "Step 1277/1767: training loss=0.00\n",
      "Step 1278/1767: training loss=0.22\n",
      "Step 1279/1767: training loss=0.00\n",
      "Step 1280/1767: training loss=0.00\n",
      "Step 1281/1767: training loss=0.00\n",
      "Step 1282/1767: training loss=0.00\n",
      "Step 1283/1767: training loss=0.00\n",
      "Step 1284/1767: training loss=1.66\n",
      "Step 1285/1767: training loss=1.81\n",
      "Step 1286/1767: training loss=0.00\n",
      "Step 1287/1767: training loss=0.55\n",
      "Step 1288/1767: training loss=0.00\n",
      "Step 1289/1767: training loss=0.00\n",
      "Step 1290/1767: training loss=0.00\n",
      "Step 1291/1767: training loss=1.60\n",
      "Step 1292/1767: training loss=0.00\n",
      "Step 1293/1767: training loss=0.00\n",
      "Step 1294/1767: training loss=0.00\n",
      "Step 1295/1767: training loss=0.00\n",
      "Step 1296/1767: training loss=1.33\n",
      "Step 1297/1767: training loss=1.59\n",
      "Step 1298/1767: training loss=0.00\n",
      "Step 1299/1767: training loss=1.44\n",
      "Step 1300/1767: training loss=0.00, validation loss=0.00\n",
      "Step 1301/1767: training loss=0.60\n",
      "Step 1302/1767: training loss=0.00\n",
      "Step 1303/1767: training loss=0.00\n",
      "Step 1304/1767: training loss=0.00\n",
      "Step 1305/1767: training loss=0.00\n",
      "Step 1306/1767: training loss=0.00\n",
      "Step 1307/1767: training loss=0.00\n",
      "Step 1308/1767: training loss=0.00\n",
      "Step 1309/1767: training loss=0.00\n",
      "Step 1310/1767: training loss=0.00\n",
      "Step 1311/1767: training loss=0.00\n",
      "Step 1312/1767: training loss=0.00\n",
      "Step 1313/1767: training loss=0.00\n",
      "Step 1314/1767: training loss=0.00\n",
      "Step 1315/1767: training loss=0.00\n",
      "Step 1316/1767: training loss=0.00\n",
      "Step 1317/1767: training loss=0.00\n",
      "Step 1318/1767: training loss=1.46\n",
      "Step 1319/1767: training loss=0.04\n",
      "Step 1320/1767: training loss=0.00\n",
      "Step 1321/1767: training loss=0.00\n",
      "Step 1322/1767: training loss=0.00\n",
      "Step 1323/1767: training loss=0.00\n",
      "Step 1324/1767: training loss=0.00\n",
      "Step 1325/1767: training loss=1.84\n",
      "Step 1326/1767: training loss=0.00\n",
      "Step 1327/1767: training loss=0.00\n",
      "Step 1328/1767: training loss=0.00\n",
      "Step 1329/1767: training loss=1.62\n",
      "Step 1330/1767: training loss=0.00\n",
      "Step 1331/1767: training loss=0.00\n",
      "Step 1332/1767: training loss=0.00\n",
      "Step 1333/1767: training loss=0.17\n",
      "Step 1334/1767: training loss=0.00\n",
      "Step 1335/1767: training loss=2.87\n",
      "Step 1336/1767: training loss=0.00\n",
      "Step 1337/1767: training loss=0.00\n",
      "Step 1338/1767: training loss=0.50\n",
      "Step 1339/1767: training loss=0.00\n",
      "Step 1340/1767: training loss=0.00\n",
      "Step 1341/1767: training loss=0.00\n",
      "Step 1342/1767: training loss=0.00\n",
      "Step 1343/1767: training loss=0.00\n",
      "Step 1344/1767: training loss=1.57\n",
      "Step 1345/1767: training loss=0.24\n",
      "Step 1346/1767: training loss=0.00\n",
      "Step 1347/1767: training loss=0.00\n",
      "Step 1348/1767: training loss=0.00\n",
      "Step 1349/1767: training loss=0.00\n",
      "Step 1350/1767: training loss=1.70\n",
      "Step 1351/1767: training loss=1.18\n",
      "Step 1352/1767: training loss=0.00\n",
      "Step 1353/1767: training loss=0.00\n",
      "Step 1354/1767: training loss=0.00\n",
      "Step 1355/1767: training loss=1.39\n",
      "Step 1356/1767: training loss=0.00\n",
      "Step 1357/1767: training loss=1.91\n",
      "Step 1358/1767: training loss=0.00\n",
      "Step 1359/1767: training loss=1.43\n",
      "Step 1360/1767: training loss=0.00\n",
      "Step 1361/1767: training loss=0.00\n",
      "Step 1362/1767: training loss=0.00\n",
      "Step 1363/1767: training loss=1.30\n",
      "Step 1364/1767: training loss=0.00\n",
      "Step 1365/1767: training loss=0.32\n",
      "Step 1366/1767: training loss=0.00\n",
      "Step 1367/1767: training loss=0.00\n",
      "Step 1368/1767: training loss=0.00\n",
      "Step 1369/1767: training loss=0.86\n",
      "Step 1370/1767: training loss=1.62\n",
      "Step 1371/1767: training loss=0.00\n",
      "Step 1372/1767: training loss=1.43\n",
      "Step 1373/1767: training loss=0.00\n",
      "Step 1374/1767: training loss=0.00\n",
      "Step 1375/1767: training loss=0.00\n",
      "Step 1376/1767: training loss=1.70\n",
      "Step 1377/1767: training loss=0.00\n",
      "Step 1378/1767: training loss=1.17\n",
      "Step 1379/1767: training loss=0.00\n",
      "Step 1380/1767: training loss=0.00\n",
      "Step 1381/1767: training loss=0.00\n",
      "Step 1382/1767: training loss=0.00\n",
      "Step 1383/1767: training loss=0.00\n",
      "Step 1384/1767: training loss=0.00\n",
      "Step 1385/1767: training loss=0.00\n",
      "Step 1386/1767: training loss=0.00\n",
      "Step 1387/1767: training loss=0.00\n",
      "Step 1388/1767: training loss=0.00\n",
      "Step 1389/1767: training loss=0.00\n",
      "Step 1390/1767: training loss=0.00\n",
      "Step 1391/1767: training loss=0.00\n",
      "Step 1392/1767: training loss=0.64\n",
      "Step 1393/1767: training loss=1.44\n",
      "Step 1394/1767: training loss=0.00\n",
      "Step 1395/1767: training loss=0.00\n",
      "Step 1396/1767: training loss=0.00\n",
      "Step 1397/1767: training loss=0.00\n",
      "Step 1398/1767: training loss=0.00\n",
      "Step 1399/1767: training loss=0.00\n",
      "Step 1400/1767: training loss=1.24, validation loss=0.00\n",
      "Step 1401/1767: training loss=0.00\n",
      "Step 1402/1767: training loss=0.00\n",
      "Step 1403/1767: training loss=0.00\n",
      "Step 1404/1767: training loss=0.00\n",
      "Step 1405/1767: training loss=0.00\n",
      "Step 1406/1767: training loss=0.00\n",
      "Step 1407/1767: training loss=0.00\n",
      "Step 1408/1767: training loss=0.74\n",
      "Step 1409/1767: training loss=0.00\n",
      "Step 1410/1767: training loss=0.38\n",
      "Step 1411/1767: training loss=1.31\n",
      "Step 1412/1767: training loss=0.00\n",
      "Step 1413/1767: training loss=0.00\n",
      "Step 1414/1767: training loss=1.10\n",
      "Step 1415/1767: training loss=0.00\n",
      "Step 1416/1767: training loss=0.44\n",
      "Step 1417/1767: training loss=0.00\n",
      "Step 1418/1767: training loss=0.16\n",
      "Step 1419/1767: training loss=0.00\n",
      "Step 1420/1767: training loss=0.00\n",
      "Step 1421/1767: training loss=0.00\n",
      "Step 1422/1767: training loss=0.00\n",
      "Step 1423/1767: training loss=0.58\n",
      "Step 1424/1767: training loss=1.46\n",
      "Step 1425/1767: training loss=1.44\n",
      "Step 1426/1767: training loss=0.00\n",
      "Step 1427/1767: training loss=0.00\n",
      "Step 1428/1767: training loss=0.00\n",
      "Step 1429/1767: training loss=0.00\n",
      "Step 1430/1767: training loss=0.00\n",
      "Step 1431/1767: training loss=0.00\n",
      "Step 1432/1767: training loss=1.58\n",
      "Step 1433/1767: training loss=0.00\n",
      "Step 1434/1767: training loss=0.62\n",
      "Step 1435/1767: training loss=0.00\n",
      "Step 1436/1767: training loss=0.00\n",
      "Step 1437/1767: training loss=0.00\n",
      "Step 1438/1767: training loss=0.00\n",
      "Step 1439/1767: training loss=0.00\n",
      "Step 1440/1767: training loss=0.00\n",
      "Step 1441/1767: training loss=0.04\n",
      "Step 1442/1767: training loss=0.66\n",
      "Step 1443/1767: training loss=0.00\n",
      "Step 1444/1767: training loss=0.00\n",
      "Step 1445/1767: training loss=1.45\n",
      "Step 1446/1767: training loss=0.25\n",
      "Step 1447/1767: training loss=0.00\n",
      "Step 1448/1767: training loss=0.00\n",
      "Step 1449/1767: training loss=0.00\n",
      "Step 1450/1767: training loss=0.00\n",
      "Step 1451/1767: training loss=0.00\n",
      "Step 1452/1767: training loss=0.00\n",
      "Step 1453/1767: training loss=0.00\n",
      "Step 1454/1767: training loss=0.00\n",
      "Step 1455/1767: training loss=0.00\n",
      "Step 1456/1767: training loss=0.00\n",
      "Step 1457/1767: training loss=0.00\n",
      "Step 1458/1767: training loss=0.48\n",
      "Step 1459/1767: training loss=0.00\n",
      "Step 1460/1767: training loss=0.00\n",
      "Step 1461/1767: training loss=0.00\n",
      "Step 1462/1767: training loss=0.00\n",
      "Step 1463/1767: training loss=0.00\n",
      "Step 1464/1767: training loss=0.00\n",
      "Step 1465/1767: training loss=1.78\n",
      "Step 1466/1767: training loss=0.00\n",
      "Step 1467/1767: training loss=0.00\n",
      "Step 1468/1767: training loss=1.59\n",
      "Step 1469/1767: training loss=0.00\n",
      "Step 1470/1767: training loss=0.00\n",
      "Step 1471/1767: training loss=1.68\n",
      "Step 1472/1767: training loss=0.00\n",
      "Step 1473/1767: training loss=0.00\n",
      "Step 1474/1767: training loss=0.00\n",
      "Step 1475/1767: training loss=0.00\n",
      "Step 1476/1767: training loss=0.00\n",
      "Step 1477/1767: training loss=0.00\n",
      "Step 1478/1767: training loss=1.06\n",
      "Step 1479/1767: training loss=0.00\n",
      "Step 1480/1767: training loss=0.00\n",
      "Step 1481/1767: training loss=0.00\n",
      "Step 1482/1767: training loss=1.41\n",
      "Step 1483/1767: training loss=0.00\n",
      "Step 1484/1767: training loss=0.00\n",
      "Step 1485/1767: training loss=0.00\n",
      "Step 1486/1767: training loss=0.00\n",
      "Step 1487/1767: training loss=0.35\n",
      "Step 1488/1767: training loss=0.00\n",
      "Step 1489/1767: training loss=0.00\n",
      "Step 1490/1767: training loss=0.00\n",
      "Step 1491/1767: training loss=0.59\n",
      "Step 1492/1767: training loss=0.00\n",
      "Step 1493/1767: training loss=0.00\n",
      "Step 1494/1767: training loss=0.46\n",
      "Step 1495/1767: training loss=0.00\n",
      "Step 1496/1767: training loss=0.00\n",
      "Step 1497/1767: training loss=1.15\n",
      "Step 1498/1767: training loss=0.00\n",
      "Step 1499/1767: training loss=0.00\n",
      "Step 1500/1767: training loss=0.00, validation loss=0.00\n",
      "Step 1501/1767: training loss=0.00\n",
      "Step 1502/1767: training loss=0.00\n",
      "Step 1503/1767: training loss=0.79\n",
      "Step 1504/1767: training loss=0.00\n",
      "Step 1505/1767: training loss=0.00\n",
      "Step 1506/1767: training loss=0.11\n",
      "Step 1507/1767: training loss=0.00\n",
      "Step 1508/1767: training loss=0.00\n",
      "Step 1509/1767: training loss=0.00\n",
      "Step 1510/1767: training loss=1.21\n",
      "Step 1511/1767: training loss=0.00\n",
      "Step 1512/1767: training loss=0.80\n",
      "Step 1513/1767: training loss=0.00\n",
      "Step 1514/1767: training loss=0.00\n",
      "Step 1515/1767: training loss=0.00\n",
      "Step 1516/1767: training loss=1.58\n",
      "Step 1517/1767: training loss=0.01\n",
      "Step 1518/1767: training loss=0.00\n",
      "Step 1519/1767: training loss=0.00\n",
      "Step 1520/1767: training loss=0.00\n",
      "Step 1521/1767: training loss=2.04\n",
      "Step 1522/1767: training loss=3.09\n",
      "Step 1523/1767: training loss=1.64\n",
      "Step 1524/1767: training loss=0.00\n",
      "Step 1525/1767: training loss=0.00\n",
      "Step 1526/1767: training loss=0.51\n",
      "Step 1527/1767: training loss=0.07\n",
      "Step 1528/1767: training loss=0.00\n",
      "Step 1529/1767: training loss=0.00\n",
      "Step 1530/1767: training loss=0.00\n",
      "Step 1531/1767: training loss=0.00\n",
      "Step 1532/1767: training loss=0.00\n",
      "Step 1533/1767: training loss=0.00\n",
      "Step 1534/1767: training loss=0.00\n",
      "Step 1535/1767: training loss=0.00\n",
      "Step 1536/1767: training loss=0.00\n",
      "Step 1537/1767: training loss=0.00\n",
      "Step 1538/1767: training loss=0.00\n",
      "Step 1539/1767: training loss=0.00\n",
      "Step 1540/1767: training loss=0.00\n",
      "Step 1541/1767: training loss=0.00\n",
      "Step 1542/1767: training loss=0.00\n",
      "Step 1543/1767: training loss=1.43\n",
      "Step 1544/1767: training loss=1.79\n",
      "Step 1545/1767: training loss=0.00\n",
      "Step 1546/1767: training loss=0.00\n",
      "Step 1547/1767: training loss=0.00\n",
      "Step 1548/1767: training loss=0.52\n",
      "Step 1549/1767: training loss=0.00\n",
      "Step 1550/1767: training loss=0.00\n",
      "Step 1551/1767: training loss=0.00\n",
      "Step 1552/1767: training loss=0.00\n",
      "Step 1553/1767: training loss=0.00\n",
      "Step 1554/1767: training loss=0.00\n",
      "Step 1555/1767: training loss=0.00\n",
      "Step 1556/1767: training loss=0.00\n",
      "Step 1557/1767: training loss=0.05\n",
      "Step 1558/1767: training loss=1.66\n",
      "Step 1559/1767: training loss=0.00\n",
      "Step 1560/1767: training loss=0.33\n",
      "Step 1561/1767: training loss=1.71\n",
      "Step 1562/1767: training loss=0.01\n",
      "Step 1563/1767: training loss=0.00\n",
      "Step 1564/1767: training loss=0.86\n",
      "Step 1565/1767: training loss=0.00\n",
      "Step 1566/1767: training loss=1.19\n",
      "Step 1567/1767: training loss=0.00\n",
      "Step 1568/1767: training loss=0.55\n",
      "Step 1569/1767: training loss=0.00\n",
      "Step 1570/1767: training loss=0.01\n",
      "Step 1571/1767: training loss=0.00\n",
      "Step 1572/1767: training loss=0.00\n",
      "Step 1573/1767: training loss=0.67\n",
      "Step 1574/1767: training loss=0.00\n",
      "Step 1575/1767: training loss=0.84\n",
      "Step 1576/1767: training loss=0.00\n",
      "Step 1577/1767: training loss=1.26\n",
      "Step 1578/1767: training loss=0.20\n",
      "Step 1579/1767: training loss=0.00\n",
      "Step 1580/1767: training loss=1.95\n",
      "Step 1581/1767: training loss=0.00\n",
      "Step 1582/1767: training loss=0.00\n",
      "Step 1583/1767: training loss=0.00\n",
      "Step 1584/1767: training loss=0.00\n",
      "Step 1585/1767: training loss=0.00\n",
      "Step 1586/1767: training loss=0.00\n",
      "Step 1587/1767: training loss=0.00\n",
      "Step 1588/1767: training loss=0.00\n",
      "Step 1589/1767: training loss=0.00\n",
      "Step 1590/1767: training loss=0.00\n",
      "Step 1591/1767: training loss=2.08\n",
      "Step 1592/1767: training loss=0.00\n",
      "Step 1593/1767: training loss=0.06\n",
      "Step 1594/1767: training loss=0.00\n",
      "Step 1595/1767: training loss=0.00\n",
      "Step 1596/1767: training loss=0.00\n",
      "Step 1597/1767: training loss=1.48\n",
      "Step 1598/1767: training loss=0.00\n",
      "Step 1599/1767: training loss=1.84\n",
      "Step 1600/1767: training loss=0.04, validation loss=1.35\n",
      "Step 1601/1767: training loss=0.00\n",
      "Step 1602/1767: training loss=2.02\n",
      "Step 1603/1767: training loss=0.16\n",
      "Step 1604/1767: training loss=1.55\n",
      "Step 1605/1767: training loss=0.00\n",
      "Step 1606/1767: training loss=0.00\n",
      "Step 1607/1767: training loss=1.72\n",
      "Step 1608/1767: training loss=0.00\n",
      "Step 1609/1767: training loss=0.90\n",
      "Step 1610/1767: training loss=0.00\n",
      "Step 1611/1767: training loss=1.60\n",
      "Step 1612/1767: training loss=0.00\n",
      "Step 1613/1767: training loss=0.01\n",
      "Step 1614/1767: training loss=0.00\n",
      "Step 1615/1767: training loss=0.00\n",
      "Step 1616/1767: training loss=0.96\n",
      "Step 1617/1767: training loss=1.10\n",
      "Step 1618/1767: training loss=1.45\n",
      "Step 1619/1767: training loss=0.00\n",
      "Step 1620/1767: training loss=0.00\n",
      "Step 1621/1767: training loss=0.00\n",
      "Step 1622/1767: training loss=0.00\n",
      "Step 1623/1767: training loss=0.00\n",
      "Step 1624/1767: training loss=0.75\n",
      "Step 1625/1767: training loss=0.01\n",
      "Step 1626/1767: training loss=0.00\n",
      "Step 1627/1767: training loss=0.00\n",
      "Step 1628/1767: training loss=0.00\n",
      "Step 1629/1767: training loss=0.00\n",
      "Step 1630/1767: training loss=0.00\n",
      "Step 1631/1767: training loss=0.00\n",
      "Step 1632/1767: training loss=0.74\n",
      "Step 1633/1767: training loss=0.00\n",
      "Step 1634/1767: training loss=1.40\n",
      "Step 1635/1767: training loss=0.00\n",
      "Step 1636/1767: training loss=0.86\n",
      "Step 1637/1767: training loss=1.43\n",
      "Step 1638/1767: training loss=0.00\n",
      "Step 1639/1767: training loss=0.00\n",
      "Step 1640/1767: training loss=0.00\n",
      "Step 1641/1767: training loss=0.00\n",
      "Step 1642/1767: training loss=0.33\n",
      "Step 1643/1767: training loss=1.39\n",
      "Step 1644/1767: training loss=0.00\n",
      "Step 1645/1767: training loss=1.46\n",
      "Step 1646/1767: training loss=0.00\n",
      "Step 1647/1767: training loss=0.00\n",
      "Step 1648/1767: training loss=0.00\n",
      "Step 1649/1767: training loss=0.15\n",
      "Step 1650/1767: training loss=0.00\n",
      "Step 1651/1767: training loss=0.00\n",
      "Step 1652/1767: training loss=1.12\n",
      "Step 1653/1767: training loss=0.00\n",
      "Step 1654/1767: training loss=0.00\n",
      "Step 1655/1767: training loss=0.66\n",
      "Step 1656/1767: training loss=0.00\n",
      "Step 1657/1767: training loss=0.00\n",
      "Step 1658/1767: training loss=0.00\n",
      "Step 1659/1767: training loss=0.02\n",
      "Step 1660/1767: training loss=0.00\n",
      "Step 1661/1767: training loss=0.00\n",
      "Step 1662/1767: training loss=0.00\n",
      "Step 1663/1767: training loss=0.00\n",
      "Step 1664/1767: training loss=0.00\n",
      "Step 1665/1767: training loss=0.00\n",
      "Step 1666/1767: training loss=0.00\n",
      "Step 1667/1767: training loss=0.00\n",
      "Step 1668/1767: training loss=1.25\n",
      "Step 1669/1767: training loss=0.98\n",
      "Step 1670/1767: training loss=1.44\n",
      "Step 1671/1767: training loss=0.92\n",
      "Step 1672/1767: training loss=0.00\n",
      "Step 1673/1767: training loss=1.44\n",
      "Step 1674/1767: training loss=1.30\n",
      "Step 1675/1767: training loss=0.00\n",
      "Step 1676/1767: training loss=0.00\n",
      "Step 1677/1767: training loss=0.00\n",
      "Step 1678/1767: training loss=1.44\n",
      "Step 1679/1767: training loss=0.00\n",
      "Step 1680/1767: training loss=0.00\n",
      "Step 1681/1767: training loss=0.00\n",
      "Step 1682/1767: training loss=0.00\n",
      "Step 1683/1767: training loss=0.00\n",
      "Step 1684/1767: training loss=0.00\n",
      "Step 1685/1767: training loss=0.00\n",
      "Step 1686/1767: training loss=0.00\n",
      "Step 1687/1767: training loss=0.25\n",
      "Step 1688/1767: training loss=0.02\n",
      "Step 1689/1767: training loss=0.00\n",
      "Step 1690/1767: training loss=0.00\n",
      "Step 1691/1767: training loss=2.73\n",
      "Step 1692/1767: training loss=0.14\n",
      "Step 1693/1767: training loss=0.20\n",
      "Step 1694/1767: training loss=1.21\n",
      "Step 1695/1767: training loss=0.00\n",
      "Step 1696/1767: training loss=0.00\n",
      "Step 1697/1767: training loss=0.05\n",
      "Step 1698/1767: training loss=0.00\n",
      "Step 1699/1767: training loss=0.00\n",
      "Step 1700/1767: training loss=1.43, validation loss=0.00\n",
      "Step 1701/1767: training loss=0.00\n",
      "Step 1702/1767: training loss=0.00\n",
      "Step 1703/1767: training loss=0.53\n",
      "Step 1704/1767: training loss=1.52\n",
      "Step 1705/1767: training loss=3.29\n",
      "Step 1706/1767: training loss=0.00\n",
      "Step 1707/1767: training loss=0.41\n",
      "Step 1708/1767: training loss=0.00\n",
      "Step 1709/1767: training loss=0.00\n",
      "Step 1710/1767: training loss=0.00\n",
      "Step 1711/1767: training loss=0.00\n",
      "Step 1712/1767: training loss=1.58\n",
      "Step 1713/1767: training loss=0.03\n",
      "Step 1714/1767: training loss=0.25\n",
      "Step 1715/1767: training loss=1.87\n",
      "Step 1716/1767: training loss=0.94\n",
      "Step 1717/1767: training loss=0.00\n",
      "Step 1718/1767: training loss=1.11\n",
      "Step 1719/1767: training loss=1.55\n",
      "Step 1720/1767: training loss=1.12\n",
      "Step 1721/1767: training loss=0.00\n",
      "Step 1722/1767: training loss=0.00\n",
      "Step 1723/1767: training loss=1.64\n",
      "Step 1724/1767: training loss=1.61\n",
      "Step 1725/1767: training loss=0.78\n",
      "Step 1726/1767: training loss=0.00\n",
      "Step 1727/1767: training loss=0.00\n",
      "Step 1728/1767: training loss=0.93\n",
      "Step 1729/1767: training loss=0.00\n",
      "Step 1730/1767: training loss=0.00\n",
      "Step 1731/1767: training loss=1.36\n",
      "Step 1732/1767: training loss=0.00\n",
      "Step 1733/1767: training loss=0.75\n",
      "Step 1734/1767: training loss=0.00\n",
      "Step 1735/1767: training loss=0.00\n",
      "Step 1736/1767: training loss=0.00\n",
      "Step 1737/1767: training loss=0.00\n",
      "Step 1738/1767: training loss=1.57\n",
      "Step 1739/1767: training loss=0.00\n",
      "Step 1740/1767: training loss=0.00\n",
      "Step 1741/1767: training loss=0.00\n",
      "Step 1742/1767: training loss=0.00\n",
      "Step 1743/1767: training loss=0.00\n",
      "Step 1744/1767: training loss=0.00\n",
      "Step 1745/1767: training loss=0.00\n",
      "Step 1746/1767: training loss=0.01\n",
      "Step 1747/1767: training loss=1.56\n",
      "Step 1748/1767: training loss=0.35\n",
      "Step 1749/1767: training loss=0.00\n",
      "Step 1750/1767: training loss=0.00\n",
      "Step 1751/1767: training loss=0.00\n",
      "Step 1752/1767: training loss=0.65\n",
      "Step 1753/1767: training loss=0.00\n",
      "Step 1754/1767: training loss=0.00\n",
      "Step 1755/1767: training loss=0.00\n",
      "Step 1756/1767: training loss=0.00\n",
      "Step 1757/1767: training loss=1.07\n",
      "Step 1758/1767: training loss=0.00\n",
      "Step 1759/1767: training loss=0.00\n",
      "Step 1760/1767: training loss=0.00\n",
      "Step 1761/1767: training loss=0.00\n",
      "Step 1762/1767: training loss=0.00\n",
      "Step 1763/1767: training loss=1.44\n",
      "Step 1764/1767: training loss=0.00\n",
      "Step 1765/1767: training loss=0.25\n",
      "Step 1766/1767: training loss=0.00\n",
      "Step 1767/1767: training loss=0.00, full validation loss=0.53\n",
      "Checkpoint created at step 589 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDMu8:ckpt-step-589\n",
      "Checkpoint created at step 1178 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDVWm:ckpt-step-1178\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=50)\n",
    "\n",
    "    \n",
    "events = [event for event in response]\n",
    "events.reverse()  \n",
    "\n",
    "for event in events:\n",
    "        \n",
    "    if hasattr(event, 'message'):\n",
    "        print(event.message)\n",
    "    else:\n",
    "        print(\"Event does not have a 'message' attribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model id: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW\n",
      "FineTuningJob(id='ftjob-Pyweyzaw7VycvlRk7qj5EjMW', created_at=1715703197, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW', finished_at=1715706486, hyperparameters=Hyperparameters(n_epochs=3, batch_size=4, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-Fi9mlsoWL88JdIl2wSCu95we', result_files=['file-dFIFbkZL9YsnDYRcl6eqJCue'], seed=1116149602, status='succeeded', trained_tokens=721605, training_file='file-a70lxStY2oLEIcM9NMlb8waJ', validation_file='file-V9YaNva6chzwov9jDnLg9nTV', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "if hasattr(response, 'fine_tuned_model'):  \n",
    "        fine_tuned_model_id = response.fine_tuned_model\n",
    "        print(\"Fine-tuned model id:\", fine_tuned_model_id)\n",
    "        response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        print(response)\n",
    "        \n",
    "else:\n",
    "        print(\"The response does not contain a 'fine_tuned_model' attribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Fine-tuned-gpt3.5(training-validation).json\"\n",
    "\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(fine_tuned_model_id, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv('/Users/Afnan/Desktop/FinalData/FinalTest.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               ...\n",
      "1         Gym        ...\n",
      "2              ...\n",
      "3      MENTION        ...\n",
      "4                       URL\n",
      "                             ...                        \n",
      "714    MENTION        ...\n",
      "715            ...\n",
      "716             ...\n",
      "717            ...\n",
      "718             ...\n",
      "Name: text, Length: 719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(testing['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               ...\n",
      "1         Gym        ...\n",
      "2              ...\n",
      "3      MENTION        ...\n",
      "4                       URL\n",
      "                             ...                        \n",
      "714    MENTION        ...\n",
      "715            ...\n",
      "716             ...\n",
      "717            ...\n",
      "718             ...\n",
      "Name: text, Length: 719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweet_data = testing['text']\n",
    "\n",
    "print(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'         '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness'}, {'role': 'user', 'content': '      '}]\n"
     ]
    }
   ],
   "source": [
    "test_messages = []\n",
    "test_messages.append({\"role\": \"system\", \n",
    "\"content\": \n",
    "\"You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness\"})\n",
    "user_message = \"      \"\n",
    "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "print(test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model ID: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Using model ID:\", fine_tuned_model_id)\n",
    "if not fine_tuned_model_id:\n",
    "    print(\"Model ID is not set. Please check the model ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
    ")\n",
    "message_content = response.choices[0].message.content\n",
    "print(message_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = []\n",
    "\n",
    "for i in range(len(tweet_data)):\n",
    "\n",
    "    test_messages = []\n",
    "    test_messages.append({\"role\": \"system\", \"content\": \"You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness\"})\n",
    "    user_message = tweet_data[i]\n",
    "    test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
    "    )\n",
    "    message_content = response.choices[0].message.content\n",
    "    label_pred.append(message_content)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "label_true = testing['label']\n",
    "label_true = label_true.tolist()\n",
    "print(label_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred_labelled = []\n",
    "\n",
    "label_labels = {\n",
    "    \"Positive\": [\"0\"],\n",
    "    \"Negative\": [\"2\"],\n",
    "    \"Neutral\": [\"1\"],\n",
    "}\n",
    "\n",
    "for predicted_label in label_pred:\n",
    "    labelled_label = None  \n",
    "    for label, country_label in label_labels.items():\n",
    "        if predicted_label in country_label:\n",
    "            labelled_label = label\n",
    "            break  \n",
    "\n",
    "    if labelled_label is not None:\n",
    "        label_pred_labelled.append(labelled_label)\n",
    "    else:\n",
    "        label_pred_labelled.append(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "print(label_pred_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = {'Predicted label': label_pred_labelled, 'Actual label': label_true}\n",
    "\n",
    "df=pd.DataFrame(label_data)\n",
    "\n",
    "df.to_csv('label_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.885952712100139\n",
      "\n",
      "\n",
      "F1-Score - macro: 0.5538385100428895\n",
      "\n",
      "\n",
      "F1-Score - weighted: 0.8784302814941476\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = accuracy_score(label_true, label_pred_labelled)\n",
    "print(\"Accuracy:\" , accuracy)\n",
    "f1score_macro = f1_score(label_true, label_pred_labelled, average='macro')\n",
    "print(\"\\n\")\n",
    "print(\"F1-Score - macro:\", f1score_macro)\n",
    "f1score_weighted = f1_score(label_true, label_pred_labelled, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"F1-Score - weighted:\", f1score_weighted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(label_true, label_pred_labelled)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "            yticklabels=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.59      0.77      0.67        73\n",
      "    Negative       0.11      0.04      0.05        28\n",
      "     Neutral       0.94      0.94      0.94       618\n",
      "\n",
      "    accuracy                           0.89       719\n",
      "   macro avg       0.55      0.58      0.55       719\n",
      "weighted avg       0.87      0.89      0.88       719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Determine unique classes in true labels\n",
    "unique_classes = sorted(set(label_true))\n",
    "\n",
    "# For testing dataset\n",
    "testing_report = classification_report(label_true, label_pred_labelled, labels=unique_classes, target_names=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "print(\"Testing Classification Report:\")\n",
    "print(testing_report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
