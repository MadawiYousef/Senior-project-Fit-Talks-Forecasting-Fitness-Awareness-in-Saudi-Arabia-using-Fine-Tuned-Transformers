{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('/Users/Afnan/Desktop/FinalData/FinalTraining.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation= pd.read_csv('/Users/Afnan/Desktop/FinalData/FinalValidation.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "def csv_to_jsonl(csv_file_path, jsonl_file_path):\n",
    "    with open(csv_file_path, 'r') as csv_file, open(jsonl_file_path, 'w') as jsonl_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            json.dump(row, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_jsonl('/Users/Afnan/Desktop/FinalData/FinalTraining.csv','train_split.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_jsonl('/Users/Afnan/Desktop/FinalData/FinalValidation.csv','validation_split.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1\n",
    "Prepare your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_jsonl(input_file_path, output_file_path):\n",
    "    entries = []\n",
    "    with open(input_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            entry = json.loads(line)\n",
    "            entries.append(entry)\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        for entry in entries:\n",
    "            messages = []\n",
    "            messages.append({\"role\": \"system\", \n",
    " \"content\": \n",
    "\"You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness \"})\n",
    "            user_message = {\"role\": \"user\", \"content\": entry[\"text\"]}\n",
    "            assistant_message = {\"role\": \"assistant\", \"content\": entry[\"label\"]}\n",
    "            messages.extend([user_message, assistant_message])\n",
    "            result = {\"messages\": messages}\n",
    "            json.dump(result, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_jsonl(\"/Users/Afnan/Desktop/FinalData/train_split.jsonl\", \"Format_train_split.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_jsonl(\"/Users/Afnan/Desktop/FinalData/validation_split.jsonl\", \"Format_validation_split.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "Upload files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "\n",
    "openai.api_key= \"   \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afnan\\anaconda3\\Lib\\site-packages\\pip\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "print(pip.__file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-a70lxStY2oLEIcM9NMlb8waJ\n",
      "Validation file id: file-V9YaNva6chzwov9jDnLg9nTV\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = openai.OpenAI(api_key=\"\")\n",
    "training_response = client.files.create(\n",
    "    file=open(\"/Users/Afnan/Desktop/FinalData/Format_train_split.jsonl\", \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file=open(\"/Users/Afnan/Desktop/FinalData/Format_validation_split.jsonl\", \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id\n",
    "\n",
    "print(\"Training file id:\", training_file_id)\n",
    "print(\"Validation file id:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Afnan\\anaconda3\\Lib\\site-packages\\pip\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "print(pip.__file__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3\n",
    "Create a fine-tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Pyweyzaw7VycvlRk7qj5EjMW', created_at=1715703197, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-Fi9mlsoWL88JdIl2wSCu95we', result_files=[], seed=1116149602, status='validating_files', trained_tokens=None, training_file='file-a70lxStY2oLEIcM9NMlb8waJ', validation_file='file-V9YaNva6chzwov9jDnLg9nTV', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
     ]
    }
   ],
   "source": [
    "suffix_name = \"samantha-test\"\n",
    "\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    validation_file=validation_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-Pyweyzaw7VycvlRk7qj5EjMW', created_at=1715703197, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-Fi9mlsoWL88JdIl2wSCu95we', result_files=[], seed=1116149602, status='validating_files', trained_tokens=None, training_file='file-a70lxStY2oLEIcM9NMlb8waJ', validation_file='file-V9YaNva6chzwov9jDnLg9nTV', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-Pyweyzaw7VycvlRk7qj5EjMW\n",
      "Validating training file: file-a70lxStY2oLEIcM9NMlb8waJ and validation file: file-V9YaNva6chzwov9jDnLg9nTV\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/1767: training loss=1.19\n",
      "Step 2/1767: training loss=0.87\n",
      "Step 3/1767: training loss=1.09\n",
      "Step 4/1767: training loss=1.32\n",
      "Step 5/1767: training loss=0.55\n",
      "Step 6/1767: training loss=0.60\n",
      "Step 7/1767: training loss=0.51\n",
      "Step 8/1767: training loss=1.47\n",
      "Step 9/1767: training loss=1.37\n",
      "Step 10/1767: training loss=0.80\n",
      "Step 11/1767: training loss=0.70\n",
      "Step 12/1767: training loss=1.30\n",
      "Step 13/1767: training loss=0.36\n",
      "Step 14/1767: training loss=0.73\n",
      "Step 15/1767: training loss=0.07\n",
      "Step 16/1767: training loss=0.15\n",
      "Step 17/1767: training loss=0.11\n",
      "Step 18/1767: training loss=1.27\n",
      "Step 19/1767: training loss=0.46\n",
      "Step 20/1767: training loss=0.42\n",
      "Step 21/1767: training loss=1.01\n",
      "Step 22/1767: training loss=0.07\n",
      "Step 23/1767: training loss=0.34\n",
      "Step 24/1767: training loss=1.05\n",
      "Step 25/1767: training loss=0.88\n",
      "Step 26/1767: training loss=0.31\n",
      "Step 27/1767: training loss=0.15\n",
      "Step 28/1767: training loss=0.01\n",
      "Step 29/1767: training loss=0.01\n",
      "Step 30/1767: training loss=0.02\n",
      "Step 31/1767: training loss=0.80\n",
      "Step 32/1767: training loss=0.25\n",
      "Step 33/1767: training loss=0.26\n",
      "Step 34/1767: training loss=0.77\n",
      "Step 35/1767: training loss=0.23\n",
      "Step 36/1767: training loss=0.31\n",
      "Step 37/1767: training loss=0.32\n",
      "Step 38/1767: training loss=0.11\n",
      "Step 39/1767: training loss=0.01\n",
      "Step 40/1767: training loss=0.04\n",
      "Step 41/1767: training loss=0.02\n",
      "Step 42/1767: training loss=0.02\n",
      "Step 43/1767: training loss=0.28\n",
      "Step 44/1767: training loss=0.46\n",
      "Step 45/1767: training loss=0.00\n",
      "Step 46/1767: training loss=0.00\n",
      "Step 47/1767: training loss=0.03\n",
      "Step 48/1767: training loss=0.00\n",
      "Step 49/1767: training loss=0.00\n",
      "Step 50/1767: training loss=0.41\n",
      "Step 51/1767: training loss=0.59\n",
      "Step 52/1767: training loss=0.85\n",
      "Step 53/1767: training loss=0.00\n",
      "Step 54/1767: training loss=0.00\n",
      "Step 55/1767: training loss=0.92\n",
      "Step 56/1767: training loss=0.81\n",
      "Step 57/1767: training loss=0.00\n",
      "Step 58/1767: training loss=1.35\n",
      "Step 59/1767: training loss=0.57\n",
      "Step 60/1767: training loss=0.41\n",
      "Step 61/1767: training loss=0.65\n",
      "Step 62/1767: training loss=0.00\n",
      "Step 63/1767: training loss=0.01\n",
      "Step 64/1767: training loss=0.42\n",
      "Step 65/1767: training loss=0.78\n",
      "Step 66/1767: training loss=0.57\n",
      "Step 67/1767: training loss=0.16\n",
      "Step 68/1767: training loss=0.00\n",
      "Step 69/1767: training loss=0.00\n",
      "Step 70/1767: training loss=0.00\n",
      "Step 71/1767: training loss=0.00\n",
      "Step 72/1767: training loss=0.00\n",
      "Step 73/1767: training loss=0.00\n",
      "Step 74/1767: training loss=0.00\n",
      "Step 75/1767: training loss=0.04\n",
      "Step 76/1767: training loss=0.00\n",
      "Step 77/1767: training loss=0.84\n",
      "Step 78/1767: training loss=0.00\n",
      "Step 79/1767: training loss=0.02\n",
      "Step 80/1767: training loss=0.00\n",
      "Step 81/1767: training loss=0.00\n",
      "Step 82/1767: training loss=0.00\n",
      "Step 83/1767: training loss=0.00\n",
      "Step 84/1767: training loss=0.00\n",
      "Step 85/1767: training loss=1.71\n",
      "Step 86/1767: training loss=0.00\n",
      "Step 87/1767: training loss=0.00\n",
      "Step 88/1767: training loss=3.97\n",
      "Step 89/1767: training loss=0.00\n",
      "Step 90/1767: training loss=0.00\n",
      "Step 91/1767: training loss=0.00\n",
      "Step 92/1767: training loss=0.00\n",
      "Step 93/1767: training loss=0.00\n",
      "Step 94/1767: training loss=0.00\n",
      "Step 95/1767: training loss=0.00\n",
      "Step 96/1767: training loss=0.00\n",
      "Step 97/1767: training loss=0.00\n",
      "Step 98/1767: training loss=1.71\n",
      "Step 99/1767: training loss=0.00\n",
      "Step 100/1767: training loss=0.63, validation loss=0.00\n",
      "Step 101/1767: training loss=0.00\n",
      "Step 102/1767: training loss=1.29\n",
      "Step 103/1767: training loss=0.00\n",
      "Step 104/1767: training loss=0.00\n",
      "Step 105/1767: training loss=1.90\n",
      "Step 106/1767: training loss=1.48\n",
      "Step 107/1767: training loss=1.37\n",
      "Step 108/1767: training loss=1.02\n",
      "Step 109/1767: training loss=1.10\n",
      "Step 110/1767: training loss=0.00\n",
      "Step 111/1767: training loss=1.44\n",
      "Step 112/1767: training loss=0.00\n",
      "Step 113/1767: training loss=1.20\n",
      "Step 114/1767: training loss=0.29\n",
      "Step 115/1767: training loss=0.00\n",
      "Step 116/1767: training loss=0.00\n",
      "Step 117/1767: training loss=0.42\n",
      "Step 118/1767: training loss=0.00\n",
      "Step 119/1767: training loss=1.07\n",
      "Step 120/1767: training loss=0.46\n",
      "Step 121/1767: training loss=0.00\n",
      "Step 122/1767: training loss=0.00\n",
      "Step 123/1767: training loss=0.00\n",
      "Step 124/1767: training loss=0.59\n",
      "Step 125/1767: training loss=0.42\n",
      "Step 126/1767: training loss=0.00\n",
      "Step 127/1767: training loss=0.00\n",
      "Step 128/1767: training loss=0.10\n",
      "Step 129/1767: training loss=0.00\n",
      "Step 130/1767: training loss=0.00\n",
      "Step 131/1767: training loss=0.34\n",
      "Step 132/1767: training loss=0.00\n",
      "Step 133/1767: training loss=1.92\n",
      "Step 134/1767: training loss=0.00\n",
      "Step 135/1767: training loss=0.00\n",
      "Step 136/1767: training loss=0.08\n",
      "Step 137/1767: training loss=0.69\n",
      "Step 138/1767: training loss=0.00\n",
      "Step 139/1767: training loss=0.00\n",
      "Step 140/1767: training loss=0.00\n",
      "Step 141/1767: training loss=0.00\n",
      "Step 142/1767: training loss=2.50\n",
      "Step 143/1767: training loss=0.00\n",
      "Step 144/1767: training loss=1.11\n",
      "Step 145/1767: training loss=1.62\n",
      "Step 146/1767: training loss=0.00\n",
      "Step 147/1767: training loss=0.00\n",
      "Step 148/1767: training loss=0.00\n",
      "Step 149/1767: training loss=1.26\n",
      "Step 150/1767: training loss=0.00\n",
      "Step 151/1767: training loss=0.09\n",
      "Step 152/1767: training loss=1.16\n",
      "Step 153/1767: training loss=0.13\n",
      "Step 154/1767: training loss=0.00\n",
      "Step 155/1767: training loss=0.09\n",
      "Step 156/1767: training loss=0.00\n",
      "Step 157/1767: training loss=0.00\n",
      "Step 158/1767: training loss=0.00\n",
      "Step 159/1767: training loss=1.12\n",
      "Step 160/1767: training loss=0.01\n",
      "Step 161/1767: training loss=0.00\n",
      "Step 162/1767: training loss=0.00\n",
      "Step 163/1767: training loss=0.00\n",
      "Step 164/1767: training loss=0.00\n",
      "Step 165/1767: training loss=1.49\n",
      "Step 166/1767: training loss=1.97\n",
      "Step 167/1767: training loss=0.00\n",
      "Step 168/1767: training loss=0.82\n",
      "Step 169/1767: training loss=1.71\n",
      "Step 170/1767: training loss=0.00\n",
      "Step 171/1767: training loss=1.80\n",
      "Step 172/1767: training loss=0.00\n",
      "Step 173/1767: training loss=1.00\n",
      "Step 174/1767: training loss=0.52\n",
      "Step 175/1767: training loss=0.02\n",
      "Step 176/1767: training loss=0.02\n",
      "Step 177/1767: training loss=1.06\n",
      "Step 178/1767: training loss=1.05\n",
      "Step 179/1767: training loss=0.00\n",
      "Step 180/1767: training loss=0.06\n",
      "Step 181/1767: training loss=0.32\n",
      "Step 182/1767: training loss=0.31\n",
      "Step 183/1767: training loss=0.17\n",
      "Step 184/1767: training loss=0.12\n",
      "Step 185/1767: training loss=0.21\n",
      "Step 186/1767: training loss=0.27\n",
      "Step 187/1767: training loss=0.00\n",
      "Step 188/1767: training loss=0.33\n",
      "Step 189/1767: training loss=0.04\n",
      "Step 190/1767: training loss=0.05\n",
      "Step 191/1767: training loss=0.02\n",
      "Step 192/1767: training loss=0.42\n",
      "Step 193/1767: training loss=0.00\n",
      "Step 194/1767: training loss=0.17\n",
      "Step 195/1767: training loss=0.83\n",
      "Step 196/1767: training loss=0.66\n",
      "Step 197/1767: training loss=0.00\n",
      "Step 198/1767: training loss=0.33\n",
      "Step 199/1767: training loss=0.29\n",
      "Step 200/1767: training loss=0.00, validation loss=0.00\n",
      "Step 201/1767: training loss=0.24\n",
      "Step 202/1767: training loss=0.00\n",
      "Step 203/1767: training loss=0.00\n",
      "Step 204/1767: training loss=1.68\n",
      "Step 205/1767: training loss=0.00\n",
      "Step 206/1767: training loss=0.52\n",
      "Step 207/1767: training loss=0.00\n",
      "Step 208/1767: training loss=1.09\n",
      "Step 209/1767: training loss=0.52\n",
      "Step 210/1767: training loss=0.03\n",
      "Step 211/1767: training loss=1.33\n",
      "Step 212/1767: training loss=0.00\n",
      "Step 213/1767: training loss=1.05\n",
      "Step 214/1767: training loss=0.00\n",
      "Step 215/1767: training loss=0.01\n",
      "Step 216/1767: training loss=0.00\n",
      "Step 217/1767: training loss=0.01\n",
      "Step 218/1767: training loss=0.36\n",
      "Step 219/1767: training loss=1.11\n",
      "Step 220/1767: training loss=1.12\n",
      "Step 221/1767: training loss=0.00\n",
      "Step 222/1767: training loss=1.31\n",
      "Step 223/1767: training loss=0.00\n",
      "Step 224/1767: training loss=0.00\n",
      "Step 225/1767: training loss=0.00\n",
      "Step 226/1767: training loss=0.52\n",
      "Step 227/1767: training loss=0.00\n",
      "Step 228/1767: training loss=0.00\n",
      "Step 229/1767: training loss=0.00\n",
      "Step 230/1767: training loss=0.88\n",
      "Step 231/1767: training loss=1.23\n",
      "Step 232/1767: training loss=0.00\n",
      "Step 233/1767: training loss=1.90\n",
      "Step 234/1767: training loss=0.98\n",
      "Step 235/1767: training loss=1.55\n",
      "Step 236/1767: training loss=0.00\n",
      "Step 237/1767: training loss=0.95\n",
      "Step 238/1767: training loss=0.00\n",
      "Step 239/1767: training loss=0.28\n",
      "Step 240/1767: training loss=0.17\n",
      "Step 241/1767: training loss=0.01\n",
      "Step 242/1767: training loss=0.00\n",
      "Step 243/1767: training loss=0.93\n",
      "Step 244/1767: training loss=0.21\n",
      "Step 245/1767: training loss=0.07\n",
      "Step 246/1767: training loss=0.01\n",
      "Step 247/1767: training loss=0.71\n",
      "Step 248/1767: training loss=0.07\n",
      "Step 249/1767: training loss=0.00\n",
      "Step 250/1767: training loss=0.09\n",
      "Step 251/1767: training loss=0.95\n",
      "Step 252/1767: training loss=0.07\n",
      "Step 253/1767: training loss=0.58\n",
      "Step 254/1767: training loss=1.23\n",
      "Step 255/1767: training loss=0.00\n",
      "Step 256/1767: training loss=0.00\n",
      "Step 257/1767: training loss=0.00\n",
      "Step 258/1767: training loss=0.00\n",
      "Step 259/1767: training loss=0.00\n",
      "Step 260/1767: training loss=0.00\n",
      "Step 261/1767: training loss=1.28\n",
      "Step 262/1767: training loss=0.49\n",
      "Step 263/1767: training loss=0.65\n",
      "Step 264/1767: training loss=0.00\n",
      "Step 265/1767: training loss=0.00\n",
      "Step 266/1767: training loss=0.00\n",
      "Step 267/1767: training loss=1.16\n",
      "Step 268/1767: training loss=1.26\n",
      "Step 269/1767: training loss=0.00\n",
      "Step 270/1767: training loss=0.00\n",
      "Step 271/1767: training loss=0.00\n",
      "Step 272/1767: training loss=2.03\n",
      "Step 273/1767: training loss=0.92\n",
      "Step 274/1767: training loss=0.84\n",
      "Step 275/1767: training loss=0.00\n",
      "Step 276/1767: training loss=0.00\n",
      "Step 277/1767: training loss=1.25\n",
      "Step 278/1767: training loss=0.00\n",
      "Step 279/1767: training loss=0.89\n",
      "Step 280/1767: training loss=0.00\n",
      "Step 281/1767: training loss=0.00\n",
      "Step 282/1767: training loss=0.00\n",
      "Step 283/1767: training loss=1.22\n",
      "Step 284/1767: training loss=0.00\n",
      "Step 285/1767: training loss=0.00\n",
      "Step 286/1767: training loss=1.64\n",
      "Step 287/1767: training loss=0.83\n",
      "Step 288/1767: training loss=0.07\n",
      "Step 289/1767: training loss=0.73\n",
      "Step 290/1767: training loss=0.00\n",
      "Step 291/1767: training loss=0.61\n",
      "Step 292/1767: training loss=0.90\n",
      "Step 293/1767: training loss=0.32\n",
      "Step 294/1767: training loss=0.01\n",
      "Step 295/1767: training loss=0.02\n",
      "Step 296/1767: training loss=0.27\n",
      "Step 297/1767: training loss=0.04\n",
      "Step 298/1767: training loss=0.12\n",
      "Step 299/1767: training loss=0.21\n",
      "Step 300/1767: training loss=0.00, validation loss=0.17\n",
      "Step 301/1767: training loss=0.05\n",
      "Step 302/1767: training loss=0.00\n",
      "Step 303/1767: training loss=0.00\n",
      "Step 304/1767: training loss=0.36\n",
      "Step 305/1767: training loss=0.00\n",
      "Step 306/1767: training loss=1.09\n",
      "Step 307/1767: training loss=0.00\n",
      "Step 308/1767: training loss=0.00\n",
      "Step 309/1767: training loss=0.00\n",
      "Step 310/1767: training loss=0.22\n",
      "Step 311/1767: training loss=1.10\n",
      "Step 312/1767: training loss=0.00\n",
      "Step 313/1767: training loss=0.00\n",
      "Step 314/1767: training loss=0.00\n",
      "Step 315/1767: training loss=0.75\n",
      "Step 316/1767: training loss=0.00\n",
      "Step 317/1767: training loss=0.00\n",
      "Step 318/1767: training loss=0.53\n",
      "Step 319/1767: training loss=0.00\n",
      "Step 320/1767: training loss=0.01\n",
      "Step 321/1767: training loss=0.00\n",
      "Step 322/1767: training loss=0.00\n",
      "Step 323/1767: training loss=0.00\n",
      "Step 324/1767: training loss=0.00\n",
      "Step 325/1767: training loss=0.00\n",
      "Step 326/1767: training loss=0.00\n",
      "Step 327/1767: training loss=0.00\n",
      "Step 328/1767: training loss=0.00\n",
      "Step 329/1767: training loss=2.65\n",
      "Step 330/1767: training loss=1.03\n",
      "Step 331/1767: training loss=0.00\n",
      "Step 332/1767: training loss=0.00\n",
      "Step 333/1767: training loss=0.00\n",
      "Step 334/1767: training loss=2.90\n",
      "Step 335/1767: training loss=0.00\n",
      "Step 336/1767: training loss=0.00\n",
      "Step 337/1767: training loss=4.08\n",
      "Step 338/1767: training loss=1.26\n",
      "Step 339/1767: training loss=0.00\n",
      "Step 340/1767: training loss=4.18\n",
      "Step 341/1767: training loss=1.25\n",
      "Step 342/1767: training loss=0.00\n",
      "Step 343/1767: training loss=2.85\n",
      "Step 344/1767: training loss=1.45\n",
      "Step 345/1767: training loss=1.65\n",
      "Step 346/1767: training loss=0.78\n",
      "Step 347/1767: training loss=0.00\n",
      "Step 348/1767: training loss=0.97\n",
      "Step 349/1767: training loss=0.00\n",
      "Step 350/1767: training loss=1.05\n",
      "Step 351/1767: training loss=0.00\n",
      "Step 352/1767: training loss=0.93\n",
      "Step 353/1767: training loss=0.00\n",
      "Step 354/1767: training loss=0.49\n",
      "Step 355/1767: training loss=0.71\n",
      "Step 356/1767: training loss=0.00\n",
      "Step 357/1767: training loss=0.79\n",
      "Step 358/1767: training loss=0.00\n",
      "Step 359/1767: training loss=0.59\n",
      "Step 360/1767: training loss=0.42\n",
      "Step 361/1767: training loss=0.12\n",
      "Step 362/1767: training loss=0.00\n",
      "Step 363/1767: training loss=0.05\n",
      "Step 364/1767: training loss=0.00\n",
      "Step 365/1767: training loss=0.00\n",
      "Step 366/1767: training loss=0.00\n",
      "Step 367/1767: training loss=0.34\n",
      "Step 368/1767: training loss=0.00\n",
      "Step 369/1767: training loss=0.62\n",
      "Step 370/1767: training loss=0.00\n",
      "Step 371/1767: training loss=0.86\n",
      "Step 372/1767: training loss=0.00\n",
      "Step 373/1767: training loss=0.31\n",
      "Step 374/1767: training loss=0.00\n",
      "Step 375/1767: training loss=0.95\n",
      "Step 376/1767: training loss=0.30\n",
      "Step 377/1767: training loss=0.01\n",
      "Step 378/1767: training loss=0.50\n",
      "Step 379/1767: training loss=0.00\n",
      "Step 380/1767: training loss=1.05\n",
      "Step 381/1767: training loss=0.00\n",
      "Step 382/1767: training loss=0.03\n",
      "Step 383/1767: training loss=0.52\n",
      "Step 384/1767: training loss=0.00\n",
      "Step 385/1767: training loss=0.59\n",
      "Step 386/1767: training loss=0.15\n",
      "Step 387/1767: training loss=0.00\n",
      "Step 388/1767: training loss=0.00\n",
      "Step 389/1767: training loss=0.01\n",
      "Step 390/1767: training loss=0.58\n",
      "Step 391/1767: training loss=0.00\n",
      "Step 392/1767: training loss=2.81\n",
      "Step 393/1767: training loss=0.17\n",
      "Step 394/1767: training loss=0.00\n",
      "Step 395/1767: training loss=0.73\n",
      "Step 396/1767: training loss=0.08\n",
      "Step 397/1767: training loss=0.00\n",
      "Step 398/1767: training loss=0.00\n",
      "Step 399/1767: training loss=0.12\n",
      "Step 400/1767: training loss=0.00, validation loss=1.21\n",
      "Step 401/1767: training loss=1.60\n",
      "Step 402/1767: training loss=0.00\n",
      "Step 403/1767: training loss=0.03\n",
      "Step 404/1767: training loss=0.00\n",
      "Step 405/1767: training loss=0.00\n",
      "Step 406/1767: training loss=0.00\n",
      "Step 407/1767: training loss=1.22\n",
      "Step 408/1767: training loss=0.00\n",
      "Step 409/1767: training loss=2.69\n",
      "Step 410/1767: training loss=0.90\n",
      "Step 411/1767: training loss=0.00\n",
      "Step 412/1767: training loss=0.12\n",
      "Step 413/1767: training loss=0.00\n",
      "Step 414/1767: training loss=0.00\n",
      "Step 415/1767: training loss=1.93\n",
      "Step 416/1767: training loss=1.25\n",
      "Step 417/1767: training loss=0.00\n",
      "Step 418/1767: training loss=0.28\n",
      "Step 419/1767: training loss=0.00\n",
      "Step 420/1767: training loss=0.06\n",
      "Step 421/1767: training loss=0.00\n",
      "Step 422/1767: training loss=0.42\n",
      "Step 423/1767: training loss=0.15\n",
      "Step 424/1767: training loss=0.00\n",
      "Step 425/1767: training loss=0.60\n",
      "Step 426/1767: training loss=0.00\n",
      "Step 427/1767: training loss=0.65\n",
      "Step 428/1767: training loss=0.00\n",
      "Step 429/1767: training loss=0.06\n",
      "Step 430/1767: training loss=1.32\n",
      "Step 431/1767: training loss=0.51\n",
      "Step 432/1767: training loss=1.58\n",
      "Step 433/1767: training loss=0.04\n",
      "Step 434/1767: training loss=1.12\n",
      "Step 435/1767: training loss=0.46\n",
      "Step 436/1767: training loss=0.00\n",
      "Step 437/1767: training loss=0.01\n",
      "Step 438/1767: training loss=1.22\n",
      "Step 439/1767: training loss=0.00\n",
      "Step 440/1767: training loss=0.36\n",
      "Step 441/1767: training loss=0.00\n",
      "Step 442/1767: training loss=0.19\n",
      "Step 443/1767: training loss=0.00\n",
      "Step 444/1767: training loss=0.00\n",
      "Step 445/1767: training loss=0.00\n",
      "Step 446/1767: training loss=1.03\n",
      "Step 447/1767: training loss=1.53\n",
      "Step 448/1767: training loss=1.33\n",
      "Step 449/1767: training loss=0.00\n",
      "Step 450/1767: training loss=0.00\n",
      "Step 451/1767: training loss=0.17\n",
      "Step 452/1767: training loss=0.00\n",
      "Step 453/1767: training loss=0.00\n",
      "Step 454/1767: training loss=0.16\n",
      "Step 455/1767: training loss=0.01\n",
      "Step 456/1767: training loss=1.29\n",
      "Step 457/1767: training loss=0.00\n",
      "Step 458/1767: training loss=1.38\n",
      "Step 459/1767: training loss=0.00\n",
      "Step 460/1767: training loss=1.13\n",
      "Step 461/1767: training loss=0.39\n",
      "Step 462/1767: training loss=0.00\n",
      "Step 463/1767: training loss=0.00\n",
      "Step 464/1767: training loss=0.00\n",
      "Step 465/1767: training loss=0.00\n",
      "Step 466/1767: training loss=1.43\n",
      "Step 467/1767: training loss=0.05\n",
      "Step 468/1767: training loss=0.00\n",
      "Step 469/1767: training loss=1.33\n",
      "Step 470/1767: training loss=0.29\n",
      "Step 471/1767: training loss=0.00\n",
      "Step 472/1767: training loss=0.00\n",
      "Step 473/1767: training loss=0.01\n",
      "Step 474/1767: training loss=2.41\n",
      "Step 475/1767: training loss=0.03\n",
      "Step 476/1767: training loss=0.00\n",
      "Step 477/1767: training loss=0.00\n",
      "Step 478/1767: training loss=0.00\n",
      "Step 479/1767: training loss=0.13\n",
      "Step 480/1767: training loss=1.61\n",
      "Step 481/1767: training loss=0.00\n",
      "Step 482/1767: training loss=0.98\n",
      "Step 483/1767: training loss=1.11\n",
      "Step 484/1767: training loss=0.00\n",
      "Step 485/1767: training loss=0.00\n",
      "Step 486/1767: training loss=0.00\n",
      "Step 487/1767: training loss=0.05\n",
      "Step 488/1767: training loss=0.53\n",
      "Step 489/1767: training loss=0.73\n",
      "Step 490/1767: training loss=0.00\n",
      "Step 491/1767: training loss=1.02\n",
      "Step 492/1767: training loss=0.00\n",
      "Step 493/1767: training loss=0.92\n",
      "Step 494/1767: training loss=0.13\n",
      "Step 495/1767: training loss=1.02\n",
      "Step 496/1767: training loss=0.00\n",
      "Step 497/1767: training loss=0.00\n",
      "Step 498/1767: training loss=0.00\n",
      "Step 499/1767: training loss=0.43\n",
      "Step 500/1767: training loss=0.40, validation loss=1.22\n",
      "Step 501/1767: training loss=0.00\n",
      "Step 502/1767: training loss=0.60\n",
      "Step 503/1767: training loss=0.88\n",
      "Step 504/1767: training loss=0.00\n",
      "Step 505/1767: training loss=0.00\n",
      "Step 506/1767: training loss=1.57\n",
      "Step 507/1767: training loss=0.47\n",
      "Step 508/1767: training loss=1.42\n",
      "Step 509/1767: training loss=0.00\n",
      "Step 510/1767: training loss=0.01\n",
      "Step 511/1767: training loss=2.20\n",
      "Step 512/1767: training loss=0.00\n",
      "Step 513/1767: training loss=0.00\n",
      "Step 514/1767: training loss=0.00\n",
      "Step 515/1767: training loss=1.10\n",
      "Step 516/1767: training loss=0.00\n",
      "Step 517/1767: training loss=0.00\n",
      "Step 518/1767: training loss=1.09\n",
      "Step 519/1767: training loss=0.00\n",
      "Step 520/1767: training loss=1.23\n",
      "Step 521/1767: training loss=0.57\n",
      "Step 522/1767: training loss=0.00\n",
      "Step 523/1767: training loss=0.00\n",
      "Step 524/1767: training loss=0.00\n",
      "Step 525/1767: training loss=0.07\n",
      "Step 526/1767: training loss=0.00\n",
      "Step 527/1767: training loss=0.00\n",
      "Step 528/1767: training loss=0.00\n",
      "Step 529/1767: training loss=0.00\n",
      "Step 530/1767: training loss=0.00\n",
      "Step 531/1767: training loss=0.62\n",
      "Step 532/1767: training loss=0.00\n",
      "Step 533/1767: training loss=0.00\n",
      "Step 534/1767: training loss=1.00\n",
      "Step 535/1767: training loss=0.00\n",
      "Step 536/1767: training loss=1.42\n",
      "Step 537/1767: training loss=0.00\n",
      "Step 538/1767: training loss=2.66\n",
      "Step 539/1767: training loss=0.00\n",
      "Step 540/1767: training loss=0.45\n",
      "Step 541/1767: training loss=0.00\n",
      "Step 542/1767: training loss=0.00\n",
      "Step 543/1767: training loss=0.00\n",
      "Step 544/1767: training loss=0.00\n",
      "Step 545/1767: training loss=0.00\n",
      "Step 546/1767: training loss=0.00\n",
      "Step 547/1767: training loss=2.70\n",
      "Step 548/1767: training loss=1.63\n",
      "Step 549/1767: training loss=1.92\n",
      "Step 550/1767: training loss=0.00\n",
      "Step 551/1767: training loss=1.13\n",
      "Step 552/1767: training loss=0.78\n",
      "Step 553/1767: training loss=0.97\n",
      "Step 554/1767: training loss=1.11\n",
      "Step 555/1767: training loss=0.00\n",
      "Step 556/1767: training loss=0.00\n",
      "Step 557/1767: training loss=0.00\n",
      "Step 558/1767: training loss=0.00\n",
      "Step 559/1767: training loss=0.00\n",
      "Step 560/1767: training loss=0.00\n",
      "Step 561/1767: training loss=0.00\n",
      "Step 562/1767: training loss=0.00\n",
      "Step 563/1767: training loss=0.47\n",
      "Step 564/1767: training loss=0.00\n",
      "Step 565/1767: training loss=0.17\n",
      "Step 566/1767: training loss=1.24\n",
      "Step 567/1767: training loss=0.00\n",
      "Step 568/1767: training loss=0.00\n",
      "Step 569/1767: training loss=0.00\n",
      "Step 570/1767: training loss=0.00\n",
      "Step 571/1767: training loss=3.86\n",
      "Step 572/1767: training loss=0.00\n",
      "Step 573/1767: training loss=1.88\n",
      "Step 574/1767: training loss=3.37\n",
      "Step 575/1767: training loss=3.43\n",
      "Step 576/1767: training loss=0.00\n",
      "Step 577/1767: training loss=1.62\n",
      "Step 578/1767: training loss=1.76\n",
      "Step 579/1767: training loss=1.44\n",
      "Step 580/1767: training loss=0.00\n",
      "Step 581/1767: training loss=0.19\n",
      "Step 582/1767: training loss=0.00\n",
      "Step 583/1767: training loss=0.00\n",
      "Step 584/1767: training loss=0.68\n",
      "Step 585/1767: training loss=0.77\n",
      "Step 586/1767: training loss=0.00\n",
      "Step 587/1767: training loss=0.00\n",
      "Step 588/1767: training loss=1.32\n",
      "Step 589/1767: training loss=0.86, full validation loss=0.33\n",
      "Step 590/1767: training loss=1.05\n",
      "Step 591/1767: training loss=0.01\n",
      "Step 592/1767: training loss=0.00\n",
      "Step 593/1767: training loss=0.00\n",
      "Step 594/1767: training loss=0.30\n",
      "Step 595/1767: training loss=0.00\n",
      "Step 596/1767: training loss=0.02\n",
      "Step 597/1767: training loss=0.00\n",
      "Step 598/1767: training loss=0.00\n",
      "Step 599/1767: training loss=0.00\n",
      "Step 600/1767: training loss=0.13, validation loss=0.00\n",
      "Step 601/1767: training loss=0.00\n",
      "Step 602/1767: training loss=0.98\n",
      "Step 603/1767: training loss=0.00\n",
      "Step 604/1767: training loss=0.24\n",
      "Step 605/1767: training loss=0.00\n",
      "Step 606/1767: training loss=1.19\n",
      "Step 607/1767: training loss=0.95\n",
      "Step 608/1767: training loss=0.03\n",
      "Step 609/1767: training loss=0.00\n",
      "Step 610/1767: training loss=0.00\n",
      "Step 611/1767: training loss=0.00\n",
      "Step 612/1767: training loss=0.00\n",
      "Step 613/1767: training loss=0.00\n",
      "Step 614/1767: training loss=0.00\n",
      "Step 615/1767: training loss=2.52\n",
      "Step 616/1767: training loss=0.00\n",
      "Step 617/1767: training loss=0.10\n",
      "Step 618/1767: training loss=0.00\n",
      "Step 619/1767: training loss=0.00\n",
      "Step 620/1767: training loss=0.00\n",
      "Step 621/1767: training loss=0.00\n",
      "Step 622/1767: training loss=0.29\n",
      "Step 623/1767: training loss=0.00\n",
      "Step 624/1767: training loss=0.00\n",
      "Step 625/1767: training loss=0.01\n",
      "Step 626/1767: training loss=0.00\n",
      "Step 627/1767: training loss=0.00\n",
      "Step 628/1767: training loss=1.85\n",
      "Step 629/1767: training loss=0.00\n",
      "Step 630/1767: training loss=1.99\n",
      "Step 631/1767: training loss=0.00\n",
      "Step 632/1767: training loss=0.00\n",
      "Step 633/1767: training loss=0.00\n",
      "Step 634/1767: training loss=0.00\n",
      "Step 635/1767: training loss=0.00\n",
      "Step 636/1767: training loss=0.12\n",
      "Step 637/1767: training loss=1.63\n",
      "Step 638/1767: training loss=1.75\n",
      "Step 639/1767: training loss=0.00\n",
      "Step 640/1767: training loss=0.00\n",
      "Step 641/1767: training loss=0.00\n",
      "Step 642/1767: training loss=2.49\n",
      "Step 643/1767: training loss=0.00\n",
      "Step 644/1767: training loss=0.00\n",
      "Step 645/1767: training loss=0.00\n",
      "Step 646/1767: training loss=1.34\n",
      "Step 647/1767: training loss=0.00\n",
      "Step 648/1767: training loss=1.14\n",
      "Step 649/1767: training loss=0.00\n",
      "Step 650/1767: training loss=1.28\n",
      "Step 651/1767: training loss=0.00\n",
      "Step 652/1767: training loss=0.00\n",
      "Step 653/1767: training loss=0.00\n",
      "Step 654/1767: training loss=0.00\n",
      "Step 655/1767: training loss=2.20\n",
      "Step 656/1767: training loss=1.08\n",
      "Step 657/1767: training loss=1.14\n",
      "Step 658/1767: training loss=0.00\n",
      "Step 659/1767: training loss=1.14\n",
      "Step 660/1767: training loss=0.00\n",
      "Step 661/1767: training loss=0.00\n",
      "Step 662/1767: training loss=0.00\n",
      "Step 663/1767: training loss=0.00\n",
      "Step 664/1767: training loss=0.62\n",
      "Step 665/1767: training loss=0.00\n",
      "Step 666/1767: training loss=0.00\n",
      "Step 667/1767: training loss=0.00\n",
      "Step 668/1767: training loss=0.00\n",
      "Step 669/1767: training loss=0.00\n",
      "Step 670/1767: training loss=0.00\n",
      "Step 671/1767: training loss=0.00\n",
      "Step 672/1767: training loss=0.00\n",
      "Step 673/1767: training loss=0.00\n",
      "Step 674/1767: training loss=0.00\n",
      "Step 675/1767: training loss=2.94\n",
      "Step 676/1767: training loss=0.00\n",
      "Step 677/1767: training loss=0.00\n",
      "Step 678/1767: training loss=0.92\n",
      "Step 679/1767: training loss=0.87\n",
      "Step 680/1767: training loss=0.00\n",
      "Step 681/1767: training loss=0.00\n",
      "Step 682/1767: training loss=0.00\n",
      "Step 683/1767: training loss=3.07\n",
      "Step 684/1767: training loss=0.00\n",
      "Step 685/1767: training loss=1.96\n",
      "Step 686/1767: training loss=0.00\n",
      "Step 687/1767: training loss=0.74\n",
      "Step 688/1767: training loss=0.00\n",
      "Step 689/1767: training loss=0.86\n",
      "Step 690/1767: training loss=0.00\n",
      "Step 691/1767: training loss=0.65\n",
      "Step 692/1767: training loss=0.00\n",
      "Step 693/1767: training loss=0.00\n",
      "Step 694/1767: training loss=1.51\n",
      "Step 695/1767: training loss=0.20\n",
      "Step 696/1767: training loss=0.00\n",
      "Step 697/1767: training loss=0.00\n",
      "Step 698/1767: training loss=0.00\n",
      "Step 699/1767: training loss=0.00\n",
      "Step 700/1767: training loss=0.05, validation loss=1.73\n",
      "Step 701/1767: training loss=0.00\n",
      "Step 702/1767: training loss=1.03\n",
      "Step 703/1767: training loss=0.00\n",
      "Step 704/1767: training loss=2.46\n",
      "Step 705/1767: training loss=0.67\n",
      "Step 706/1767: training loss=1.51\n",
      "Step 707/1767: training loss=2.52\n",
      "Step 708/1767: training loss=0.00\n",
      "Step 709/1767: training loss=0.00\n",
      "Step 710/1767: training loss=0.94\n",
      "Step 711/1767: training loss=1.39\n",
      "Step 712/1767: training loss=0.00\n",
      "Step 713/1767: training loss=0.00\n",
      "Step 714/1767: training loss=1.04\n",
      "Step 715/1767: training loss=0.00\n",
      "Step 716/1767: training loss=0.00\n",
      "Step 717/1767: training loss=0.89\n",
      "Step 718/1767: training loss=0.00\n",
      "Step 719/1767: training loss=0.10\n",
      "Step 720/1767: training loss=2.08\n",
      "Step 721/1767: training loss=0.00\n",
      "Step 722/1767: training loss=0.00\n",
      "Step 723/1767: training loss=0.00\n",
      "Step 724/1767: training loss=1.18\n",
      "Step 725/1767: training loss=0.00\n",
      "Step 726/1767: training loss=2.76\n",
      "Step 727/1767: training loss=0.00\n",
      "Step 728/1767: training loss=1.16\n",
      "Step 729/1767: training loss=0.00\n",
      "Step 730/1767: training loss=0.98\n",
      "Step 731/1767: training loss=0.33\n",
      "Step 732/1767: training loss=0.00\n",
      "Step 733/1767: training loss=0.00\n",
      "Step 734/1767: training loss=0.00\n",
      "Step 735/1767: training loss=0.00\n",
      "Step 736/1767: training loss=0.32\n",
      "Step 737/1767: training loss=1.43\n",
      "Step 738/1767: training loss=0.00\n",
      "Step 739/1767: training loss=0.00\n",
      "Step 740/1767: training loss=2.91\n",
      "Step 741/1767: training loss=0.00\n",
      "Step 742/1767: training loss=1.33\n",
      "Step 743/1767: training loss=0.00\n",
      "Step 744/1767: training loss=1.87\n",
      "Step 745/1767: training loss=0.00\n",
      "Step 746/1767: training loss=0.02\n",
      "Step 747/1767: training loss=0.82\n",
      "Step 748/1767: training loss=1.45\n",
      "Step 749/1767: training loss=1.15\n",
      "Step 750/1767: training loss=0.00\n",
      "Step 751/1767: training loss=0.56\n",
      "Step 752/1767: training loss=1.12\n",
      "Step 753/1767: training loss=1.73\n",
      "Step 754/1767: training loss=0.00\n",
      "Step 755/1767: training loss=0.72\n",
      "Step 756/1767: training loss=0.00\n",
      "Step 757/1767: training loss=0.81\n",
      "Step 758/1767: training loss=0.00\n",
      "Step 759/1767: training loss=0.00\n",
      "Step 760/1767: training loss=0.00\n",
      "Step 761/1767: training loss=0.00\n",
      "Step 762/1767: training loss=0.00\n",
      "Step 763/1767: training loss=0.00\n",
      "Step 764/1767: training loss=0.00\n",
      "Step 765/1767: training loss=0.00\n",
      "Step 766/1767: training loss=0.00\n",
      "Step 767/1767: training loss=0.00\n",
      "Step 768/1767: training loss=1.68\n",
      "Step 769/1767: training loss=0.00\n",
      "Step 770/1767: training loss=0.00\n",
      "Step 771/1767: training loss=1.46\n",
      "Step 772/1767: training loss=0.00\n",
      "Step 773/1767: training loss=1.82\n",
      "Step 774/1767: training loss=0.00\n",
      "Step 775/1767: training loss=0.00\n",
      "Step 776/1767: training loss=0.00\n",
      "Step 777/1767: training loss=1.85\n",
      "Step 778/1767: training loss=0.00\n",
      "Step 779/1767: training loss=1.99\n",
      "Step 780/1767: training loss=0.00\n",
      "Step 781/1767: training loss=0.20\n",
      "Step 782/1767: training loss=0.00\n",
      "Step 783/1767: training loss=0.00\n",
      "Step 784/1767: training loss=0.00\n",
      "Step 785/1767: training loss=0.00\n",
      "Step 786/1767: training loss=0.00\n",
      "Step 787/1767: training loss=0.00\n",
      "Step 788/1767: training loss=0.00\n",
      "Step 789/1767: training loss=1.40\n",
      "Step 790/1767: training loss=1.93\n",
      "Step 791/1767: training loss=1.35\n",
      "Step 792/1767: training loss=0.00\n",
      "Step 793/1767: training loss=0.75\n",
      "Step 794/1767: training loss=1.45\n",
      "Step 795/1767: training loss=0.00\n",
      "Step 796/1767: training loss=0.00\n",
      "Step 797/1767: training loss=1.32\n",
      "Step 798/1767: training loss=0.00\n",
      "Step 799/1767: training loss=0.00\n",
      "Step 800/1767: training loss=1.49, validation loss=0.00\n",
      "Step 801/1767: training loss=0.00\n",
      "Step 802/1767: training loss=2.68\n",
      "Step 803/1767: training loss=0.00\n",
      "Step 804/1767: training loss=0.64\n",
      "Step 805/1767: training loss=1.34\n",
      "Step 806/1767: training loss=0.00\n",
      "Step 807/1767: training loss=1.22\n",
      "Step 808/1767: training loss=0.65\n",
      "Step 809/1767: training loss=2.40\n",
      "Step 810/1767: training loss=0.00\n",
      "Step 811/1767: training loss=1.53\n",
      "Step 812/1767: training loss=0.91\n",
      "Step 813/1767: training loss=0.00\n",
      "Step 814/1767: training loss=0.00\n",
      "Step 815/1767: training loss=0.97\n",
      "Step 816/1767: training loss=0.00\n",
      "Step 817/1767: training loss=1.31\n",
      "Step 818/1767: training loss=0.00\n",
      "Step 819/1767: training loss=0.00\n",
      "Step 820/1767: training loss=0.00\n",
      "Step 821/1767: training loss=0.00\n",
      "Step 822/1767: training loss=0.00\n",
      "Step 823/1767: training loss=0.00\n",
      "Step 824/1767: training loss=0.00\n",
      "Step 825/1767: training loss=0.00\n",
      "Step 826/1767: training loss=0.00\n",
      "Step 827/1767: training loss=0.00\n",
      "Step 828/1767: training loss=2.45\n",
      "Step 829/1767: training loss=0.00\n",
      "Step 830/1767: training loss=1.02\n",
      "Step 831/1767: training loss=0.00\n",
      "Step 832/1767: training loss=1.27\n",
      "Step 833/1767: training loss=0.00\n",
      "Step 834/1767: training loss=0.00\n",
      "Step 835/1767: training loss=1.47\n",
      "Step 836/1767: training loss=0.00\n",
      "Step 837/1767: training loss=0.00\n",
      "Step 838/1767: training loss=0.00\n",
      "Step 839/1767: training loss=0.00\n",
      "Step 840/1767: training loss=0.00\n",
      "Step 841/1767: training loss=1.21\n",
      "Step 842/1767: training loss=0.00\n",
      "Step 843/1767: training loss=0.00\n",
      "Step 844/1767: training loss=2.86\n",
      "Step 845/1767: training loss=0.00\n",
      "Step 846/1767: training loss=0.00\n",
      "Step 847/1767: training loss=0.00\n",
      "Step 848/1767: training loss=0.00\n",
      "Step 849/1767: training loss=1.65\n",
      "Step 850/1767: training loss=0.00\n",
      "Step 851/1767: training loss=0.00\n",
      "Step 852/1767: training loss=0.00\n",
      "Step 853/1767: training loss=1.69\n",
      "Step 854/1767: training loss=0.00\n",
      "Step 855/1767: training loss=2.24\n",
      "Step 856/1767: training loss=0.00\n",
      "Step 857/1767: training loss=1.37\n",
      "Step 858/1767: training loss=0.00\n",
      "Step 859/1767: training loss=0.00\n",
      "Step 860/1767: training loss=0.00\n",
      "Step 861/1767: training loss=0.00\n",
      "Step 862/1767: training loss=0.75\n",
      "Step 863/1767: training loss=0.00\n",
      "Step 864/1767: training loss=0.00\n",
      "Step 865/1767: training loss=2.02\n",
      "Step 866/1767: training loss=0.00\n",
      "Step 867/1767: training loss=0.00\n",
      "Step 868/1767: training loss=0.13\n",
      "Step 869/1767: training loss=1.14\n",
      "Step 870/1767: training loss=0.00\n",
      "Step 871/1767: training loss=2.49\n",
      "Step 872/1767: training loss=1.06\n",
      "Step 873/1767: training loss=0.00\n",
      "Step 874/1767: training loss=0.00\n",
      "Step 875/1767: training loss=0.00\n",
      "Step 876/1767: training loss=0.00\n",
      "Step 877/1767: training loss=0.00\n",
      "Step 878/1767: training loss=0.00\n",
      "Step 879/1767: training loss=0.91\n",
      "Step 880/1767: training loss=0.00\n",
      "Step 881/1767: training loss=0.00\n",
      "Step 882/1767: training loss=1.03\n",
      "Step 883/1767: training loss=0.00\n",
      "Step 884/1767: training loss=1.31\n",
      "Step 885/1767: training loss=1.25\n",
      "Step 886/1767: training loss=0.15\n",
      "Step 887/1767: training loss=0.00\n",
      "Step 888/1767: training loss=0.74\n",
      "Step 889/1767: training loss=0.95\n",
      "Step 890/1767: training loss=0.00\n",
      "Step 891/1767: training loss=0.00\n",
      "Step 892/1767: training loss=0.00\n",
      "Step 893/1767: training loss=0.00\n",
      "Step 894/1767: training loss=0.00\n",
      "Step 895/1767: training loss=0.00\n",
      "Step 896/1767: training loss=0.00\n",
      "Step 897/1767: training loss=2.07\n",
      "Step 898/1767: training loss=0.00\n",
      "Step 899/1767: training loss=0.00\n",
      "Step 900/1767: training loss=0.00, validation loss=0.00\n",
      "Step 901/1767: training loss=0.93\n",
      "Step 902/1767: training loss=0.00\n",
      "Step 903/1767: training loss=0.00\n",
      "Step 904/1767: training loss=0.00\n",
      "Step 905/1767: training loss=0.00\n",
      "Step 906/1767: training loss=0.00\n",
      "Step 907/1767: training loss=0.00\n",
      "Step 908/1767: training loss=0.00\n",
      "Step 909/1767: training loss=0.15\n",
      "Step 910/1767: training loss=1.29\n",
      "Step 911/1767: training loss=0.31\n",
      "Step 912/1767: training loss=0.00\n",
      "Step 913/1767: training loss=0.00\n",
      "Step 914/1767: training loss=0.00\n",
      "Step 915/1767: training loss=1.78\n",
      "Step 916/1767: training loss=0.79\n",
      "Step 917/1767: training loss=0.00\n",
      "Step 918/1767: training loss=0.00\n",
      "Step 919/1767: training loss=0.00\n",
      "Step 920/1767: training loss=0.00\n",
      "Step 921/1767: training loss=0.00\n",
      "Step 922/1767: training loss=0.00\n",
      "Step 923/1767: training loss=1.49\n",
      "Step 924/1767: training loss=1.33\n",
      "Step 925/1767: training loss=1.47\n",
      "Step 926/1767: training loss=0.00\n",
      "Step 927/1767: training loss=0.56\n",
      "Step 928/1767: training loss=1.35\n",
      "Step 929/1767: training loss=0.00\n",
      "Step 930/1767: training loss=1.56\n",
      "Step 931/1767: training loss=1.54\n",
      "Step 932/1767: training loss=0.00\n",
      "Step 933/1767: training loss=0.00\n",
      "Step 934/1767: training loss=0.00\n",
      "Step 935/1767: training loss=0.50\n",
      "Step 936/1767: training loss=1.15\n",
      "Step 937/1767: training loss=1.22\n",
      "Step 938/1767: training loss=0.00\n",
      "Step 939/1767: training loss=0.92\n",
      "Step 940/1767: training loss=0.00\n",
      "Step 941/1767: training loss=0.00\n",
      "Step 942/1767: training loss=0.97\n",
      "Step 943/1767: training loss=0.00\n",
      "Step 944/1767: training loss=0.60\n",
      "Step 945/1767: training loss=0.00\n",
      "Step 946/1767: training loss=1.63\n",
      "Step 947/1767: training loss=0.00\n",
      "Step 948/1767: training loss=0.00\n",
      "Step 949/1767: training loss=0.00\n",
      "Step 950/1767: training loss=0.00\n",
      "Step 951/1767: training loss=0.00\n",
      "Step 952/1767: training loss=0.00\n",
      "Step 953/1767: training loss=0.58\n",
      "Step 954/1767: training loss=0.00\n",
      "Step 955/1767: training loss=0.76\n",
      "Step 956/1767: training loss=0.00\n",
      "Step 957/1767: training loss=1.56\n",
      "Step 958/1767: training loss=0.00\n",
      "Step 959/1767: training loss=0.00\n",
      "Step 960/1767: training loss=0.00\n",
      "Step 961/1767: training loss=0.00\n",
      "Step 962/1767: training loss=0.00\n",
      "Step 963/1767: training loss=0.00\n",
      "Step 964/1767: training loss=0.00\n",
      "Step 965/1767: training loss=0.00\n",
      "Step 966/1767: training loss=1.39\n",
      "Step 967/1767: training loss=0.00\n",
      "Step 968/1767: training loss=0.00\n",
      "Step 969/1767: training loss=0.00\n",
      "Step 970/1767: training loss=0.00\n",
      "Step 971/1767: training loss=1.23\n",
      "Step 972/1767: training loss=0.00\n",
      "Step 973/1767: training loss=0.00\n",
      "Step 974/1767: training loss=0.00\n",
      "Step 975/1767: training loss=0.00\n",
      "Step 976/1767: training loss=2.44\n",
      "Step 977/1767: training loss=1.74\n",
      "Step 978/1767: training loss=0.00\n",
      "Step 979/1767: training loss=1.52\n",
      "Step 980/1767: training loss=0.00\n",
      "Step 981/1767: training loss=0.00\n",
      "Step 982/1767: training loss=0.00\n",
      "Step 983/1767: training loss=0.79\n",
      "Step 984/1767: training loss=1.75\n",
      "Step 985/1767: training loss=0.00\n",
      "Step 986/1767: training loss=0.00\n",
      "Step 987/1767: training loss=1.76\n",
      "Step 988/1767: training loss=0.50\n",
      "Step 989/1767: training loss=0.00\n",
      "Step 990/1767: training loss=0.00\n",
      "Step 991/1767: training loss=0.00\n",
      "Step 992/1767: training loss=0.00\n",
      "Step 993/1767: training loss=1.76\n",
      "Step 994/1767: training loss=1.63\n",
      "Step 995/1767: training loss=1.33\n",
      "Step 996/1767: training loss=0.00\n",
      "Step 997/1767: training loss=0.00\n",
      "Step 998/1767: training loss=0.00\n",
      "Step 999/1767: training loss=0.29\n",
      "Step 1000/1767: training loss=0.00, validation loss=2.00\n",
      "Step 1001/1767: training loss=1.10\n",
      "Step 1002/1767: training loss=0.00\n",
      "Step 1003/1767: training loss=1.39\n",
      "Step 1004/1767: training loss=0.17\n",
      "Step 1005/1767: training loss=0.00\n",
      "Step 1006/1767: training loss=0.79\n",
      "Step 1007/1767: training loss=0.00\n",
      "Step 1008/1767: training loss=1.10\n",
      "Step 1009/1767: training loss=0.59\n",
      "Step 1010/1767: training loss=0.00\n",
      "Step 1011/1767: training loss=0.00\n",
      "Step 1012/1767: training loss=1.12\n",
      "Step 1013/1767: training loss=0.00\n",
      "Step 1014/1767: training loss=1.31\n",
      "Step 1015/1767: training loss=1.35\n",
      "Step 1016/1767: training loss=0.00\n",
      "Step 1017/1767: training loss=2.16\n",
      "Step 1018/1767: training loss=0.00\n",
      "Step 1019/1767: training loss=0.11\n",
      "Step 1020/1767: training loss=0.00\n",
      "Step 1021/1767: training loss=0.08\n",
      "Step 1022/1767: training loss=0.01\n",
      "Step 1023/1767: training loss=0.00\n",
      "Step 1024/1767: training loss=0.00\n",
      "Step 1025/1767: training loss=0.87\n",
      "Step 1026/1767: training loss=0.00\n",
      "Step 1027/1767: training loss=0.00\n",
      "Step 1028/1767: training loss=0.00\n",
      "Step 1029/1767: training loss=0.00\n",
      "Step 1030/1767: training loss=0.00\n",
      "Step 1031/1767: training loss=0.00\n",
      "Step 1032/1767: training loss=0.00\n",
      "Step 1033/1767: training loss=0.00\n",
      "Step 1034/1767: training loss=0.00\n",
      "Step 1035/1767: training loss=0.89\n",
      "Step 1036/1767: training loss=0.00\n",
      "Step 1037/1767: training loss=0.78\n",
      "Step 1038/1767: training loss=0.00\n",
      "Step 1039/1767: training loss=0.00\n",
      "Step 1040/1767: training loss=0.00\n",
      "Step 1041/1767: training loss=1.66\n",
      "Step 1042/1767: training loss=0.00\n",
      "Step 1043/1767: training loss=0.00\n",
      "Step 1044/1767: training loss=0.00\n",
      "Step 1045/1767: training loss=0.00\n",
      "Step 1046/1767: training loss=0.00\n",
      "Step 1047/1767: training loss=0.00\n",
      "Step 1048/1767: training loss=0.00\n",
      "Step 1049/1767: training loss=0.00\n",
      "Step 1050/1767: training loss=0.00\n",
      "Step 1051/1767: training loss=0.27\n",
      "Step 1052/1767: training loss=0.00\n",
      "Step 1053/1767: training loss=0.00\n",
      "Step 1054/1767: training loss=0.00\n",
      "Step 1055/1767: training loss=0.00\n",
      "Step 1056/1767: training loss=0.00\n",
      "Step 1057/1767: training loss=0.00\n",
      "Step 1058/1767: training loss=0.00\n",
      "Step 1059/1767: training loss=0.00\n",
      "Step 1060/1767: training loss=0.00\n",
      "Step 1061/1767: training loss=0.00\n",
      "Step 1062/1767: training loss=0.00\n",
      "Step 1063/1767: training loss=0.00\n",
      "Step 1064/1767: training loss=0.00\n",
      "Step 1065/1767: training loss=0.00\n",
      "Step 1066/1767: training loss=1.95\n",
      "Step 1067/1767: training loss=0.00\n",
      "Step 1068/1767: training loss=0.00\n",
      "Step 1069/1767: training loss=1.31\n",
      "Step 1070/1767: training loss=0.00\n",
      "Step 1071/1767: training loss=0.94\n",
      "Step 1072/1767: training loss=0.00\n",
      "Step 1073/1767: training loss=0.00\n",
      "Step 1074/1767: training loss=0.00\n",
      "Step 1075/1767: training loss=0.00\n",
      "Step 1076/1767: training loss=1.59\n",
      "Step 1077/1767: training loss=0.00\n",
      "Step 1078/1767: training loss=0.26\n",
      "Step 1079/1767: training loss=0.00\n",
      "Step 1080/1767: training loss=0.00\n",
      "Step 1081/1767: training loss=0.00\n",
      "Step 1082/1767: training loss=0.00\n",
      "Step 1083/1767: training loss=0.09\n",
      "Step 1084/1767: training loss=0.00\n",
      "Step 1085/1767: training loss=1.53\n",
      "Step 1086/1767: training loss=0.00\n",
      "Step 1087/1767: training loss=0.00\n",
      "Step 1088/1767: training loss=0.00\n",
      "Step 1089/1767: training loss=0.00\n",
      "Step 1090/1767: training loss=0.00\n",
      "Step 1091/1767: training loss=0.00\n",
      "Step 1092/1767: training loss=0.88\n",
      "Step 1093/1767: training loss=2.84\n",
      "Step 1094/1767: training loss=0.00\n",
      "Step 1095/1767: training loss=0.07\n",
      "Step 1096/1767: training loss=3.07\n",
      "Step 1097/1767: training loss=0.00\n",
      "Step 1098/1767: training loss=0.57\n",
      "Step 1099/1767: training loss=0.00\n",
      "Step 1100/1767: training loss=0.00, validation loss=0.00\n",
      "Step 1101/1767: training loss=1.03\n",
      "Step 1102/1767: training loss=0.00\n",
      "Step 1103/1767: training loss=1.36\n",
      "Step 1104/1767: training loss=1.50\n",
      "Step 1105/1767: training loss=0.00\n",
      "Step 1106/1767: training loss=0.00\n",
      "Step 1107/1767: training loss=0.00\n",
      "Step 1108/1767: training loss=1.23\n",
      "Step 1109/1767: training loss=0.00\n",
      "Step 1110/1767: training loss=0.57\n",
      "Step 1111/1767: training loss=1.66\n",
      "Step 1112/1767: training loss=0.01\n",
      "Step 1113/1767: training loss=0.00\n",
      "Step 1114/1767: training loss=1.24\n",
      "Step 1115/1767: training loss=0.01\n",
      "Step 1116/1767: training loss=1.47\n",
      "Step 1117/1767: training loss=0.00\n",
      "Step 1118/1767: training loss=0.00\n",
      "Step 1119/1767: training loss=0.00\n",
      "Step 1120/1767: training loss=0.00\n",
      "Step 1121/1767: training loss=0.00\n",
      "Step 1122/1767: training loss=0.00\n",
      "Step 1123/1767: training loss=0.00\n",
      "Step 1124/1767: training loss=0.00\n",
      "Step 1125/1767: training loss=2.19\n",
      "Step 1126/1767: training loss=0.00\n",
      "Step 1127/1767: training loss=0.00\n",
      "Step 1128/1767: training loss=2.18\n",
      "Step 1129/1767: training loss=0.00\n",
      "Step 1130/1767: training loss=0.00\n",
      "Step 1131/1767: training loss=0.00\n",
      "Step 1132/1767: training loss=0.00\n",
      "Step 1133/1767: training loss=0.33\n",
      "Step 1134/1767: training loss=0.00\n",
      "Step 1135/1767: training loss=0.01\n",
      "Step 1136/1767: training loss=0.57\n",
      "Step 1137/1767: training loss=1.88\n",
      "Step 1138/1767: training loss=0.00\n",
      "Step 1139/1767: training loss=0.00\n",
      "Step 1140/1767: training loss=0.00\n",
      "Step 1141/1767: training loss=0.00\n",
      "Step 1142/1767: training loss=0.00\n",
      "Step 1143/1767: training loss=0.00\n",
      "Step 1144/1767: training loss=0.00\n",
      "Step 1145/1767: training loss=0.00\n",
      "Step 1146/1767: training loss=1.04\n",
      "Step 1147/1767: training loss=0.00\n",
      "Step 1148/1767: training loss=1.67\n",
      "Step 1149/1767: training loss=1.95\n",
      "Step 1150/1767: training loss=0.00\n",
      "Step 1151/1767: training loss=0.00\n",
      "Step 1152/1767: training loss=1.54\n",
      "Step 1153/1767: training loss=0.00\n",
      "Step 1154/1767: training loss=0.00\n",
      "Step 1155/1767: training loss=0.00\n",
      "Step 1156/1767: training loss=0.00\n",
      "Step 1157/1767: training loss=1.71\n",
      "Step 1158/1767: training loss=0.00\n",
      "Step 1159/1767: training loss=1.29\n",
      "Step 1160/1767: training loss=1.48\n",
      "Step 1161/1767: training loss=0.23\n",
      "Step 1162/1767: training loss=1.10\n",
      "Step 1163/1767: training loss=1.31\n",
      "Step 1164/1767: training loss=0.21\n",
      "Step 1165/1767: training loss=0.00\n",
      "Step 1166/1767: training loss=0.00\n",
      "Step 1167/1767: training loss=0.00\n",
      "Step 1168/1767: training loss=0.00\n",
      "Step 1169/1767: training loss=0.00\n",
      "Step 1170/1767: training loss=1.11\n",
      "Step 1171/1767: training loss=1.18\n",
      "Step 1172/1767: training loss=0.00\n",
      "Step 1173/1767: training loss=0.00\n",
      "Step 1174/1767: training loss=0.00\n",
      "Step 1175/1767: training loss=1.18\n",
      "Step 1176/1767: training loss=0.00\n",
      "Step 1177/1767: training loss=0.00\n",
      "Step 1178/1767: training loss=0.00, full validation loss=0.62\n",
      "Step 1179/1767: training loss=0.00\n",
      "Step 1180/1767: training loss=0.00\n",
      "Step 1181/1767: training loss=0.00\n",
      "Step 1182/1767: training loss=1.57\n",
      "Step 1183/1767: training loss=1.51\n",
      "Step 1184/1767: training loss=0.00\n",
      "Step 1185/1767: training loss=0.00\n",
      "Step 1186/1767: training loss=0.00\n",
      "Step 1187/1767: training loss=0.00\n",
      "Step 1188/1767: training loss=1.45\n",
      "Step 1189/1767: training loss=0.00\n",
      "Step 1190/1767: training loss=1.41\n",
      "Step 1191/1767: training loss=0.00\n",
      "Step 1192/1767: training loss=0.00\n",
      "Step 1193/1767: training loss=0.46\n",
      "Step 1194/1767: training loss=1.63\n",
      "Step 1195/1767: training loss=0.00\n",
      "Step 1196/1767: training loss=0.00\n",
      "Step 1197/1767: training loss=0.00\n",
      "Step 1198/1767: training loss=0.99\n",
      "Step 1199/1767: training loss=1.53\n",
      "Step 1200/1767: training loss=1.35, validation loss=1.21\n",
      "Step 1201/1767: training loss=1.13\n",
      "Step 1202/1767: training loss=0.00\n",
      "Step 1203/1767: training loss=1.14\n",
      "Step 1204/1767: training loss=0.88\n",
      "Step 1205/1767: training loss=0.00\n",
      "Step 1206/1767: training loss=0.00\n",
      "Step 1207/1767: training loss=0.00\n",
      "Step 1208/1767: training loss=0.00\n",
      "Step 1209/1767: training loss=1.59\n",
      "Step 1210/1767: training loss=0.00\n",
      "Step 1211/1767: training loss=0.00\n",
      "Step 1212/1767: training loss=0.00\n",
      "Step 1213/1767: training loss=0.78\n",
      "Step 1214/1767: training loss=1.36\n",
      "Step 1215/1767: training loss=0.00\n",
      "Step 1216/1767: training loss=0.00\n",
      "Step 1217/1767: training loss=0.00\n",
      "Step 1218/1767: training loss=0.00\n",
      "Step 1219/1767: training loss=1.56\n",
      "Step 1220/1767: training loss=0.00\n",
      "Step 1221/1767: training loss=0.00\n",
      "Step 1222/1767: training loss=0.00\n",
      "Step 1223/1767: training loss=1.42\n",
      "Step 1224/1767: training loss=0.00\n",
      "Step 1225/1767: training loss=0.00\n",
      "Step 1226/1767: training loss=0.00\n",
      "Step 1227/1767: training loss=0.00\n",
      "Step 1228/1767: training loss=0.00\n",
      "Step 1229/1767: training loss=0.00\n",
      "Step 1230/1767: training loss=0.64\n",
      "Step 1231/1767: training loss=0.00\n",
      "Step 1232/1767: training loss=0.00\n",
      "Step 1233/1767: training loss=0.00\n",
      "Step 1234/1767: training loss=0.00\n",
      "Step 1235/1767: training loss=0.00\n",
      "Step 1236/1767: training loss=0.00\n",
      "Step 1237/1767: training loss=0.00\n",
      "Step 1238/1767: training loss=0.00\n",
      "Step 1239/1767: training loss=0.00\n",
      "Step 1240/1767: training loss=0.00\n",
      "Step 1241/1767: training loss=1.85\n",
      "Step 1242/1767: training loss=0.00\n",
      "Step 1243/1767: training loss=0.00\n",
      "Step 1244/1767: training loss=0.00\n",
      "Step 1245/1767: training loss=0.00\n",
      "Step 1246/1767: training loss=0.00\n",
      "Step 1247/1767: training loss=0.00\n",
      "Step 1248/1767: training loss=0.00\n",
      "Step 1249/1767: training loss=0.00\n",
      "Step 1250/1767: training loss=1.50\n",
      "Step 1251/1767: training loss=0.00\n",
      "Step 1252/1767: training loss=0.00\n",
      "Step 1253/1767: training loss=3.82\n",
      "Step 1254/1767: training loss=0.91\n",
      "Step 1255/1767: training loss=0.00\n",
      "Step 1256/1767: training loss=0.00\n",
      "Step 1257/1767: training loss=1.49\n",
      "Step 1258/1767: training loss=0.00\n",
      "Step 1259/1767: training loss=0.00\n",
      "Step 1260/1767: training loss=1.97\n",
      "Step 1261/1767: training loss=0.00\n",
      "Step 1262/1767: training loss=0.00\n",
      "Step 1263/1767: training loss=0.00\n",
      "Step 1264/1767: training loss=2.44\n",
      "Step 1265/1767: training loss=0.54\n",
      "Step 1266/1767: training loss=0.00\n",
      "Step 1267/1767: training loss=0.00\n",
      "Step 1268/1767: training loss=0.12\n",
      "Step 1269/1767: training loss=1.32\n",
      "Step 1270/1767: training loss=0.00\n",
      "Step 1271/1767: training loss=0.04\n",
      "Step 1272/1767: training loss=0.00\n",
      "Step 1273/1767: training loss=0.00\n",
      "Step 1274/1767: training loss=0.00\n",
      "Step 1275/1767: training loss=1.15\n",
      "Step 1276/1767: training loss=1.37\n",
      "Step 1277/1767: training loss=0.00\n",
      "Step 1278/1767: training loss=0.22\n",
      "Step 1279/1767: training loss=0.00\n",
      "Step 1280/1767: training loss=0.00\n",
      "Step 1281/1767: training loss=0.00\n",
      "Step 1282/1767: training loss=0.00\n",
      "Step 1283/1767: training loss=0.00\n",
      "Step 1284/1767: training loss=1.66\n",
      "Step 1285/1767: training loss=1.81\n",
      "Step 1286/1767: training loss=0.00\n",
      "Step 1287/1767: training loss=0.55\n",
      "Step 1288/1767: training loss=0.00\n",
      "Step 1289/1767: training loss=0.00\n",
      "Step 1290/1767: training loss=0.00\n",
      "Step 1291/1767: training loss=1.60\n",
      "Step 1292/1767: training loss=0.00\n",
      "Step 1293/1767: training loss=0.00\n",
      "Step 1294/1767: training loss=0.00\n",
      "Step 1295/1767: training loss=0.00\n",
      "Step 1296/1767: training loss=1.33\n",
      "Step 1297/1767: training loss=1.59\n",
      "Step 1298/1767: training loss=0.00\n",
      "Step 1299/1767: training loss=1.44\n",
      "Step 1300/1767: training loss=0.00, validation loss=0.00\n",
      "Step 1301/1767: training loss=0.60\n",
      "Step 1302/1767: training loss=0.00\n",
      "Step 1303/1767: training loss=0.00\n",
      "Step 1304/1767: training loss=0.00\n",
      "Step 1305/1767: training loss=0.00\n",
      "Step 1306/1767: training loss=0.00\n",
      "Step 1307/1767: training loss=0.00\n",
      "Step 1308/1767: training loss=0.00\n",
      "Step 1309/1767: training loss=0.00\n",
      "Step 1310/1767: training loss=0.00\n",
      "Step 1311/1767: training loss=0.00\n",
      "Step 1312/1767: training loss=0.00\n",
      "Step 1313/1767: training loss=0.00\n",
      "Step 1314/1767: training loss=0.00\n",
      "Step 1315/1767: training loss=0.00\n",
      "Step 1316/1767: training loss=0.00\n",
      "Step 1317/1767: training loss=0.00\n",
      "Step 1318/1767: training loss=1.46\n",
      "Step 1319/1767: training loss=0.04\n",
      "Step 1320/1767: training loss=0.00\n",
      "Step 1321/1767: training loss=0.00\n",
      "Step 1322/1767: training loss=0.00\n",
      "Step 1323/1767: training loss=0.00\n",
      "Step 1324/1767: training loss=0.00\n",
      "Step 1325/1767: training loss=1.84\n",
      "Step 1326/1767: training loss=0.00\n",
      "Step 1327/1767: training loss=0.00\n",
      "Step 1328/1767: training loss=0.00\n",
      "Step 1329/1767: training loss=1.62\n",
      "Step 1330/1767: training loss=0.00\n",
      "Step 1331/1767: training loss=0.00\n",
      "Step 1332/1767: training loss=0.00\n",
      "Step 1333/1767: training loss=0.17\n",
      "Step 1334/1767: training loss=0.00\n",
      "Step 1335/1767: training loss=2.87\n",
      "Step 1336/1767: training loss=0.00\n",
      "Step 1337/1767: training loss=0.00\n",
      "Step 1338/1767: training loss=0.50\n",
      "Step 1339/1767: training loss=0.00\n",
      "Step 1340/1767: training loss=0.00\n",
      "Step 1341/1767: training loss=0.00\n",
      "Step 1342/1767: training loss=0.00\n",
      "Step 1343/1767: training loss=0.00\n",
      "Step 1344/1767: training loss=1.57\n",
      "Step 1345/1767: training loss=0.24\n",
      "Step 1346/1767: training loss=0.00\n",
      "Step 1347/1767: training loss=0.00\n",
      "Step 1348/1767: training loss=0.00\n",
      "Step 1349/1767: training loss=0.00\n",
      "Step 1350/1767: training loss=1.70\n",
      "Step 1351/1767: training loss=1.18\n",
      "Step 1352/1767: training loss=0.00\n",
      "Step 1353/1767: training loss=0.00\n",
      "Step 1354/1767: training loss=0.00\n",
      "Step 1355/1767: training loss=1.39\n",
      "Step 1356/1767: training loss=0.00\n",
      "Step 1357/1767: training loss=1.91\n",
      "Step 1358/1767: training loss=0.00\n",
      "Step 1359/1767: training loss=1.43\n",
      "Step 1360/1767: training loss=0.00\n",
      "Step 1361/1767: training loss=0.00\n",
      "Step 1362/1767: training loss=0.00\n",
      "Step 1363/1767: training loss=1.30\n",
      "Step 1364/1767: training loss=0.00\n",
      "Step 1365/1767: training loss=0.32\n",
      "Step 1366/1767: training loss=0.00\n",
      "Step 1367/1767: training loss=0.00\n",
      "Step 1368/1767: training loss=0.00\n",
      "Step 1369/1767: training loss=0.86\n",
      "Step 1370/1767: training loss=1.62\n",
      "Step 1371/1767: training loss=0.00\n",
      "Step 1372/1767: training loss=1.43\n",
      "Step 1373/1767: training loss=0.00\n",
      "Step 1374/1767: training loss=0.00\n",
      "Step 1375/1767: training loss=0.00\n",
      "Step 1376/1767: training loss=1.70\n",
      "Step 1377/1767: training loss=0.00\n",
      "Step 1378/1767: training loss=1.17\n",
      "Step 1379/1767: training loss=0.00\n",
      "Step 1380/1767: training loss=0.00\n",
      "Step 1381/1767: training loss=0.00\n",
      "Step 1382/1767: training loss=0.00\n",
      "Step 1383/1767: training loss=0.00\n",
      "Step 1384/1767: training loss=0.00\n",
      "Step 1385/1767: training loss=0.00\n",
      "Step 1386/1767: training loss=0.00\n",
      "Step 1387/1767: training loss=0.00\n",
      "Step 1388/1767: training loss=0.00\n",
      "Step 1389/1767: training loss=0.00\n",
      "Step 1390/1767: training loss=0.00\n",
      "Step 1391/1767: training loss=0.00\n",
      "Step 1392/1767: training loss=0.64\n",
      "Step 1393/1767: training loss=1.44\n",
      "Step 1394/1767: training loss=0.00\n",
      "Step 1395/1767: training loss=0.00\n",
      "Step 1396/1767: training loss=0.00\n",
      "Step 1397/1767: training loss=0.00\n",
      "Step 1398/1767: training loss=0.00\n",
      "Step 1399/1767: training loss=0.00\n",
      "Step 1400/1767: training loss=1.24, validation loss=0.00\n",
      "Step 1401/1767: training loss=0.00\n",
      "Step 1402/1767: training loss=0.00\n",
      "Step 1403/1767: training loss=0.00\n",
      "Step 1404/1767: training loss=0.00\n",
      "Step 1405/1767: training loss=0.00\n",
      "Step 1406/1767: training loss=0.00\n",
      "Step 1407/1767: training loss=0.00\n",
      "Step 1408/1767: training loss=0.74\n",
      "Step 1409/1767: training loss=0.00\n",
      "Step 1410/1767: training loss=0.38\n",
      "Step 1411/1767: training loss=1.31\n",
      "Step 1412/1767: training loss=0.00\n",
      "Step 1413/1767: training loss=0.00\n",
      "Step 1414/1767: training loss=1.10\n",
      "Step 1415/1767: training loss=0.00\n",
      "Step 1416/1767: training loss=0.44\n",
      "Step 1417/1767: training loss=0.00\n",
      "Step 1418/1767: training loss=0.16\n",
      "Step 1419/1767: training loss=0.00\n",
      "Step 1420/1767: training loss=0.00\n",
      "Step 1421/1767: training loss=0.00\n",
      "Step 1422/1767: training loss=0.00\n",
      "Step 1423/1767: training loss=0.58\n",
      "Step 1424/1767: training loss=1.46\n",
      "Step 1425/1767: training loss=1.44\n",
      "Step 1426/1767: training loss=0.00\n",
      "Step 1427/1767: training loss=0.00\n",
      "Step 1428/1767: training loss=0.00\n",
      "Step 1429/1767: training loss=0.00\n",
      "Step 1430/1767: training loss=0.00\n",
      "Step 1431/1767: training loss=0.00\n",
      "Step 1432/1767: training loss=1.58\n",
      "Step 1433/1767: training loss=0.00\n",
      "Step 1434/1767: training loss=0.62\n",
      "Step 1435/1767: training loss=0.00\n",
      "Step 1436/1767: training loss=0.00\n",
      "Step 1437/1767: training loss=0.00\n",
      "Step 1438/1767: training loss=0.00\n",
      "Step 1439/1767: training loss=0.00\n",
      "Step 1440/1767: training loss=0.00\n",
      "Step 1441/1767: training loss=0.04\n",
      "Step 1442/1767: training loss=0.66\n",
      "Step 1443/1767: training loss=0.00\n",
      "Step 1444/1767: training loss=0.00\n",
      "Step 1445/1767: training loss=1.45\n",
      "Step 1446/1767: training loss=0.25\n",
      "Step 1447/1767: training loss=0.00\n",
      "Step 1448/1767: training loss=0.00\n",
      "Step 1449/1767: training loss=0.00\n",
      "Step 1450/1767: training loss=0.00\n",
      "Step 1451/1767: training loss=0.00\n",
      "Step 1452/1767: training loss=0.00\n",
      "Step 1453/1767: training loss=0.00\n",
      "Step 1454/1767: training loss=0.00\n",
      "Step 1455/1767: training loss=0.00\n",
      "Step 1456/1767: training loss=0.00\n",
      "Step 1457/1767: training loss=0.00\n",
      "Step 1458/1767: training loss=0.48\n",
      "Step 1459/1767: training loss=0.00\n",
      "Step 1460/1767: training loss=0.00\n",
      "Step 1461/1767: training loss=0.00\n",
      "Step 1462/1767: training loss=0.00\n",
      "Step 1463/1767: training loss=0.00\n",
      "Step 1464/1767: training loss=0.00\n",
      "Step 1465/1767: training loss=1.78\n",
      "Step 1466/1767: training loss=0.00\n",
      "Step 1467/1767: training loss=0.00\n",
      "Step 1468/1767: training loss=1.59\n",
      "Step 1469/1767: training loss=0.00\n",
      "Step 1470/1767: training loss=0.00\n",
      "Step 1471/1767: training loss=1.68\n",
      "Step 1472/1767: training loss=0.00\n",
      "Step 1473/1767: training loss=0.00\n",
      "Step 1474/1767: training loss=0.00\n",
      "Step 1475/1767: training loss=0.00\n",
      "Step 1476/1767: training loss=0.00\n",
      "Step 1477/1767: training loss=0.00\n",
      "Step 1478/1767: training loss=1.06\n",
      "Step 1479/1767: training loss=0.00\n",
      "Step 1480/1767: training loss=0.00\n",
      "Step 1481/1767: training loss=0.00\n",
      "Step 1482/1767: training loss=1.41\n",
      "Step 1483/1767: training loss=0.00\n",
      "Step 1484/1767: training loss=0.00\n",
      "Step 1485/1767: training loss=0.00\n",
      "Step 1486/1767: training loss=0.00\n",
      "Step 1487/1767: training loss=0.35\n",
      "Step 1488/1767: training loss=0.00\n",
      "Step 1489/1767: training loss=0.00\n",
      "Step 1490/1767: training loss=0.00\n",
      "Step 1491/1767: training loss=0.59\n",
      "Step 1492/1767: training loss=0.00\n",
      "Step 1493/1767: training loss=0.00\n",
      "Step 1494/1767: training loss=0.46\n",
      "Step 1495/1767: training loss=0.00\n",
      "Step 1496/1767: training loss=0.00\n",
      "Step 1497/1767: training loss=1.15\n",
      "Step 1498/1767: training loss=0.00\n",
      "Step 1499/1767: training loss=0.00\n",
      "Step 1500/1767: training loss=0.00, validation loss=0.00\n",
      "Step 1501/1767: training loss=0.00\n",
      "Step 1502/1767: training loss=0.00\n",
      "Step 1503/1767: training loss=0.79\n",
      "Step 1504/1767: training loss=0.00\n",
      "Step 1505/1767: training loss=0.00\n",
      "Step 1506/1767: training loss=0.11\n",
      "Step 1507/1767: training loss=0.00\n",
      "Step 1508/1767: training loss=0.00\n",
      "Step 1509/1767: training loss=0.00\n",
      "Step 1510/1767: training loss=1.21\n",
      "Step 1511/1767: training loss=0.00\n",
      "Step 1512/1767: training loss=0.80\n",
      "Step 1513/1767: training loss=0.00\n",
      "Step 1514/1767: training loss=0.00\n",
      "Step 1515/1767: training loss=0.00\n",
      "Step 1516/1767: training loss=1.58\n",
      "Step 1517/1767: training loss=0.01\n",
      "Step 1518/1767: training loss=0.00\n",
      "Step 1519/1767: training loss=0.00\n",
      "Step 1520/1767: training loss=0.00\n",
      "Step 1521/1767: training loss=2.04\n",
      "Step 1522/1767: training loss=3.09\n",
      "Step 1523/1767: training loss=1.64\n",
      "Step 1524/1767: training loss=0.00\n",
      "Step 1525/1767: training loss=0.00\n",
      "Step 1526/1767: training loss=0.51\n",
      "Step 1527/1767: training loss=0.07\n",
      "Step 1528/1767: training loss=0.00\n",
      "Step 1529/1767: training loss=0.00\n",
      "Step 1530/1767: training loss=0.00\n",
      "Step 1531/1767: training loss=0.00\n",
      "Step 1532/1767: training loss=0.00\n",
      "Step 1533/1767: training loss=0.00\n",
      "Step 1534/1767: training loss=0.00\n",
      "Step 1535/1767: training loss=0.00\n",
      "Step 1536/1767: training loss=0.00\n",
      "Step 1537/1767: training loss=0.00\n",
      "Step 1538/1767: training loss=0.00\n",
      "Step 1539/1767: training loss=0.00\n",
      "Step 1540/1767: training loss=0.00\n",
      "Step 1541/1767: training loss=0.00\n",
      "Step 1542/1767: training loss=0.00\n",
      "Step 1543/1767: training loss=1.43\n",
      "Step 1544/1767: training loss=1.79\n",
      "Step 1545/1767: training loss=0.00\n",
      "Step 1546/1767: training loss=0.00\n",
      "Step 1547/1767: training loss=0.00\n",
      "Step 1548/1767: training loss=0.52\n",
      "Step 1549/1767: training loss=0.00\n",
      "Step 1550/1767: training loss=0.00\n",
      "Step 1551/1767: training loss=0.00\n",
      "Step 1552/1767: training loss=0.00\n",
      "Step 1553/1767: training loss=0.00\n",
      "Step 1554/1767: training loss=0.00\n",
      "Step 1555/1767: training loss=0.00\n",
      "Step 1556/1767: training loss=0.00\n",
      "Step 1557/1767: training loss=0.05\n",
      "Step 1558/1767: training loss=1.66\n",
      "Step 1559/1767: training loss=0.00\n",
      "Step 1560/1767: training loss=0.33\n",
      "Step 1561/1767: training loss=1.71\n",
      "Step 1562/1767: training loss=0.01\n",
      "Step 1563/1767: training loss=0.00\n",
      "Step 1564/1767: training loss=0.86\n",
      "Step 1565/1767: training loss=0.00\n",
      "Step 1566/1767: training loss=1.19\n",
      "Step 1567/1767: training loss=0.00\n",
      "Step 1568/1767: training loss=0.55\n",
      "Step 1569/1767: training loss=0.00\n",
      "Step 1570/1767: training loss=0.01\n",
      "Step 1571/1767: training loss=0.00\n",
      "Step 1572/1767: training loss=0.00\n",
      "Step 1573/1767: training loss=0.67\n",
      "Step 1574/1767: training loss=0.00\n",
      "Step 1575/1767: training loss=0.84\n",
      "Step 1576/1767: training loss=0.00\n",
      "Step 1577/1767: training loss=1.26\n",
      "Step 1578/1767: training loss=0.20\n",
      "Step 1579/1767: training loss=0.00\n",
      "Step 1580/1767: training loss=1.95\n",
      "Step 1581/1767: training loss=0.00\n",
      "Step 1582/1767: training loss=0.00\n",
      "Step 1583/1767: training loss=0.00\n",
      "Step 1584/1767: training loss=0.00\n",
      "Step 1585/1767: training loss=0.00\n",
      "Step 1586/1767: training loss=0.00\n",
      "Step 1587/1767: training loss=0.00\n",
      "Step 1588/1767: training loss=0.00\n",
      "Step 1589/1767: training loss=0.00\n",
      "Step 1590/1767: training loss=0.00\n",
      "Step 1591/1767: training loss=2.08\n",
      "Step 1592/1767: training loss=0.00\n",
      "Step 1593/1767: training loss=0.06\n",
      "Step 1594/1767: training loss=0.00\n",
      "Step 1595/1767: training loss=0.00\n",
      "Step 1596/1767: training loss=0.00\n",
      "Step 1597/1767: training loss=1.48\n",
      "Step 1598/1767: training loss=0.00\n",
      "Step 1599/1767: training loss=1.84\n",
      "Step 1600/1767: training loss=0.04, validation loss=1.35\n",
      "Step 1601/1767: training loss=0.00\n",
      "Step 1602/1767: training loss=2.02\n",
      "Step 1603/1767: training loss=0.16\n",
      "Step 1604/1767: training loss=1.55\n",
      "Step 1605/1767: training loss=0.00\n",
      "Step 1606/1767: training loss=0.00\n",
      "Step 1607/1767: training loss=1.72\n",
      "Step 1608/1767: training loss=0.00\n",
      "Step 1609/1767: training loss=0.90\n",
      "Step 1610/1767: training loss=0.00\n",
      "Step 1611/1767: training loss=1.60\n",
      "Step 1612/1767: training loss=0.00\n",
      "Step 1613/1767: training loss=0.01\n",
      "Step 1614/1767: training loss=0.00\n",
      "Step 1615/1767: training loss=0.00\n",
      "Step 1616/1767: training loss=0.96\n",
      "Step 1617/1767: training loss=1.10\n",
      "Step 1618/1767: training loss=1.45\n",
      "Step 1619/1767: training loss=0.00\n",
      "Step 1620/1767: training loss=0.00\n",
      "Step 1621/1767: training loss=0.00\n",
      "Step 1622/1767: training loss=0.00\n",
      "Step 1623/1767: training loss=0.00\n",
      "Step 1624/1767: training loss=0.75\n",
      "Step 1625/1767: training loss=0.01\n",
      "Step 1626/1767: training loss=0.00\n",
      "Step 1627/1767: training loss=0.00\n",
      "Step 1628/1767: training loss=0.00\n",
      "Step 1629/1767: training loss=0.00\n",
      "Step 1630/1767: training loss=0.00\n",
      "Step 1631/1767: training loss=0.00\n",
      "Step 1632/1767: training loss=0.74\n",
      "Step 1633/1767: training loss=0.00\n",
      "Step 1634/1767: training loss=1.40\n",
      "Step 1635/1767: training loss=0.00\n",
      "Step 1636/1767: training loss=0.86\n",
      "Step 1637/1767: training loss=1.43\n",
      "Step 1638/1767: training loss=0.00\n",
      "Step 1639/1767: training loss=0.00\n",
      "Step 1640/1767: training loss=0.00\n",
      "Step 1641/1767: training loss=0.00\n",
      "Step 1642/1767: training loss=0.33\n",
      "Step 1643/1767: training loss=1.39\n",
      "Step 1644/1767: training loss=0.00\n",
      "Step 1645/1767: training loss=1.46\n",
      "Step 1646/1767: training loss=0.00\n",
      "Step 1647/1767: training loss=0.00\n",
      "Step 1648/1767: training loss=0.00\n",
      "Step 1649/1767: training loss=0.15\n",
      "Step 1650/1767: training loss=0.00\n",
      "Step 1651/1767: training loss=0.00\n",
      "Step 1652/1767: training loss=1.12\n",
      "Step 1653/1767: training loss=0.00\n",
      "Step 1654/1767: training loss=0.00\n",
      "Step 1655/1767: training loss=0.66\n",
      "Step 1656/1767: training loss=0.00\n",
      "Step 1657/1767: training loss=0.00\n",
      "Step 1658/1767: training loss=0.00\n",
      "Step 1659/1767: training loss=0.02\n",
      "Step 1660/1767: training loss=0.00\n",
      "Step 1661/1767: training loss=0.00\n",
      "Step 1662/1767: training loss=0.00\n",
      "Step 1663/1767: training loss=0.00\n",
      "Step 1664/1767: training loss=0.00\n",
      "Step 1665/1767: training loss=0.00\n",
      "Step 1666/1767: training loss=0.00\n",
      "Step 1667/1767: training loss=0.00\n",
      "Step 1668/1767: training loss=1.25\n",
      "Step 1669/1767: training loss=0.98\n",
      "Step 1670/1767: training loss=1.44\n",
      "Step 1671/1767: training loss=0.92\n",
      "Step 1672/1767: training loss=0.00\n",
      "Step 1673/1767: training loss=1.44\n",
      "Step 1674/1767: training loss=1.30\n",
      "Step 1675/1767: training loss=0.00\n",
      "Step 1676/1767: training loss=0.00\n",
      "Step 1677/1767: training loss=0.00\n",
      "Step 1678/1767: training loss=1.44\n",
      "Step 1679/1767: training loss=0.00\n",
      "Step 1680/1767: training loss=0.00\n",
      "Step 1681/1767: training loss=0.00\n",
      "Step 1682/1767: training loss=0.00\n",
      "Step 1683/1767: training loss=0.00\n",
      "Step 1684/1767: training loss=0.00\n",
      "Step 1685/1767: training loss=0.00\n",
      "Step 1686/1767: training loss=0.00\n",
      "Step 1687/1767: training loss=0.25\n",
      "Step 1688/1767: training loss=0.02\n",
      "Step 1689/1767: training loss=0.00\n",
      "Step 1690/1767: training loss=0.00\n",
      "Step 1691/1767: training loss=2.73\n",
      "Step 1692/1767: training loss=0.14\n",
      "Step 1693/1767: training loss=0.20\n",
      "Step 1694/1767: training loss=1.21\n",
      "Step 1695/1767: training loss=0.00\n",
      "Step 1696/1767: training loss=0.00\n",
      "Step 1697/1767: training loss=0.05\n",
      "Step 1698/1767: training loss=0.00\n",
      "Step 1699/1767: training loss=0.00\n",
      "Step 1700/1767: training loss=1.43, validation loss=0.00\n",
      "Step 1701/1767: training loss=0.00\n",
      "Step 1702/1767: training loss=0.00\n",
      "Step 1703/1767: training loss=0.53\n",
      "Step 1704/1767: training loss=1.52\n",
      "Step 1705/1767: training loss=3.29\n",
      "Step 1706/1767: training loss=0.00\n",
      "Step 1707/1767: training loss=0.41\n",
      "Step 1708/1767: training loss=0.00\n",
      "Step 1709/1767: training loss=0.00\n",
      "Step 1710/1767: training loss=0.00\n",
      "Step 1711/1767: training loss=0.00\n",
      "Step 1712/1767: training loss=1.58\n",
      "Step 1713/1767: training loss=0.03\n",
      "Step 1714/1767: training loss=0.25\n",
      "Step 1715/1767: training loss=1.87\n",
      "Step 1716/1767: training loss=0.94\n",
      "Step 1717/1767: training loss=0.00\n",
      "Step 1718/1767: training loss=1.11\n",
      "Step 1719/1767: training loss=1.55\n",
      "Step 1720/1767: training loss=1.12\n",
      "Step 1721/1767: training loss=0.00\n",
      "Step 1722/1767: training loss=0.00\n",
      "Step 1723/1767: training loss=1.64\n",
      "Step 1724/1767: training loss=1.61\n",
      "Step 1725/1767: training loss=0.78\n",
      "Step 1726/1767: training loss=0.00\n",
      "Step 1727/1767: training loss=0.00\n",
      "Step 1728/1767: training loss=0.93\n",
      "Step 1729/1767: training loss=0.00\n",
      "Step 1730/1767: training loss=0.00\n",
      "Step 1731/1767: training loss=1.36\n",
      "Step 1732/1767: training loss=0.00\n",
      "Step 1733/1767: training loss=0.75\n",
      "Step 1734/1767: training loss=0.00\n",
      "Step 1735/1767: training loss=0.00\n",
      "Step 1736/1767: training loss=0.00\n",
      "Step 1737/1767: training loss=0.00\n",
      "Step 1738/1767: training loss=1.57\n",
      "Step 1739/1767: training loss=0.00\n",
      "Step 1740/1767: training loss=0.00\n",
      "Step 1741/1767: training loss=0.00\n",
      "Step 1742/1767: training loss=0.00\n",
      "Step 1743/1767: training loss=0.00\n",
      "Step 1744/1767: training loss=0.00\n",
      "Step 1745/1767: training loss=0.00\n",
      "Step 1746/1767: training loss=0.01\n",
      "Step 1747/1767: training loss=1.56\n",
      "Step 1748/1767: training loss=0.35\n",
      "Step 1749/1767: training loss=0.00\n",
      "Step 1750/1767: training loss=0.00\n",
      "Step 1751/1767: training loss=0.00\n",
      "Step 1752/1767: training loss=0.65\n",
      "Step 1753/1767: training loss=0.00\n",
      "Step 1754/1767: training loss=0.00\n",
      "Step 1755/1767: training loss=0.00\n",
      "Step 1756/1767: training loss=0.00\n",
      "Step 1757/1767: training loss=1.07\n",
      "Step 1758/1767: training loss=0.00\n",
      "Step 1759/1767: training loss=0.00\n",
      "Step 1760/1767: training loss=0.00\n",
      "Step 1761/1767: training loss=0.00\n",
      "Step 1762/1767: training loss=0.00\n",
      "Step 1763/1767: training loss=1.44\n",
      "Step 1764/1767: training loss=0.00\n",
      "Step 1765/1767: training loss=0.25\n",
      "Step 1766/1767: training loss=0.00\n",
      "Step 1767/1767: training loss=0.00, full validation loss=0.53\n",
      "Checkpoint created at step 589 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDMu8:ckpt-step-589\n",
      "Checkpoint created at step 1178 with Snapshot ID: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDVWm:ckpt-step-1178\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.list_events(fine_tuning_job_id=job_id, limit=50)\n",
    "\n",
    "    \n",
    "events = [event for event in response]\n",
    "events.reverse()  \n",
    "\n",
    "for event in events:\n",
    "        \n",
    "    if hasattr(event, 'message'):\n",
    "        print(event.message)\n",
    "    else:\n",
    "        print(\"Event does not have a 'message' attribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model id: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW\n",
      "FineTuningJob(id='ftjob-Pyweyzaw7VycvlRk7qj5EjMW', created_at=1715703197, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW', finished_at=1715706486, hyperparameters=Hyperparameters(n_epochs=3, batch_size=4, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-Fi9mlsoWL88JdIl2wSCu95we', result_files=['file-dFIFbkZL9YsnDYRcl6eqJCue'], seed=1116149602, status='succeeded', trained_tokens=721605, training_file='file-a70lxStY2oLEIcM9NMlb8waJ', validation_file='file-V9YaNva6chzwov9jDnLg9nTV', estimated_finish=None, integrations=[], user_provided_suffix='samantha-test')\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "if hasattr(response, 'fine_tuned_model'):  \n",
    "        fine_tuned_model_id = response.fine_tuned_model\n",
    "        print(\"Fine-tuned model id:\", fine_tuned_model_id)\n",
    "        response = client.fine_tuning.jobs.retrieve(job_id)\n",
    "        print(response)\n",
    "        \n",
    "else:\n",
    "        print(\"The response does not contain a 'fine_tuned_model' attribute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"Fine-tuned-gpt3.5(training-validation).json\"\n",
    "\n",
    "with open(file_path, \"w\") as json_file:\n",
    "    json.dump(fine_tuned_model_id, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = pd.read_csv('/Users/Afnan/Desktop/FinalData/FinalTest.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      افضل الرياضة صحيا وفيه اجر هي رياضة المشي الى ...\n",
      "1      • زحمة ال Gym يوم السبت زي زحم ة الطريق يوم ال...\n",
      "2      الحمدلله اليوم جددت اشتراك سنة مع فتنس تايم مس...\n",
      "3      MENTION انا احب العبها ككارديو لو صامل الوعد ف...\n",
      "4                فطوري بعدين اقول ليه فيني سوء تغذية URL\n",
      "                             ...                        \n",
      "714    MENTION صادق انا اتمرن وبديت تضخيم من اول الشت...\n",
      "715    لازم الحق ابدا نظام غذائي الوضع قاعد يتفاقم يا...\n",
      "716    اختبار تحمل ركض روحه جيه ٤٨ مرة✅✅ بعتبره اقوى ...\n",
      "717    اوكي خلاص سويت ميزانية للراتب وطلع مافي فلوس ا...\n",
      "718    اخووك يمثلني اتذكر انا وصديقتي اول يوم دايت و ...\n",
      "Name: text, Length: 719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(testing['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      افضل الرياضة صحيا وفيه اجر هي رياضة المشي الى ...\n",
      "1      • زحمة ال Gym يوم السبت زي زحم ة الطريق يوم ال...\n",
      "2      الحمدلله اليوم جددت اشتراك سنة مع فتنس تايم مس...\n",
      "3      MENTION انا احب العبها ككارديو لو صامل الوعد ف...\n",
      "4                فطوري بعدين اقول ليه فيني سوء تغذية URL\n",
      "                             ...                        \n",
      "714    MENTION صادق انا اتمرن وبديت تضخيم من اول الشت...\n",
      "715    لازم الحق ابدا نظام غذائي الوضع قاعد يتفاقم يا...\n",
      "716    اختبار تحمل ركض روحه جيه ٤٨ مرة✅✅ بعتبره اقوى ...\n",
      "717    اوكي خلاص سويت ميزانية للراتب وطلع مافي فلوس ا...\n",
      "718    اخووك يمثلني اتذكر انا وصديقتي اول يوم دايت و ...\n",
      "Name: text, Length: 719, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tweet_data = testing['text']\n",
    "\n",
    "print(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'افضل الرياضة صحيا وفيه اجر هي رياضة المشي الى المسجد'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness'}, {'role': 'user', 'content': 'حتى لو مافيك تعمل تمارين امشي بس'}]\n"
     ]
    }
   ],
   "source": [
    "test_messages = []\n",
    "test_messages.append({\"role\": \"system\", \n",
    "\"content\": \n",
    "\"You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness\"})\n",
    "user_message = \"حتى لو مافيك تعمل تمارين امشي بس\"\n",
    "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "print(test_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model ID: ft:gpt-3.5-turbo-0125:personal:samantha-test:9OpvDoJW\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Using model ID:\", fine_tuned_model_id)\n",
    "if not fine_tuned_model_id:\n",
    "    print(\"Model ID is not set. Please check the model ID.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
    ")\n",
    "message_content = response.choices[0].message.content\n",
    "print(message_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred = []\n",
    "\n",
    "for i in range(len(tweet_data)):\n",
    "\n",
    "    test_messages = []\n",
    "    test_messages.append({\"role\": \"system\", \"content\": \"You are an assistant to classify the Arabic posts into positive, negative or neutral towards health awareness\"})\n",
    "    user_message = tweet_data[i]\n",
    "    test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=fine_tuned_model_id, messages=test_messages, temperature=0, max_tokens=500\n",
    "    )\n",
    "    message_content = response.choices[0].message.content\n",
    "    label_pred.append(message_content)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "print(label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Neutral', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Neutral', 'Positive', 'Neutral', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "label_true = testing['label']\n",
    "label_true = label_true.tolist()\n",
    "print(label_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pred_labelled = []\n",
    "\n",
    "label_labels = {\n",
    "    \"Positive\": [\"0\"],\n",
    "    \"Negative\": [\"2\"],\n",
    "    \"Neutral\": [\"1\"],\n",
    "}\n",
    "\n",
    "for predicted_label in label_pred:\n",
    "    labelled_label = None  \n",
    "    for label, country_label in label_labels.items():\n",
    "        if predicted_label in country_label:\n",
    "            labelled_label = label\n",
    "            break  \n",
    "\n",
    "    if labelled_label is not None:\n",
    "        label_pred_labelled.append(labelled_label)\n",
    "    else:\n",
    "        label_pred_labelled.append(predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Negative', 'Negative', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Neutral', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Negative', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Neutral', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Positive', 'Positive', 'Positive', 'Negative', 'Positive', 'Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "print(label_pred_labelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = {'Predicted label': label_pred_labelled, 'Actual label': label_true}\n",
    "\n",
    "df=pd.DataFrame(label_data)\n",
    "\n",
    "df.to_csv('label_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.885952712100139\n",
      "\n",
      "\n",
      "F1-Score - macro: 0.5538385100428895\n",
      "\n",
      "\n",
      "F1-Score - weighted: 0.8784302814941476\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAokAAAIhCAYAAAAimCCiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABV/klEQVR4nO3dd3RU1fr/8c+kF0ggARKCoRcpQaoQVGoAERV+Nkq8UiIqIBgB4SJXAZEEuEpXUEQSkXovRbBwQSlXBJQqVSyEJsmlBZAkpJ7fHyzm63ACJoFhJsz75Zq1Mvvs2eeZcVZ48ux99rEYhmEIAAAA+BM3RwcAAAAA50OSCAAAABOSRAAAAJiQJAIAAMCEJBEAAAAmJIkAAAAwIUkEAACACUkiAAAATEgSAQAAYEKSCBQDe/fuVZ8+fVSlShX5+PioRIkSatSokSZNmqTz58/b9dy7d+9Wq1atFBgYKIvFoqlTp972c1gsFo0ZM+a2j/tXEhISZLFYZLFYtHHjRtNxwzBUvXp1WSwWtW7dukjneP/995WQkFCo12zcuPGGMQHAneLh6AAA3NycOXM0YMAA1apVS6+99prq1Kmj7Oxs7dixQ7Nnz9bWrVu1YsUKu52/b9++SktL0+LFi1W6dGlVrlz5tp9j69atuueee277uAVVsmRJzZ0715QIbtq0Sb/99ptKlixZ5LHff/99lSlTRr179y7waxo1aqStW7eqTp06RT4vANwqkkTAiW3dulX9+/dX+/bttXLlSnl7e1uPtW/fXkOHDtWaNWvsGsP+/fvVr18/derUyW7naN68ud3GLohu3bppwYIFeu+99xQQEGBtnzt3riIjI3Xp0qU7Ekd2drYsFosCAgIc/pkAANPNgBOLi4uTxWLRhx9+aJMgXuPl5aXHH3/c+jwvL0+TJk3SvffeK29vb5UrV07PPfecTp48afO61q1bq169etq+fbseeugh+fn5qWrVqpowYYLy8vIk/d9UbE5OjmbNmmWdlpWkMWPGWH/+s2uvOXr0qLVt/fr1at26tYKDg+Xr66uKFSvqySefVHp6urVPftPN+/fvV5cuXVS6dGn5+PioQYMGSkxMtOlzbVp20aJFGjVqlMLCwhQQEKCoqCgdPny4YB+ypB49ekiSFi1aZG27ePGili1bpr59++b7mrFjx6pZs2YKCgpSQECAGjVqpLlz58owDGufypUr68CBA9q0aZP187tWib0W+/z58zV06FBVqFBB3t7e+vXXX03TzWfPnlV4eLhatGih7Oxs6/gHDx6Uv7+//va3vxX4vQJAQZEkAk4qNzdX69evV+PGjRUeHl6g1/Tv318jRoxQ+/bttWrVKo0bN05r1qxRixYtdPbsWZu+KSkpio6O1rPPPqtVq1apU6dOGjlypD799FNJUufOnbV161ZJ0lNPPaWtW7danxfU0aNH1blzZ3l5eenjjz/WmjVrNGHCBPn7+ysrK+uGrzt8+LBatGihAwcOaPr06Vq+fLnq1Kmj3r17a9KkSab+r7/+uo4dO6aPPvpIH374oX755Rc99thjys3NLVCcAQEBeuqpp/Txxx9b2xYtWiQ3Nzd169bthu/txRdf1NKlS7V8+XI98cQTGjRokMaNG2fts2LFClWtWlUNGza0fn7XLw0YOXKkjh8/rtmzZ2v16tUqV66c6VxlypTR4sWLtX37do0YMUKSlJ6erqeffloVK1bU7NmzC/Q+AaBQDABOKSUlxZBkdO/evUD9Dx06ZEgyBgwYYNP+/fffG5KM119/3drWqlUrQ5Lx/fff2/StU6eO0bFjR5s2ScbAgQNt2kaPHm3k9+tj3rx5hiQjKSnJMAzD+Pe//21IMvbs2XPT2CUZo0ePtj7v3r274e3tbRw/ftymX6dOnQw/Pz/jwoULhmEYxoYNGwxJxiOPPGLTb+nSpYYkY+vWrTc977V4t2/fbh1r//79hmEYRtOmTY3evXsbhmEYdevWNVq1anXDcXJzc43s7GzjrbfeMoKDg428vDzrsRu99tr5WrZsecNjGzZssGmfOHGiIclYsWKF0atXL8PX19fYu3fvTd8jABQVlUTgLrFhwwZJMl0gcf/996t27dr65ptvbNpDQ0N1//3327TVr19fx44du20xNWjQQF5eXnrhhReUmJioI0eOFOh169evV7t27UwV1N69eys9Pd1U0fzzlLt09X1IKtR7adWqlapVq6aPP/5Y+/bt0/bt22841XwtxqioKAUGBsrd3V2enp568803de7cOZ0+fbrA533yyScL3Pe1115T586d1aNHDyUmJmrGjBmKiIgo8OsBoDBIEgEnVaZMGfn5+SkpKalA/c+dOydJKl++vOlYWFiY9fg1wcHBpn7e3t7KyMgoQrT5q1atmr7++muVK1dOAwcOVLVq1VStWjVNmzbtpq87d+7cDd/HteN/dv17ubZ+szDvxWKxqE+fPvr00081e/Zs1axZUw899FC+fX/44Qd16NBB0tWrz7/77jtt375do0aNKvR583ufN4uxd+/eunLlikJDQ1mLCMCuSBIBJ+Xu7q527dpp586dpgtP8nMtUUpOTjYdO3XqlMqUKXPbYvPx8ZEkZWZm2rRfv+5Rkh566CGtXr1aFy9e1LZt2xQZGanY2FgtXrz4huMHBwff8H1Iuq3v5c969+6ts2fPavbs2erTp88N+y1evFienp76/PPP9cwzz6hFixZq0qRJkc6Z3wVAN5KcnKyBAweqQYMGOnfunIYNG1akcwJAQZAkAk5s5MiRMgxD/fr1y/dCj+zsbK1evVqS1LZtW0myXnhyzfbt23Xo0CG1a9futsV17QrdvXv32rRfiyU/7u7uatasmd577z1J0q5du27Yt127dlq/fr01Kbzmk08+kZ+fn922h6lQoYJee+01PfbYY+rVq9cN+1ksFnl4eMjd3d3alpGRofnz55v63q7qbG5urnr06CGLxaKvvvpK8fHxmjFjhpYvX37LYwNAftgnEXBikZGRmjVrlgYMGKDGjRurf//+qlu3rrKzs7V79259+OGHqlevnh577DHVqlVLL7zwgmbMmCE3Nzd16tRJR48e1RtvvKHw8HC9+uqrty2uRx55REFBQYqJidFbb70lDw8PJSQk6MSJEzb9Zs+erfXr16tz586qWLGirly5Yr2COCoq6objjx49Wp9//rnatGmjN998U0FBQVqwYIG++OILTZo0SYGBgbftvVxvwoQJf9mnc+fOmjx5snr27KkXXnhB586d0zvvvJPvNkURERFavHixlixZoqpVq8rHx6dI6whHjx6tb7/9VmvXrlVoaKiGDh2qTZs2KSYmRg0bNlSVKlUKPSYA3AxJIuDk+vXrp/vvv19TpkzRxIkTlZKSIk9PT9WsWVM9e/bUyy+/bO07a9YsVatWTXPnztV7772nwMBAPfzww4qPj893DWJRBQQEaM2aNYqNjdWzzz6rUqVK6fnnn1enTp30/PPPW/s1aNBAa9eu1ejRo5WSkqISJUqoXr16WrVqlXVNX35q1aqlLVu26PXXX9fAgQOVkZGh2rVra968eYW6c4m9tG3bVh9//LEmTpyoxx57TBUqVFC/fv1Urlw5xcTE2PQdO3askpOT1a9fP/3xxx+qVKmSzT6SBbFu3TrFx8frjTfesKkIJyQkqGHDhurWrZs2b94sLy+v2/H2AECSZDGMP+38CgAAAIg1iQAAAMgHSSIAAABMSBIBAABgQpIIAAAAE5JEAAAAmJAkAgAAwIQkEQAAACZ35Wbav18w374McLTgEmx0DOeSnZPn6BAAGyV9HFe78m348l93KqKM3TPtNrY9UUkEAACAyV1ZSQQAACgUC3Wz65EkAgAAWCyOjsDpkDYDAADAhEoiAAAA080mfCIAAAAwoZIIAADAmkQTKokAAAAwoZIIAADAmkQTPhEAAACYUEkEAABgTaIJSSIAAADTzSZ8IgAAADChkggAAMB0swmVRAAAAJhQSQQAAGBNogmfCAAAAEyoJAIAALAm0YRKIgAAAEyoJAIAALAm0YQkEQAAgOlmE9JmAAAAmFBJBAAAYLrZhE8EAAAAJlQSAQAAqCSa8IkAAADAhEoiAACAG1c3X49KIgAAAEyoJAIAALAm0YQkEQAAgM20TUibAQAAYEIlEQAAgOlmEz4RAAAAmFBJBAAAYE2iCZVEAAAAmFBJBAAAYE2iCZ8IAAAATKgkAgAAsCbRhCQRAACA6WYTPhEAAACYUEkEAABgutmESiIAAABMqCQCAACwJtGETwQAAAAmVBIBAABYk2hCJREAAAAmVBIBAABYk2hCkggAAECSaMInAgAAABMqiQAAAFy4YkIlEQAAACZUEgEAAFiTaMInAgAAABOnSRLnz5+vBx54QGFhYTp27JgkaerUqfrss88cHBkAALjrWSz2exRTTpEkzpo1S0OGDNEjjzyiCxcuKDc3V5JUqlQpTZ061bHBAQAAuCCnSBJnzJihOXPmaNSoUXJ3d7e2N2nSRPv27XNgZAAAwCVY3Oz3KKac4sKVpKQkNWzY0NTu7e2ttLQ0B0QEAABcSjGeFrYXp0hvq1Spoj179pjav/rqK9WpU+fOBwQAAODinKKS+Nprr2ngwIG6cuWKDMPQDz/8oEWLFik+Pl4fffSRo8MDAAB3OQuVRBOnqCT26dNHo0eP1vDhw5Wenq6ePXtq9uzZmjZtmrp37+7o8AAAAO6IMWPGyGKx2DxCQ0Otxw3D0JgxYxQWFiZfX1+1bt1aBw4csBkjMzNTgwYNUpkyZeTv76/HH39cJ0+eLHQsTpEkSlK/fv107NgxnT59WikpKTpx4oRiYmIcHRYAAHAB1ydmt/NRWHXr1lVycrL18eeLeCdNmqTJkydr5syZ2r59u0JDQ9W+fXv98ccf1j6xsbFasWKFFi9erM2bN+vy5ct69NFHrbvHFJRTJIljx47Vb7/9JkkqU6aMypUr5+CIAAAAbo/MzExdunTJ5pGZmXnD/h4eHgoNDbU+ypYtK+lqFXHq1KkaNWqUnnjiCdWrV0+JiYlKT0/XwoULJUkXL17U3Llz9e677yoqKkoNGzbUp59+qn379unrr78uVNxOkSQuW7ZMNWvWVPPmzTVz5kydOXPG0SEBAABXYrHfIz4+XoGBgTaP+Pj4G4byyy+/KCwsTFWqVFH37t115MgRSVd3g0lJSVGHDh2sfb29vdWqVStt2bJFkrRz505lZ2fb9AkLC1O9evWsfQrKKZLEvXv3au/evWrbtq0mT56sChUq6JFHHtHChQuVnp7u6PAAAACKbOTIkbp48aLNY+TIkfn2bdasmT755BP95z//0Zw5c5SSkqIWLVro3LlzSklJkSSFhITYvCYkJMR6LCUlRV5eXipduvQN+xSUUySJ0tX597i4OB05ckQbNmxQlSpVFBsba7NYEwAAwB7suSbR29tbAQEBNg9vb+984+jUqZOefPJJRUREKCoqSl988YUkKTEx0SbWPzMM4y/XPhakz/WcJkn8M39/f/n6+srLy0vZ2dmODgcAANzlnOnClT/z9/dXRESEfvnlF2vh7PqK4OnTp63VxdDQUGVlZSk1NfWGfQrKaZLEpKQkjR8/XnXq1FGTJk20a9cujRkzptClUQAAgLtFZmamDh06pPLly6tKlSoKDQ3VunXrrMezsrK0adMmtWjRQpLUuHFjeXp62vRJTk7W/v37rX0Kyik2046MjNQPP/ygiIgI9enTRz179lSFChUcHRYAAHARzrKZ9rBhw/TYY4+pYsWKOn36tN5++21dunRJvXr1ksViUWxsrOLi4lSjRg3VqFFDcXFx8vPzU8+ePSVJgYGBiomJ0dChQxUcHKygoCANGzbMOn1dGE6RJLZp00YfffSR6tat6+hQAAAAHObkyZPq0aOHzp49q7Jly6p58+batm2bKlWqJEkaPny4MjIyNGDAAKWmpqpZs2Zau3atSpYsaR1jypQp8vDw0DPPPKOMjAy1a9dOCQkJcnd3L1QsFsMwjNv67pzA7xeyHB0CYBJcwsvRIQA2snPyHB0CYKOkj+NWwQX2mG+3sS8u+pvdxrYnh1UShwwZonHjxsnf319Dhgy5ad/JkyffoahcT8Kc9/XJR7Ns2koHBWvZVxutz48lHdGH703R3l07lGfkqXKV6noz7h2FhJa/w9HC1S1ZtEAJ8+bq7Jkzqla9hob//XU1atzE0WHBBezauV3zEz7WoUMHdPbMGb0zZYZat/2/qbsm99XO93WDXx2m53pz9zAUTw5LEnfv3m29cnn37t2OCgOSKletrndmzrE+d3P7v7/kfj95Qq+88Jw6Pf6EevcbIP8SJXQ8KUleXlTFcGet+epLTZoQr1FvjFaDho3076WLNeDFflqx6guVDwtzdHi4y2VkZKhGrVp6rMv/0/Chr5iOr/nmvzbPt2z+VuPG/ENtozqY+sJJOceSRKfisCRxw4YN+f6MO8/d3V1BwWXyPfbxrOm6v8VDenHQ/1V7wyqE36nQAKv5ifP0/558Uk889bQkafjIUdqyZbOWLlmkV14d6uDocLd74MGWeuDBljc8XqZMWZvnmzauV5OmzXTPPfy+RPHlFFvg9O3b1+bG1NekpaWpb9++DojItfx+4rie7txWPbs+rHGjXtOp309IkvLy8rRty38VXrGShg9+UU883EoD+vbU5k3fODhiuJrsrCwdOnhAkS0etGmPbPGAftzDTAScy7lzZ7X5203q8v+edHQoKARn3SfRkZwiSUxMTFRGRoapPSMjQ5988okDInIdtetG6O+jx2vitNka+vponT9/VoOe/5suXrygC6nnlZGerkWffKymkQ9o0vQP9GCrtho94lX9uGu7o0OHC0m9kKrc3FwFBwfbtAcHl9HZs9zrHc7l81Ur5e/nrzbt2js6FOCWOHQLnEuXLskwDBmGoT/++EM+Pj7WY7m5ufryyy9Vrly5m46RmZmpzMzM69osN7zdDWw1a/GQzfM6Effp2Sce0dovPlOb9p0kSS1attbTPZ6TJFWvea8O7PtRq5b/S/c1anrH44VrK8qtqIA7bdXK5Xr4kUf5d6iY4XeJmUMriaVKlVJQUJAsFotq1qyp0qVLWx9lypRR3759NXDgwJuOER8fr8DAQJvHzCmT7tA7uPv4+vqpavUaOnniuAJLlZa7u4cqValm06dS5So6/b9kB0UIV1S6VGm5u7vr7NmzNu3nz59T8A3W0wKOsHvXDh07mqSuTzzl6FBQSEw3mzm0krhhwwYZhqG2bdtq2bJlCgoKsh7z8vJSpUqVFPYXVy2OHDnStIXO2Yzi+z/E0bKysnQs6Ygi7mskT09P1apTVyeOHbXpc+L4Mba/wR3l6eWl2nXqatuW79Qu6v+m8LZt2aLWbds5MDLA1mcrlql2nbqqWeteR4cC3DKHJomtWrWSdPW+zRUrVixStu3t7W0q6f+Rx2baBTVr2jtq8VArlQstrwvnz2v+vA+VnpamDp27SJK6PdtH40YNU/2GjdWw8f36Ydtmbd28SVPe/9jBkcPV/K1XH436+3DVqVdP993XUMv+tUTJycl6ult3R4cGF5CenqYTx49bn//++0kd/umQAgMDFVr+ajHj8uXL+nrtfxQ7dLijwsQtKM4VP3txWJK4d+9e1atXT25ubrp48aL27dt3w77169e/g5G5lrOn/6e33xihixdSFVg6SHXq1tfMuQusv/Qeat1Or454UwsTP9LMyRMUXrGyxsZPVkSDRg6OHK7m4U6P6OKFVH04632dOXNa1WvU1HuzP1RYGPd5h/0dPHBALz3fy/p8yjsTJUmPPt5VY8bFS5LWrvlShgw93KmzQ2IEbjeH3ZbPzc1NKSkpKleunNzc3GSxWJRfKBaLRbm5uYUam9vywRlxWz44G27LB2fjyNvyBfdaZLexzyX2sNvY9uSwSmJSUpLKli1r/RkAAADOw2FJYqVKlfL9GQAA4E5jTaKZ02ym/cUXX1ifDx8+XKVKlVKLFi107NgxB0YGAADgmpwiSYyLi5Ovr68kaevWrZo5c6YmTZqkMmXK6NVXX3VwdAAA4G7HPolmDt0C55oTJ06oevXqkqSVK1fqqaee0gsvvKAHHnhArVu3dmxwAADgrleckzl7cYpKYokSJXTu3DlJ0tq1axUVFSVJ8vHxyfeezgAAALAvp6gktm/fXs8//7waNmyon3/+WZ07X91j6sCBA6pcubJjgwMAAHc/CokmTlFJfO+99xQZGakzZ85o2bJlCg4OliTt3LlTPXoUz72FAAAAijOHbaZtT2ymDWfEZtpwNmymDWfjyM20Q57/l93G/t9HT9ttbHtyiulmSbpw4YLmzp2rQ4cOyWKxqHbt2oqJiVFgYKCjQwMAAHA5TjHdvGPHDlWrVk1TpkzR+fPndfbsWU2ZMkXVqlXTrl27HB0eAAC4y7EFjplTVBJfffVVPf7445ozZ448PK6GlJOTo+eff16xsbH673//6+AIAQAAXItTJIk7duywSRAlycPDQ8OHD1eTJk0cGBkAAHAFxbniZy9OMd0cEBCg48ePm9pPnDihkiVLOiAiAADgSphuNnOKJLFbt26KiYnRkiVLdOLECZ08eVKLFy/W888/zxY4AAAADuAU083vvPOO3Nzc9NxzzyknJ0eS5Onpqf79+2vChAkOjg4AANz1im/Bz24cmiSmp6frtdde08qVK5Wdna2uXbvq5ZdfVmBgoKpXry4/Pz9HhgcAAOCyHJokjh49WgkJCYqOjpavr68WLlyovLw8/etf9tvQEgAA4HrFee2gvTg0SVy+fLnmzp2r7t27S5Kio6P1wAMPKDc3V+7u7o4MDQAAwKU59MKVEydO6KGHHrI+v//+++Xh4aFTp045MCoAAOBquLrZzKFJYm5urry8bO9n6+HhYb14BQAAAI7h0OlmwzDUu3dveXt7W9uuXLmil156Sf7+/ta25cuXOyI8AADgIopzxc9eHJok9urVy9T27LPPOiASAADg0sgRTRyaJM6bN8+RpwcAAMANOMVm2gAAAI7EdLOZU9yWDwAAAM6FSiIAAHB5VBLNqCQCAADAhEoiAABweVQSzagkAgAAwIRKIgAAcHlUEs1IEgEAAMgRTZhuBgAAgAmVRAAA4PKYbjajkggAAAATKokAAMDlUUk0o5IIAAAAEyqJAADA5VFINKOSCAAAABMqiQAAwOWxJtGMJBEAALg8ckQzppsBAABgQiURAAC4PKabzagkAgAAwIRKIgAAcHkUEs2oJAIAAMCESiIAAHB5bm6UEq9HJREAAAAmVBIBAIDLY02iGUkiAABweWyBY8Z0MwAAAEyoJAIAAJdHIdGMSiIAAABMqCQCAACXx5pEMyqJAAAAMKGSCAAAXB6VRDMqiQAAADAhSQQAAC7PYrHf41bEx8fLYrEoNjbW2mYYhsaMGaOwsDD5+vqqdevWOnDggM3rMjMzNWjQIJUpU0b+/v56/PHHdfLkyUKdmyQRAAC4PIvFYrdHUW3fvl0ffvih6tevb9M+adIkTZ48WTNnztT27dsVGhqq9u3b648//rD2iY2N1YoVK7R48WJt3rxZly9f1qOPPqrc3NwCn58kEQAAwMlcvnxZ0dHRmjNnjkqXLm1tNwxDU6dO1ahRo/TEE0+oXr16SkxMVHp6uhYuXChJunjxoubOnat3331XUVFRatiwoT799FPt27dPX3/9dYFjIEkEAAAuz57TzZmZmbp06ZLNIzMz86bxDBw4UJ07d1ZUVJRNe1JSklJSUtShQwdrm7e3t1q1aqUtW7ZIknbu3Kns7GybPmFhYapXr561T0GQJAIAANhRfHy8AgMDbR7x8fE37L948WLt2rUr3z4pKSmSpJCQEJv2kJAQ67GUlBR5eXnZVCCv71MQbIEDAABcnj23wBk5cqSGDBli0+bt7Z1v3xMnTuiVV17R2rVr5ePjc8Mxr4/XMIy/fA8F6fNnVBIBAADsyNvbWwEBATaPGyWJO3fu1OnTp9W4cWN5eHjIw8NDmzZt0vTp0+Xh4WGtIF5fETx9+rT1WGhoqLKyspSamnrDPgVBkggAAFyes2yB065dO+3bt0979uyxPpo0aaLo6Gjt2bNHVatWVWhoqNatW2d9TVZWljZt2qQWLVpIkho3bixPT0+bPsnJydq/f7+1T0Ew3QwAAOAkSpYsqXr16tm0+fv7Kzg42NoeGxuruLg41ahRQzVq1FBcXJz8/PzUs2dPSVJgYKBiYmI0dOhQBQcHKygoSMOGDVNERITpQpibIUkEAAAurzjdlm/48OHKyMjQgAEDlJqaqmbNmmnt2rUqWbKktc+UKVPk4eGhZ555RhkZGWrXrp0SEhLk7u5e4PNYDMMw7PEGHOn3C1mODgEwCS7h5egQABvZOXmODgGwUdLHcavgmo7faLext49qbbex7YlKIgAAcHnFqJB4x5AkAgAAl1ecppvvFK5uBgAAgAmVRAAA4PIoJJrdlUliaT9PR4cAAABQrN2VSSIAAEBhsCbRjDWJAAAAMKGSCAAAXB6FRDMqiQAAADChkggAAFweaxLNSBIBAIDLI0c0Y7oZAAAAJlQSAQCAy2O62YxKIgAAAEyoJAIAAJdHJdGMSiIAAABMqCQCAACXRyHRjEoiAAAATKgkAgAAl8eaRDOSRAAA4PLIEc2YbgYAAIAJlUQAAODymG42o5IIAAAAEyqJAADA5VFINKOSCAAAABMqiQAAwOW5UUo0oZIIAAAAEyqJAADA5VFINCNJBAAALo8tcMyYbgYAAIAJlUQAAODy3CgkmlBJBAAAgAmVRAAA4PJYk2hGJREAAAAmVBIBAIDLo5BoRiURAAAAJlQSAQCAy7OIUuL1SBIBAIDLYwscM6abAQAAYEIlEQAAuDy2wDGjkggAAAATKokAAMDlUUg0o5IIAAAAEyqJAADA5blRSjShkggAAAATKokAAMDlUUg0I0kEAAAujy1wzJhuBgAAgAmVRAAA4PIoJJoVKEmcPn16gQccPHhwkYMBAACAcyhQkjhlypQCDWaxWEgSAQBAscMWOGYFShKTkpLsHQcAAACcSJEvXMnKytLhw4eVk5NzO+MBAAC44yx2fBRXhU4S09PTFRMTIz8/P9WtW1fHjx+XdHUt4oQJE257gAAAALjzCp0kjhw5Uj/++KM2btwoHx8fa3tUVJSWLFlS5EC+/fZbPfvss4qMjNTvv/8uSZo/f742b95c5DEBAAAKwmKx2O1RXBU6SVy5cqVmzpypBx980OaN16lTR7/99luRgli2bJk6duwoX19f7d69W5mZmZKkP/74Q3FxcUUaEwAAoKDcLPZ7FFeFThLPnDmjcuXKmdrT0tKKnC2//fbbmj17tubMmSNPT09re4sWLbRr164ijQkAAICiK3SS2LRpU33xxRfW59cSwzlz5igyMrJIQRw+fFgtW7Y0tQcEBOjChQtFGhMAAKCgmG42K/QdV+Lj4/Xwww/r4MGDysnJ0bRp03TgwAFt3bpVmzZtKlIQ5cuX16+//qrKlSvbtG/evFlVq1Yt0pgAAAAoukJXElu0aKHvvvtO6enpqlatmtauXauQkBBt3bpVjRs3LlIQL774ol555RV9//33slgsOnXqlBYsWKBhw4ZpwIABRRoTAACgoCwW+z2KqyLduzkiIkKJiYm3LYjhw4fr4sWLatOmja5cuaKWLVvK29tbw4YN08svv3zbzgMAAICCsRiGYRT2Rbm5uVqxYoUOHToki8Wi2rVrq0uXLvLwKFLOaZWenq6DBw8qLy9PderUUYkSJYo2Tlah3xJgd27F+RI33JWyc/IcHQJgo6RPke/xccueW7jXbmN/0rO+3ca2p0Jndfv371eXLl2UkpKiWrVqSZJ+/vlnlS1bVqtWrVJEREShg0hMTNRTTz0lf39/NWnSpNCvBwAAwO1V6JT9+eefV926dXXy5Ent2rVLu3bt0okTJ1S/fn298MILRQpi2LBhKleunLp3767PP/+cW/0BAIA7in0SzQqdJP7444+Kj49X6dKlrW2lS5fW+PHjtWfPniIFkZycrCVLlsjd3V3du3dX+fLlNWDAAG3ZsqVI4wEAABQGW+CYFTpJrFWrlv73v/+Z2k+fPq3q1asXKQgPDw89+uijWrBggU6fPq2pU6fq2LFjatOmjapVq1akMQEAAFB0BVqTeOnSJevPcXFxGjx4sMaMGaPmzZtLkrZt26a33npLEydOvOWA/Pz81LFjR6WmpurYsWM6dOjQLY8JAABwM8W33mc/BUoSS5UqZVMuNQxDzzzzjLXt2gXSjz32mHJzc4sUSHp6ulasWKEFCxbo66+/Vnh4uHr06KF//etfRRoPAAAARVegJHHDhg12DaJHjx5avXq1/Pz89PTTT2vjxo1q0aKFXc8JAABwjVsxXjtoLwVKElu1amXXICwWi5YsWaKOHTve8l6LAAAAxdWsWbM0a9YsHT16VJJUt25dvfnmm+rUqZOkq7O3Y8eO1YcffqjU1FQ1a9ZM7733nurWrWsdIzMzU8OGDdOiRYuUkZGhdu3a6f3339c999xTqFiKtJm2dHV6+Pjx48rKyrJpr1/f8RtGspk2nBGbacPZsJk2nI0jN9Put3S/3cae80y9AvddvXq13N3drRcDJyYm6p///Kd2796tunXrauLEiRo/frwSEhJUs2ZNvf322/rvf/+rw4cPq2TJkpKk/v37a/Xq1UpISFBwcLCGDh2q8+fPa+fOnXJ3dy9wLIVOEs+cOaM+ffroq6++yvd4QdckTp8+XS+88IJ8fHw0ffr0m/YdPHhwYUIkSYRTIkmEsyFJhLMhScxfUFCQ/vnPf6pv374KCwtTbGysRowYIelq1TAkJEQTJ07Uiy++qIsXL6ps2bKaP3++unXrJkk6deqUwsPD9eWXX6pjx44FPm+h53ZjY2OVmpqqbdu2qU2bNlqxYoX+97//6e2339a7775b4HGmTJmi6Oho+fj4aMqUKTfsZ7FYCp0kAgAAFIY99zPMzMxUZmamTZu3t7e8vb1v+rrc3Fz961//UlpamiIjI5WUlKSUlBR16NDBZpxWrVppy5YtevHFF7Vz505lZ2fb9AkLC1O9evW0ZcsW+yaJ69ev12effaamTZvKzc1NlSpVUvv27RUQEKD4+Hh17ty5QOMkJSXl+zMAAMDdJD4+XmPHjrVpGz16tMaMGZNv/3379ikyMlJXrlxRiRIltGLFCtWpU8d6k5GQkBCb/iEhITp27JgkKSUlRV5eXjY3PbnWJyUlpVBxF7qum5aWpnLlykm6Wv48c+aMJCkiIkK7du0q7HCSpLfeekvp6emm9oyMDL311ltFGhMAAKCgLBb7PUaOHKmLFy/aPEaOHHnDWGrVqqU9e/Zo27Zt6t+/v3r16qWDBw/+KVbbqqdhGH9ZCS1In+sV6Y4rhw8fliQ1aNBAH3zwgX7//XfNnj1b5cuXL+xwkqSxY8fq8uXLpvb09HRT5g0AAHC7uVksdnt4e3srICDA5nGzqWYvLy9Vr15dTZo0UXx8vO677z5NmzZNoaGhkmSqCJ4+fdpaXQwNDVVWVpZSU1Nv2KfAn0mheuvqmsTk5GRJV0ula9asUcWKFTV9+nTFxcUVdjhJN85uf/zxRwUFBRVpTAAAgLuBYRjKzMxUlSpVFBoaqnXr1lmPZWVladOmTdb9pRs3bixPT0+bPsnJydq/f3+h96Au9JrE6Oho688NGzbU0aNH9dNPP6lixYoqU6ZMocYqXbq09ebXNWvWtEkUc3NzdfnyZb300kuFDREAAKBQnGUv7ddff12dOnVSeHi4/vjjDy1evFgbN27UmjVrZLFYFBsbq7i4ONWoUUM1atRQXFyc/Pz81LNnT0lSYGCgYmJiNHToUAUHBysoKEjDhg1TRESEoqKiChXLLe9c7efnp0aNGhXptVOnTpVhGOrbt6/Gjh2rwMBA6zEvLy9VrlxZkZGRtxoiAABAsfC///1Pf/vb35ScnKzAwEDVr19fa9asUfv27SVJw4cPV0ZGhgYMGGDdTHvt2rXWPRKlqzvIeHh46JlnnrFupp2QkFCoPRKlAu6TOGTIkAIPOHny5EIFIMlaJvX09Cz0a/PDPolwRuyTCGfDPolwNo7cJ3HgikN2G/u9/1fbbmPbU4Eqibt37y7QYEXdY+jPt/3LyMhQdna2zfGAgIAbvja/vYdyLV5/ufcQAAAAbqxASeKGDRvsGkR6erqGDx+upUuX6ty5c6bjN7uLS357D73+jzc16o0xtztMAABwl3JcDdN5OcVn8tprr2n9+vV6//335e3trY8++khjx45VWFiYPvnkk5u+Nr+9h4YNv/HeQwAAAPhrt3zhyu2wevVqffLJJ2rdurX69u2rhx56SNWrV1elSpW0YMECmyuqr5ffbW1YkwgAAArDnrflK66copJ4/vx5ValSRdLV9Yfnz5+XJD344IP673//68jQAACAC3Cz2O9RXDlFkli1alUdPXpUklSnTh0tXbpU0tUKY6lSpRwXGAAAgItyiiSxT58++vHHHyVdXWN4bW3iq6++qtdee83B0QEAgLsdlUSzAu2TeL358+dr9uzZSkpK0tatW1WpUiVNnTpVVapUUZcuXW45qOPHj2vHjh2qVq2a7rvvvkK/njWJcEbskwhnwz6JcDaO3CdxyKqf7Db25MfvtdvY9lTo/xuzZs3SkCFD9Mgjj+jChQvW7WlKlSqlqVOn3pagKlasqCeeeKJICSIAAEBhXbtNsD0exVWhr26eMWOG5syZo65du2rChAnW9iZNmmjYsGFFCmL69On5tlssFvn4+Kh69epq2bJloW8nAwAAgKIpdJKYlJSkhg0bmtq9vb2VlpZWpCCmTJmiM2fOKD09XaVLl5ZhGLpw4YL8/PxUokQJnT59WlWrVtWGDRsUHh5epHMAAADcCCuCzAo93VylShXt2bPH1P7VV1+pTp06RQoiLi5OTZs21S+//KJz587p/Pnz+vnnn9WsWTNNmzZNx48fV2hoqF599dUijQ8AAIDCKXQl8bXXXtPAgQN15coVGYahH374QYsWLVJ8fLw++uijIgXxj3/8Q8uWLVO1atWsbdWrV9c777yjJ598UkeOHNGkSZP05JNPFml8AACAmynGSwftptBJYp8+fZSTk6Phw4crPT1dPXv2VIUKFTRt2jR17969SEEkJycrJyfH1J6Tk6OUlBRJUlhYmP74448ijQ8AAHAzbmSJJkW61rxfv346duyYTp8+rZSUFJ04cUIxMTFFDqJNmzZ68cUXtXv3bmvb7t271b9/f7Vt21aStG/fPutdWQAAAGBft7QhUZkyZVSuXLlbDmLu3LkKCgpS48aNrfdibtKkiYKCgjR37lxJUokSJfTuu+/e8rkAAACu52bHR3FV6OnmKlWq3HTPnyNHjhQ6iNDQUK1bt04//fSTfv75ZxmGoXvvvVe1atWy9mnTpk2hxwUAAEDRFDpJjI2NtXmenZ2t3bt3a82aNbd8C72qVavKYrGoWrVq8vAodGgAAABFwpJEs0JnYq+88kq+7e+995527NhRpCDS09M1aNAgJSYmSpJ+/vlnVa1aVYMHD1ZYWJj+/ve/F2lcAAAAFM1tmyrv1KmTli1bVqTXjhw5Uj/++KM2btwoHx8fa3tUVJSWLFlyu0IEAADIl5vFYrdHcXXb5nT//e9/KygoqEivXblypZYsWaLmzZvbrHesU6eOfvvtt9sVIgAAAAqo0Eliw4YNbRI5wzCUkpKiM2fO6P333y9SEGfOnMn3Kum0tLRifWNsAABQPJBumBU6SezatavNczc3N5UtW1atW7fWvffeW6QgmjZtqi+++EKDBg2SJGtiOGfOHEVGRhZpTAAAgILi3s1mhUoSc3JyVLlyZXXs2FGhoaG3LYj4+Hg9/PDDOnjwoHJycjRt2jQdOHBAW7du1aZNm27beQAAAFAwhbpwxcPDQ/3791dmZuZtDaJFixb67rvvlJ6ermrVqmnt2rUKCQnR1q1b1bhx49t6LgAAgOtx4YpZoaebmzVrpt27d6tSpUq3NZCIiAjrFjgAAABwrEIniQMGDNDQoUN18uRJNW7cWP7+/jbH69evX+Cx3Nzc/vLCFIvFopycnMKGCQAAUGDFuOBnNxbDMIyCdOzbt6+mTp2qUqVKmQexWGQYhiwWi3Jzcwt88s8+++yGx7Zs2aIZM2bIMAxlZGQUeExJSs8q0FsC7ig3VkXDyWTn5Dk6BMBGSR/H3el43Ne/2m3sN6Kq221seypwkuju7q7k5OS/TNhudRr6p59+0siRI7V69WpFR0dr3LhxqlixYqHGIEmEMyJJhLMhSYSzcWSSOP4b+yWJo9oVzySxwNPN13LJ270W8ZpTp05p9OjRSkxMVMeOHbVnzx7Vq1fPLucCAADAzRUqZbfHxtYXL17UiBEjVL16dR04cEDffPONVq9eTYIIAADuGIsd/yuuCnXhSs2aNf8yUTx//nyBx5s0aZImTpyo0NBQLVq0SF26dClMOAAAALcFK4LMCrwm0c3NTVOnTlVgYOBN+/Xq1avAJ3dzc5Ovr6+ioqLk7u5+w37Lly8v8JgSaxLhnFiTCGfDmkQ4G0euSZyw/je7jf33ttXsNrY9FaqS2L1793zvsVxUzz33HPdmBgAADsff8WYFThLtkcwlJCTc9jEBAABw6wp9dTMAAMDdhplNswIniXl5rF0BAABwFYW+LR8AAMDdhjWJZo67jAgAAABOi0oiAABweSxJNCNJBAAALs+NLNGE6WYAAACYUEkEAAAujwtXzKgkAgAAwIRKIgAAcHksSTSjkggAAAATKokAAMDluYlS4vWoJAIAAMCESiIAAHB5rEk0I0kEAAAujy1wzJhuBgAAgAmVRAAA4PK4LZ8ZlUQAAACYUEkEAAAuj0KiGZVEAAAAmFBJBAAALo81iWZUEgEAAGBCJREAALg8ColmJIkAAMDlMbVqxmcCAAAAEyqJAADA5VmYbzahkggAAAATKokAAMDlUUc0o5IIAAAAEyqJAADA5bGZthmVRAAAAJhQSQQAAC6POqIZSSIAAHB5zDabMd0MAAAAE5JEAADg8iwWi90ehREfH6+mTZuqZMmSKleunLp27arDhw/b9DEMQ2PGjFFYWJh8fX3VunVrHThwwKZPZmamBg0apDJlysjf31+PP/64Tp48WahYSBIBAACcxKZNmzRw4EBt27ZN69atU05Ojjp06KC0tDRrn0mTJmny5MmaOXOmtm/frtDQULVv315//PGHtU9sbKxWrFihxYsXa/Pmzbp8+bIeffRR5ebmFjgWi2EYxm19d04gPeuue0u4C7i5seAFziU7J8/RIQA2Svo4rna1ZPfvdhu7a50yyszMtGnz9vaWt7f3X772zJkzKleunDZt2qSWLVvKMAyFhYUpNjZWI0aMkHS1ahgSEqKJEyfqxRdf1MWLF1W2bFnNnz9f3bp1kySdOnVK4eHh+vLLL9WxY8cCxU0lEQAAwI7i4+MVGBho84iPjy/Qay9evChJCgoKkiQlJSUpJSVFHTp0sPbx9vZWq1attGXLFknSzp07lZ2dbdMnLCxM9erVs/YpCK5uBgAALq+wawcLY+TIkRoyZIhNW0GqiIZhaMiQIXrwwQdVr149SVJKSookKSQkxKZvSEiIjh07Zu3j5eWl0qVLm/pce31BkCQCAADYUUGnlq/38ssva+/evdq8ebPp2PVJrWEYf5noFqTPnzHdDAAAXJ7Fjo+iGDRokFatWqUNGzbonnvusbaHhoZKkqkiePr0aWt1MTQ0VFlZWUpNTb1hn4IgSQQAAHAShmHo5Zdf1vLly7V+/XpVqVLF5niVKlUUGhqqdevWWduysrK0adMmtWjRQpLUuHFjeXp62vRJTk7W/v37rX0KgulmAADg8uy5JrEwBg4cqIULF+qzzz5TyZIlrRXDwMBA+fr6ymKxKDY2VnFxcapRo4Zq1KihuLg4+fn5qWfPnta+MTExGjp0qIKDgxUUFKRhw4YpIiJCUVFRBY7lrkwSr2SzrQOcj48nhXs4l3KRgx0dAmAjY/dMh53bWX5Dz5o1S5LUunVrm/Z58+apd+/ekqThw4crIyNDAwYMUGpqqpo1a6a1a9eqZMmS1v5TpkyRh4eHnnnmGWVkZKhdu3ZKSEiQu7t7gWO5K/dJPJ9W8I0igTuFJBHOJrjZIEeHANhwZJK4/Mdku439xH3l7Ta2Pd2VlUQAAIDCcJbpZmdCaQMAAAAmVBIBAIDLo45oRiURAAAAJlQSAQCAy2NJohmVRAAAAJhQSQQAAC7PjVWJJiSJAADA5THdbMZ0MwAAAEyoJAIAAJdnYbrZhEoiAAAATKgkAgAAl8eaRDMqiQAAADChkggAAFweW+CYUUkEAACACZVEAADg8liTaEaSCAAAXB5JohnTzQAAADChkggAAFwem2mbUUkEAACACZVEAADg8twoJJpQSQQAAIAJlUQAAODyWJNoRiURAAAAJlQSAQCAy2OfRDOSRAAA4PKYbjZjuhkAAAAmVBIBAIDLYwscMyqJAAAAMKGSCAAAXB5rEs2oJAIAAMCESiIAAHB5bIFjRiURAAAAJlQSAQCAy6OQaEaSCAAAXJ4b880mTDcDAADAhEoiAABwedQRzagkAgAAwIRKIgAAAKVEEyqJAAAAMKGSCAAAXB635TOjkggAAAATKokAAMDlsU2iGUkiAABweeSIZkw3AwAAwIRKIgAAAKVEEyqJAAAAMKGSCAAAXB5b4JhRSQQAAIAJlUQAAODy2ALHjEoiAAAATKgkAgAAl0ch0YwkEQAAgCzRxGFJ4qVLlwrcNyAgwI6RAAAA4HoOSxJLlSoly1+sEjUMQxaLRbm5uXcoKgAA4IrYAsfMYUnihg0bHHVqAAAA/AWHJYmtWrVy1KkBAABssAWOmVNduJKenq7jx48rKyvLpr1+/foOiggAAMA1OUWSeObMGfXp00dfffVVvsdZkwgAAOyJQqKZU2ymHRsbq9TUVG3btk2+vr5as2aNEhMTVaNGDa1atcrR4QEAALgcp6gkrl+/Xp999pmaNm0qNzc3VapUSe3bt1dAQIDi4+PVuXNnR4cIAADuZpQSTZyikpiWlqZy5cpJkoKCgnTmzBlJUkREhHbt2uXI0AAAgAuw2PG/4sopksRatWrp8OHDkqQGDRrogw8+0O+//67Zs2erfPnyDo4OAADA9TjFdHNsbKySk5MlSaNHj1bHjh21YMECeXl5KSEhwbHBAQCAux5b4Jg5RZIYHR1t/blhw4Y6evSofvrpJ1WsWFFlypRxYGQAAACuyeHTzdnZ2apataoOHjxobfPz81OjRo1IEAEAwB1hseOjuHJ4kujp6anMzMy/vI8zAAAA7hyHJ4mSNGjQIE2cOFE5OTmODgUAALgiSokmTpEkfv/991q+fLkqVqyojh076oknnrB5AAAAuIr//ve/euyxxxQWFiaLxaKVK1faHDcMQ2PGjFFYWJh8fX3VunVrHThwwKZPZmamBg0apDJlysjf31+PP/64Tp48Wag4nCJJLFWqlJ588kl17NhRYWFhCgwMtHkAAADYkzPtk5iWlqb77rtPM2fOzPf4pEmTNHnyZM2cOVPbt29XaGio2rdvrz/++MPaJzY2VitWrNDixYu1efNmXb58WY8++mihbnVsMQzDKHT0Tu58Gvd6hvPx8XSKv8kAq+BmgxwdAmAjY3f+SdGdcOD3NLuNXbeCf5Ffa7FYtGLFCnXt2lXS1SpiWFiYYmNjNWLECElXq4YhISGaOHGiXnzxRV28eFFly5bV/Pnz1a1bN0nSqVOnFB4eri+//FIdO3Ys0Lmd4l+ttm3b6sKFC6b2S5cuqW3btnc+IAAA4FIsFvs9MjMzdenSJZtHZmZmkeJMSkpSSkqKOnToYG3z9vZWq1attGXLFknSzp07lZ2dbdMnLCxM9erVs/YpCKdIEjdu3KisrCxT+5UrV/Ttt986ICIAAOBK7HndSnx8vGkpXXx8fJHiTElJkSSFhITYtIeEhFiPpaSkyMvLS6VLl75hn4Jw6Gbae/futf588OBBm8Bzc3O1Zs0aVahQwRGhAQAA3BYjR47UkCFDbNq8vb1vaczrtw40DOMvtxMsSJ8/c2iS2KBBA1ksFlkslnynlX19fTVjxoybjpGZmWkq2WbmeNzyhw8AAFyIHbeq8fb2vm15SWhoqKSr1cLy5ctb20+fPm2tLoaGhiorK0upqak21cTTp0+rRYsWBT6XQ6ebk5KS9Ntvv8kwDP3www9KSkqyPn7//XddunRJffv2vekY+ZVwp74z4Q69AwAAgDunSpUqCg0N1bp166xtWVlZ2rRpkzUBbNy4sTw9PW36JCcna//+/YVKEh1aSaxUqZIkKS8vr8hj5FfCTctxiltSAwCAYqIoW9XYy+XLl/Xrr79anyclJWnPnj0KCgpSxYoVFRsbq7i4ONWoUUM1atRQXFyc/Pz81LNnT0lSYGCgYmJiNHToUAUHBysoKEjDhg1TRESEoqKiChyHU2RTn3zyyU2PP/fcczc8ll8JN4ctcAAAQDG1Y8cOtWnTxvr8WjGsV69eSkhI0PDhw5WRkaEBAwYoNTVVzZo109q1a1WyZEnra6ZMmSIPDw8988wzysjIULt27ZSQkCB3d/cCx+EU+yRef/VNdna20tPT5eXlJT8/P50/f75Q47FPIpwR+yTC2bBPIpyNI/dJPJySbrexa4X62W1se3KKf7VSU1NtHpcvX9bhw4f14IMPatGiRY4ODwAAwOU4RZKYnxo1amjChAl65ZVXHB0KAAC4y9lzn8TiyinWJN6Iu7u7Tp065egwAADA3a44Z3N24hRJ4qpVq2yeG4ah5ORkzZw5Uw888ICDogIAAHBdTpEkXrtp9TUWi0Vly5ZV27Zt9e677zomKAAA4DKcaQscZ+EUSeKt7JMIAACA28+pLlzJysrS4cOHlZOT4+hQAACAC7FY7PcorpwiSUxPT1ffvn3l5+enunXr6vjx45KkwYMHa8IEbrEHAABwpzlFkjhy5Ejt3btXGzdulI+Pj7U9KipKS5YscWBkAADAFbAFjplTrElcuXKllixZoubNm8vyp7psnTp19NtvvzkwMgAAANfkFEnimTNnVK5cOVN7WlqaTdIIAABgF6QbJk4x3dy0aVN98cUX1ufXEsM5c+YoMjLSUWEBAAAXYbHjf8WVU1QS4+Pj9fDDD+vgwYPKycnRtGnTdODAAW3dulWbNm1ydHgAAAAuxykqiS1atNB3332n9PR0VatWTWvXrlVISIi2bt2qxo0bOzo8AABwl2MLHDOnqCRKUkREhBITEx0dBgAAAOTgJNHNze0vL0yxWCxsrg0AAOyqGBf87MahSeKKFStueGzLli2aMWOGDMO4gxEBAABAcnCS2KVLF1PbTz/9pJEjR2r16tWKjo7WuHHjHBAZAABwKZQSTZziwhVJOnXqlPr166f69esrJydHe/bsUWJioipWrOjo0AAAAFyOw5PEixcvasSIEapevboOHDigb775RqtXr1a9evUcHRoAAHAR7JNo5tDp5kmTJmnixIkKDQ3VokWL8p1+BgAAsLfivFWNvVgMB14Z4ubmJl9fX0VFRcnd3f2G/ZYvX16occ+n5d5qaMBt5+Pp8MI9YCO42SBHhwDYyNg902HnPn4+025jVwzyttvY9uTQSuJzzz3HvZkBAIDDkY2YOTRJTEhIcOTpAQAAcANOc8cVAAAAR2Fi04xFUgAAADChkggAAMCqRBMqiQAAADChkggAAFweaxLNSBIBAIDLI0c0Y7oZAAAAJlQSAQCAy2O62YxKIgAAAEyoJAIAAJdnYVWiCZVEAAAAmFBJBAAAoJBoQiURAAAAJlQSAQCAy6OQaEaSCAAAXB5b4Jgx3QwAAAATKokAAMDlsQWOGZVEAAAAmFBJBAAAoJBoQiURAAAAJlQSAQCAy6OQaEYlEQAAACZUEgEAgMtjn0QzkkQAAODy2ALHjOlmAAAAmFBJBAAALo/pZjMqiQAAADAhSQQAAIAJSSIAAABMWJMIAABcHmsSzagkAgAAwIRKIgAAcHnsk2hGkggAAFwe081mTDcDAADAhEoiAABweRQSzagkAgAAwIRKIgAAAKVEEyqJAAAAMKGSCAAAXB5b4JhRSQQAAIAJlUQAAODy2CfRjEoiAAAATKgkAgAAl0ch0YwkEQAAgCzRhOlmAAAAmJAkAgAAl2ex439F8f7776tKlSry8fFR48aN9e23397md/zXSBIBAACcyJIlSxQbG6tRo0Zp9+7deuihh9SpUycdP378jsZhMQzDuKNnvAPOp+U6OgTAxMeTv8ngXIKbDXJ0CICNjN0zHXbuKzn2G9unkFeANGvWTI0aNdKsWbOsbbVr11bXrl0VHx9/m6O7Mf7VAgAAsKPMzExdunTJ5pGZmZlv36ysLO3cuVMdOnSwae/QoYO2bNlyJ8K1uiuvbg7yd3d0CHeFzMxMxcfHa+TIkfL29nZ0OADfydvMkVWbuwnfy7tDYat9hTHm7XiNHTvWpm306NEaM2aMqe/Zs2eVm5urkJAQm/aQkBClpKTYL8h83JXTzbg9Ll26pMDAQF28eFEBAQGODgfgOwmnxPcSfyUzM9NUOfT29s73j4pTp06pQoUK2rJliyIjI63t48eP1/z58/XTTz/ZPd5r7spKIgAAgLO4UUKYnzJlysjd3d1UNTx9+rSpumhvrEkEAABwEl5eXmrcuLHWrVtn075u3Tq1aNHijsZCJREAAMCJDBkyRH/729/UpEkTRUZG6sMPP9Tx48f10ksv3dE4SBJxQ97e3ho9ejQLseE0+E7CGfG9xO3WrVs3nTt3Tm+99ZaSk5NVr149ffnll6pUqdIdjYMLVwAAAGDCmkQAAACYkCQCAADAhCQRAAAAJiSJMDl69KgsFov27Nlz036tW7dWbGzsHYkJKIrKlStr6tSpjg4DKJKNGzfKYrHowoULjg4FLooksRjr3bu3LBaLLBaLPD09VbVqVQ0bNkxpaWm3NG54eLj1airpxr+oli9frnHjxt3SuVB8Xfv+TZgwwaZ95cqVslgsdzSWhIQElSpVytS+fft2vfDCC3c0FjifO/VdLegf2EBxQZJYzD388MNKTk7WkSNH9Pbbb+v999/XsGHDbmlMd3d3hYaGysPj5jskBQUFqWTJkrd0LhRvPj4+mjhxolJTUx0dSr7Kli0rPz8/R4cBJ+BM39WsrCxHhwAUCEliMeft7a3Q0FCFh4erZ8+eio6O1sqVK5WZmanBgwerXLly8vHx0YMPPqjt27dbX5eamqro6GiVLVtWvr6+qlGjhubNmyfJ9q/ho0ePqk2bNpKk0qVLy2KxqHfv3pJsp5tHjhyp5s2bm+KrX7++Ro8ebX0+b9481a5dWz4+Prr33nv1/vvv2+mTwZ0QFRWl0NBQxcfH37DPli1b1LJlS/n6+io8PFyDBw+2qXYnJyerc+fO8vX1VZUqVbRw4ULTNPHkyZMVEREhf39/hYeHa8CAAbp8+bKkq5XuPn366OLFi9bK+pgxYyTZTjf36NFD3bt3t4ktOztbZcqUsX73DcPQpEmTVLVqVfn6+uq+++7Tv//979vwScHRbsd31WKxaOXKlTavKVWqlBISEiRJVapUkSQ1bNhQFotFrVu3lnS1ktm1a1fFx8crLCxMNWvWlCR9+umnatKkiUqWLKnQ0FD17NlTp0+fvn1vGrhFJIl3GV9fX2VnZ2v48OFatmyZEhMTtWvXLlWvXl0dO3bU+fPnJUlvvPGGDh48qK+++kqHDh3SrFmzVKZMGdN44eHhWrZsmSTp8OHDSk5O1rRp00z9oqOj9f333+u3336zth04cED79u1TdHS0JGnOnDkaNWqUxo8fr0OHDikuLk5vvPGGEhMT7fFR4A5wd3dXXFycZsyYoZMnT5qO79u3Tx07dtQTTzyhvXv3asmSJdq8ebNefvlla5/nnntOp06d0saNG7Vs2TJ9+OGHpn8o3dzcNH36dO3fv1+JiYlav369hg8fLklq0aKFpk6dqoCAACUnJys5OTnfanp0dLRWrVplTS4l6T//+Y/S0tL05JNPSpL+8Y9/aN68eZo1a5YOHDigV199Vc8++6w2bdp0Wz4vOM7t+K7+lR9++EGS9PXXXys5OVnLly+3Hvvmm2906NAhrVu3Tp9//rmkqxXFcePG6ccff9TKlSuVlJRk/SMccAoGiq1evXoZXbp0sT7//vvvjeDgYOOpp54yPD09jQULFliPZWVlGWFhYcakSZMMwzCMxx57zOjTp0++4yYlJRmSjN27dxuGYRgbNmwwJBmpqak2/Vq1amW88sor1uf169c33nrrLevzkSNHGk2bNrU+Dw8PNxYuXGgzxrhx44zIyMjCvG04iT9//5o3b2707dvXMAzDWLFihXHtV8vf/vY344UXXrB53bfffmu4ubkZGRkZxqFDhwxJxvbt263Hf/nlF0OSMWXKlBuee+nSpUZwcLD1+bx584zAwEBTv0qVKlnHycrKMsqUKWN88skn1uM9evQwnn76acMwDOPy5cuGj4+PsWXLFpsxYmJijB49etz8w4BTux3fVcMwDEnGihUrbPoEBgYa8+bNMwzD/Lvzz+cPCQkxMjMzbxrnDz/8YEgy/vjjD8Mwbvy7F7hTqCQWc59//rlKlCghHx8fRUZGqmXLlho0aJCys7P1wAMPWPt5enrq/vvv16FDhyRJ/fv31+LFi9WgQQMNHz5cW7ZsueVYoqOjtWDBAklXp+0WLVpkrSKeOXNGJ06cUExMjEqUKGF9vP322zbVRxRPEydOVGJiog4ePGjTvnPnTiUkJNj8P+/YsaPy8vKUlJSkw4cPy8PDQ40aNbK+pnr16ipdurTNOBs2bFD79u1VoUIFlSxZUs8995zOnTtXqIu0PD099fTTT1u/o2lpafrss8+s39GDBw/qypUrat++vU28n3zyCd/Ru0hRv6u3KiIiQl5eXjZtu3fvVpcuXVSpUiWVLFnSOj19/PjxWz4fcDtw7+Zirk2bNpo1a5Y8PT0VFhYmT09P/fjjj5JkumrPMAxrW6dOnXTs2DF98cUX+vrrr9WuXTsNHDhQ77zzTpFj6dmzp/7+979r165dysjI0IkTJ6xrwPLy8iRdnXJu1qyZzevc3d2LfE44h5YtW6pjx456/fXXbabL8vLy9OKLL2rw4MGm11SsWFGHDx/OdzzjT3cLPXbsmB555BG99NJLGjdunIKCgrR582bFxMQoOzu7UHFGR0erVatWOn36tNatWycfHx916tTJGqskffHFF6pQoYLN67gn792jqN9V6ervVOO6O9kW9Dvo7+9v8zwtLU0dOnRQhw4d9Omnn6ps2bI6fvy4OnbsyIUtcBokicWcv7+/qlevbtNWvXp1eXl5afPmzerZs6ekq7/IduzYYbOvYdmyZdW7d2/17t1bDz30kF577bV8k8Rrf/3m5ubeNJZ77rlHLVu21IIFC5SRkaGoqCiFhIRIkkJCQlShQgUdOXLEWrnB3WXChAlq0KCBdVG+JDVq1EgHDhwwfUevuffee5WTk6Pdu3ercePGkqRff/3VZrulHTt2KCcnR++++67c3K5OfixdutRmHC8vr7/8fkpX1y+Gh4dryZIl+uqrr/T0009bv9916tSRt7e3jh8/rlatWhXqvaN4Kcp3Vbr6OzM5Odn6/JdfflF6err1eUF/V0rSTz/9pLNnz2rChAkKDw+XdPW7DjgTksS7kL+/v/r376/XXntNQUFBqlixoiZNmqT09HTFxMRIkt588001btxYdevWVWZmpj7//HPVrl073/EqVaoki8Wizz//XI888oh8fX1VokSJfPtGR0drzJgxysrK0pQpU2yOjRkzRoMHD1ZAQIA6deqkzMxM7dixQ6mpqRoyZMjt/RBwx0VERCg6OlozZsywto0YMULNmzfXwIED1a9fP/n7+1sX78+YMUP33nuvoqKi9MILL1gr4kOHDpWvr6+16l2tWjXl5ORoxowZeuyxx/Tdd99p9uzZNueuXLmyLl++rG+++Ub33Xef/Pz88t36xmKxqGfPnpo9e7Z+/vlnbdiwwXqsZMmSGjZsmF599VXl5eXpwQcf1KVLl7RlyxaVKFFCvXr1stMnhzutKN9VSWrbtq1mzpyp5s2bKy8vTyNGjJCnp6d1jHLlysnX11dr1qzRPffcIx8fHwUGBuYbQ8WKFeXl5aUZM2bopZde0v79+9l3Fs7HsUsicSuuv3DlzzIyMoxBgwYZZcqUMby9vY0HHnjA+OGHH6zHx40bZ9SuXdvw9fU1goKCjC5duhhHjhwxDCP/xddvvfWWERoaalgsFqNXr16GYZgvXDEMw0hNTTW8vb0NPz8/6+LrP1uwYIHRoEEDw8vLyyhdurTRsmVLY/ny5bf0OcAx8vv+HT161PD29jb+/Kvlhx9+MNq3b2+UKFHC8Pf3N+rXr2+MHz/eevzUqVNGp06dDG9vb6NSpUrGwoULjXLlyhmzZ8+29pk8ebJRvnx5w9fX1+jYsaPxySefmBb0v/TSS0ZwcLAhyRg9erRhGLYXrlxz4MABQ5JRqVIlIy8vz+ZYXl6eMW3aNKNWrVqGp6enUbZsWaNjx47Gpk2bbu3DgkPdru/q77//bnTo0MHw9/c3atSoYXz55Zc2F64YhmHMmTPHCA8PN9zc3IxWrVrd8PyGYRgLFy40KleubHh7exuRkZHGqlWrCnTRIHCnWAzjugUWAOBAJ0+eVHh4uHWtLADAMUgSATjU+vXrdfnyZUVERCg5OVnDhw/X77//rp9//tlmKg8AcGexJhGAQ2VnZ+v111/XkSNHVLJkSbVo0UILFiwgQQQAB6OSCAAAABM20wYAAIAJSSIAAABMSBIBAABgQpIIAAAAE5JEAAAAmJAkAritxowZowYNGlif9+7dW127dr3jcRw9elQWi0V79uy5YZ/KlStr6tSpBR4zISFBpUqVuuXYLBaLVq5cecvjAIA9kSQCLqB3796yWCyyWCzy9PRU1apVNWzYMKWlpdn93NOmTVNCQkKB+hYksQMA3Blspg24iIcffljz5s1Tdna2vv32Wz3//PNKS0vTrFmzTH2zs7Nv22bWgYGBt2UcAMCdRSURcBHe3t4KDQ1VeHi4evbsqejoaOuU57Up4o8//lhVq1aVt7e3DMPQxYsX9cILL6hcuXIKCAhQ27Zt9eOPP9qMO2HCBIWEhKhkyZKKiYnRlStXbI5fP92cl5eniRMnqnr16vL29lbFihU1fvx4SVKVKlUkSQ0bNpTFYlHr1q2tr5s3b55q164tHx8f3XvvvXr//fdtzvPDDz+oYcOG8vHxUZMmTbR79+5Cf0aTJ09WRESE/P39FR4ergEDBujy5cumfitXrlTNmjXl4+Oj9u3b68SJEzbHV69ercaNG8vHx0dVq1bV2LFjlZOTU+h4AMCRSBIBF+Xr66vs7Gzr819//VVLly7VsmXLrNO9nTt3VkpKir788kvt3LlTjRo1Urt27XT+/HlJ0tKlSzV69GiNHz9eO3bsUPny5U3J2/VGjhypiRMn6o033tDBgwe1cOFChYSESLqa6EnS119/reTkZC1fvlySNGfOHI0aNUrjx4/XoUOHFBcXpzfeeEOJiYmSpLS0ND366KOqVauWdu7cqTFjxmjYsGGF/kzc3Nw0ffp07d+/X4mJiVq/fr2GDx9u0yc9PV3jx49XYmKivvvuO126dEndu3e3Hv/Pf/6jZ599VoMHD9bBgwf1wQcfKCEhwZoIA0CxYQC46/Xq1cvo0qWL9fn3339vBAcHG88884xhGIYxevRow9PT0zh9+rS1zzfffGMEBAQYV65csRmrWrVqxgcffGAYhmFERkYaL730ks3xZs2aGffdd1++57506ZLh7e1tzJkzJ984k5KSDEnG7t27bdrDw8ONhQsX2rSNGzfOiIyMNAzDMD744AMjKCjISEtLsx6fNWtWvmP9WaVKlYwpU6bc8PjSpUuN4OBg6/N58+YZkoxt27ZZ2w4dOmRIMr7//nvDMAzjoYceMuLi4mzGmT9/vlG+fHnrc0nGihUrbnheAHAGrEkEXMTnn3+uEiVKKCcnR9nZ2erSpYtmzJhhPV6pUiWVLVvW+nznzp26fPmygoODbcbJyMjQb7/9Jkk6dOiQXnrpJZvjkZGR2rBhQ74xHDp0SJmZmWrXrl2B4z5z5oxOnDihmJgY9evXz9qek5NjXe946NAh3XffffLz87OJo7A2bNiguLg4HTx4UJcuXVJOTo6uXLmitLQ0+fv7S5I8PDzUpEkT62vuvfdelSpVSocOHdL999+vnTt3avv27TaVw9zcXF25ckXp6ek2MQKAMyNJBFxEmzZtNGvWLHl6eiosLMx0Ycq1JOiavLw8lS9fXhs3bjSNVdRtYHx9fQv9mry8PElXp5ybNWtmc8zd3V2SZBhGkeL5s2PHjumRRx7RSy+9pHHjxikoKEibN29WTEyMzbS8dHULm+tda8vLy9PYsWP1xBNPmPr4+PjccpwAcKeQJAIuwt/fX9WrVy9w/0aNGiklJUUeHh6qXLlyvn1q166tbdu26bnnnrO2bdu27YZj1qhRQ76+vvrmm2/0/PPPm457eXlJulp5uyYkJEQVKlTQkSNHFB0dne+4derU0fz585WRkWFNRG8WR3527NihnJwcvfvuu3Jzu7pce+nSpaZ+OTk52rFjh+6//35J0uHDh3XhwgXde++9kq5+bocPHy7UZw0AzogkEUC+oqKiFBkZqa5du2rixImqVauWTp06pS+//FJdu3ZVkyZN9Morr6hXr15q0qSJHnzwQS1YsEAHDhxQ1apV8x3Tx8dHI0aM0PDhw+Xl5aUHHnhAZ86c0YEDBxQTE6Ny5crJ19dXa9as0T333CMfHx8FBgZqzJgxGjx4sAICAtSpUydlZmZqx44dSk1N1ZAhQ9SzZ0+NGjVKMTEx+sc//qGjR4/qnXfeKdT7rVatmnJycjRjxgw99thj+u677zR79mxTP09PTw0aNEjTp0+Xp6enXn75ZTVv3tyaNL755pt69NFHFR4erqefflpubm7au3ev9u3bp7fffrvw/yMAwEG4uhlAviwWi7788ku1bNlSffv2Vc2aNdW9e3cdPXrUejVyt27d9Oabb2rEiBFq3Lixjh07pv79+9903DfeeENDhw7Vm2++qdq1a6tbt246ffq0pKvr/aZPn64PPvhAYWFh6tKliyTp+eef10cffaSEhARFRESoVatWSkhIsG6ZU6JECa1evVoHDx5Uw4YNNWrUKE2cOLFQ77dBgwaaPHmyJk6cqHr16mnBggWKj4839fPz89OIESPUs2dPRUZGytfXV4sXL7Ye79ixoz7//HOtW7dOTZs2VfPmzTV58mRVqlSpUPEAgKNZjNuxmAcAAAB3FSqJAAAAMCFJBAAAgAlJIgAAAExIEgEAAGBCkggAAAATkkQAAACYkCQCAADAhCQRAAAAJiSJAAAAMCFJBAAAgAlJIgAAAEz+P8Q6moiMcnDGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = accuracy_score(label_true, label_pred_labelled)\n",
    "print(\"Accuracy:\" , accuracy)\n",
    "f1score_macro = f1_score(label_true, label_pred_labelled, average='macro')\n",
    "print(\"\\n\")\n",
    "print(\"F1-Score - macro:\", f1score_macro)\n",
    "f1score_weighted = f1_score(label_true, label_pred_labelled, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"F1-Score - weighted:\", f1score_weighted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(label_true, label_pred_labelled)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=[\"Positive\", \"Negative\", \"Neutral\"],\n",
    "            yticklabels=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.59      0.77      0.67        73\n",
      "    Negative       0.11      0.04      0.05        28\n",
      "     Neutral       0.94      0.94      0.94       618\n",
      "\n",
      "    accuracy                           0.89       719\n",
      "   macro avg       0.55      0.58      0.55       719\n",
      "weighted avg       0.87      0.89      0.88       719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Determine unique classes in true labels\n",
    "unique_classes = sorted(set(label_true))\n",
    "\n",
    "# For testing dataset\n",
    "testing_report = classification_report(label_true, label_pred_labelled, labels=unique_classes, target_names=[\"Positive\", \"Negative\", \"Neutral\"])\n",
    "print(\"Testing Classification Report:\")\n",
    "print(testing_report)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
